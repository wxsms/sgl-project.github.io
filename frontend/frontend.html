
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>SGLang Frontend Language &#8212; SGLang</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
    <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=61678c10"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=ccdb6887"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'frontend/frontend';</script>
    <link rel="icon" href="../_static/logo.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Choices Methods in SGLang" href="choices_methods.html" />
    <link rel="prev" title="LoRA Serving" href="../backend/lora.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="May 14, 2025"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="SGLang - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="SGLang - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../start/install.html">Install SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Backend Tutorial</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../references/deepseek.html">DeepSeek Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/llama4.html">Llama4 Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/send_request.html">Sending Requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/openai_api_completions.html">OpenAI APIs - Completions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/openai_api_vision.html">OpenAI APIs - Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/openai_api_embeddings.html">OpenAI APIs - Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/native_api.html">SGLang Native APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/offline_engine_api.html">Offline Engine API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/server_arguments.html">Server Arguments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/sampling_params.html">Sampling Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/hyperparameter_tuning.html">Hyperparameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/attention_backend.html">Attention Backend</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supported Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../supported_models/generative_models.html">Large Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/vision_language_models.html">Vision Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/embedding_models.html">Embedding Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/reward_models.html">Reward Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/support_new_models.html">How to Support New Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../backend/speculative_decoding.html">Speculative Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/structured_outputs.html">Structured Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/function_calling.html">Tool and Function Calling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/separate_reasoning.html">Reasoning Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/structured_outputs_for_reasoning_models.html">Structured Outputs For Reasoning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/custom_chat_template.html">Custom Chat Template</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend/lora.html">LoRA Serving</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frontend Tutorial</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">SGLang Frontend Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="choices_methods.html">Choices Methods in SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">SGLang Router</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../router/router.html">Router for Data Parallelism</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../references/general.html">General Guidance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/hardware.html">Hardware Supports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/advanced_deploy.html">Multi-Node Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/performance_tuning.html">Performance Tuning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/blob/main/frontend/frontend.ipynb?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/edit/main/frontend/frontend.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/issues/new?title=Issue%20on%20page%20%2Ffrontend/frontend.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/frontend/frontend.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>SGLang Frontend Language</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Launch-A-Server">Launch A Server</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Basic-Usage">Basic Usage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Multi-turn-Dialog">Multi-turn Dialog</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Control-flow">Control flow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Parallelism">Parallelism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Constrained-Decoding">Constrained Decoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Batching">Batching</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Streaming">Streaming</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Complex-Prompts">Complex Prompts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Multi-modal-Generation">Multi-modal Generation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <style>
    .output_area.stderr, .output_area.stdout {
        color: #d3d3d3 !important; /* light gray */
    }
</style><section id="SGLang-Frontend-Language">
<h1>SGLang Frontend Language<a class="headerlink" href="#SGLang-Frontend-Language" title="Link to this heading">#</a></h1>
<p>SGLang frontend language can be used to define simple and easy prompts in a convenient, structured way.</p>
<section id="Launch-A-Server">
<h2>Launch A Server<a class="headerlink" href="#Launch-A-Server" title="Link to this heading">#</a></h2>
<p>Launch the server in your terminal and wait for it to initialize.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sglang</span><span class="w"> </span><span class="kn">import</span> <span class="n">assistant_begin</span><span class="p">,</span> <span class="n">assistant_end</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang</span><span class="w"> </span><span class="kn">import</span> <span class="n">assistant</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span> <span class="n">gen</span><span class="p">,</span> <span class="n">system</span><span class="p">,</span> <span class="n">user</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang</span><span class="w"> </span><span class="kn">import</span> <span class="n">image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang</span><span class="w"> </span><span class="kn">import</span> <span class="n">RuntimeEndpoint</span><span class="p">,</span> <span class="n">set_default_backend</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang.srt.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang.test.test_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">is_in_ci</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">print_highlight</span><span class="p">,</span> <span class="n">terminate_process</span><span class="p">,</span> <span class="n">wait_for_server</span>

<span class="k">if</span> <span class="n">is_in_ci</span><span class="p">():</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">patch</span><span class="w"> </span><span class="kn">import</span> <span class="n">launch_server_cmd</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">sglang.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">launch_server_cmd</span>


<span class="n">server_process</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">launch_server_cmd</span><span class="p">(</span>
    <span class="s2">&quot;python -m sglang.launch_server --model-path Qwen/Qwen2.5-7B-Instruct --host 0.0.0.0&quot;</span>
<span class="p">)</span>

<span class="n">wait_for_server</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Server started on http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:25:47] server_args=ServerArgs(model_path=&#39;Qwen/Qwen2.5-7B-Instruct&#39;, tokenizer_path=&#39;Qwen/Qwen2.5-7B-Instruct&#39;, tokenizer_mode=&#39;auto&#39;, skip_tokenizer_init=False, enable_tokenizer_batch_encode=False, load_format=&#39;auto&#39;, trust_remote_code=False, dtype=&#39;auto&#39;, kv_cache_dtype=&#39;auto&#39;, quantization=None, quantization_param_path=None, context_length=None, device=&#39;cuda&#39;, served_model_name=&#39;Qwen/Qwen2.5-7B-Instruct&#39;, chat_template=None, completion_template=None, is_embedding=False, revision=None, host=&#39;0.0.0.0&#39;, port=36739, mem_fraction_static=0.88, max_running_requests=200, max_total_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy=&#39;fcfs&#39;, schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=932576446, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level=&#39;info&#39;, log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, enable_request_time_stats_logging=False, api_key=None, file_storage_path=&#39;sglang_storage&#39;, enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method=&#39;round_robin&#39;, ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args=&#39;{}&#39;, lora_paths=None, max_loras_per_batch=8, lora_backend=&#39;triton&#39;, attention_backend=None, sampling_backend=&#39;flashinfer&#39;, grammar_backend=&#39;xgrammar&#39;, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type=&#39;qk&#39;, ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode=&#39;auto&#39;, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config=&#39;&#39;, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy=&#39;write_through_selective&#39;, flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, mm_attention_backend=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode=&#39;null&#39;, disaggregation_bootstrap_port=8998, disaggregation_transfer_backend=&#39;mooncake&#39;, disaggregation_ib_device=None, pdlb_url=None)
[2025-05-14 00:25:55] Attention backend not set. Use fa3 backend by default.
[2025-05-14 00:25:55] Init torch distributed begin.
[2025-05-14 00:25:55] Init torch distributed ends. mem usage=0.00 GB
[2025-05-14 00:25:55] Load weight begin. avail mem=37.05 GB
[2025-05-14 00:25:56] Using model weights format [&#39;*.safetensors&#39;]
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00&lt;?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00&lt;00:01,  1.64it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01&lt;00:01,  1.55it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01&lt;00:00,  1.48it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02&lt;00:00,  1.47it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02&lt;00:00,  1.50it/s]

[2025-05-14 00:25:59] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=22.61 GB, mem usage=14.44 GB.
[2025-05-14 00:25:59] KV Cache is allocated. #tokens: 20480, K size: 0.55 GB, V size: 0.55 GB
[2025-05-14 00:25:59] Memory pool end. avail mem=21.32 GB
[2025-05-14 00:26:00] max_total_num_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=200, context_len=32768
[2025-05-14 00:26:00] INFO:     Started server process [1655160]
[2025-05-14 00:26:00] INFO:     Waiting for application startup.
[2025-05-14 00:26:00] INFO:     Application startup complete.
[2025-05-14 00:26:00] INFO:     Uvicorn running on http://0.0.0.0:36739 (Press CTRL+C to quit)
[2025-05-14 00:26:01] INFO:     127.0.0.1:35484 - &#34;GET /v1/models HTTP/1.1&#34; 200 OK
[2025-05-14 00:26:01] INFO:     127.0.0.1:35490 - &#34;GET /get_model_info HTTP/1.1&#34; 200 OK
[2025-05-14 00:26:01] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:03] INFO:     127.0.0.1:35492 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-05-14 00:26:03] The server is fired up and ready to roll!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    We are running those notebooks in a CI parallel environment, so the throughput is not representative of the actual performance.<br>                    </strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Server started on http://localhost:36739
</pre></div></div>
</div>
<p>Set the default backend. Note: Besides the local server, you may use also <code class="docutils literal notranslate"><span class="pre">OpenAI</span></code> or other API endpoints.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_default_backend</span><span class="p">(</span><span class="n">RuntimeEndpoint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:06] INFO:     127.0.0.1:50126 - &#34;GET /get_model_info HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
</section>
<section id="Basic-Usage">
<h2>Basic Usage<a class="headerlink" href="#Basic-Usage" title="Link to this heading">#</a></h2>
<p>The most simple way of using SGLang frontend language is a simple question answer dialog between a user and an assistant.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">basic_qa</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">question</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">system</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;You are a helpful assistant than can answer questions.&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;answer&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">basic_qa</span><span class="p">(</span><span class="s2">&quot;List 3 countries and their capitals.&quot;</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;answer&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:06] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:06] Decode batch. #running-req: 1, #token: 64, token usage: 0.00, cuda graph: False, gen throughput (token/s): 5.77, #queue-req: 0
[2025-05-14 00:26:06] INFO:     127.0.0.1:50128 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Sure! Here are three countries with their capitals:<br><br>1. **France** - Paris<br>2. **Spain** - Madrid<br>3. **Italy** - Rome</strong></div>
</div>
</section>
<section id="Multi-turn-Dialog">
<h2>Multi-turn Dialog<a class="headerlink" href="#Multi-turn-Dialog" title="Link to this heading">#</a></h2>
<p>SGLang frontend language can also be used to define multi-turn dialogs.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">multi_turn_qa</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">system</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;You are a helpful assistant than can answer questions.&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="s2">&quot;Please give me a list of 3 countries and their capitals.&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;first_answer&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">))</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="s2">&quot;Please give me another list of 3 countries and their capitals.&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;second_answer&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">s</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">multi_turn_qa</span><span class="p">()</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;first_answer&quot;</span><span class="p">])</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;second_answer&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:06] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 18, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:07] INFO:     127.0.0.1:50138 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-05-14 00:26:07] Decode batch. #running-req: 1, #token: 0, token usage: 0.00, cuda graph: False, gen throughput (token/s): 61.80, #queue-req: 0
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Certainly! Here is a list of three countries along with their capitals:<br><br>1. **Germany** - Berlin<br>2. **France** - Paris<br>3. **Spain** - Madrid</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:07] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 73, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:08] INFO:     127.0.0.1:50152 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Of course! Here is another list of three countries and their capitals:<br><br>1. **Italy** - Rome<br>2. **Canada** - Ottawa<br>3. **Japan** - Tokyo</strong></div>
</div>
</section>
<section id="Control-flow">
<h2>Control flow<a class="headerlink" href="#Control-flow" title="Link to this heading">#</a></h2>
<p>You may use any Python code within the function to define more complex control flows.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">tool_use</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">question</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span>
        <span class="s2">&quot;To answer this question: &quot;</span>
        <span class="o">+</span> <span class="n">question</span>
        <span class="o">+</span> <span class="s2">&quot;. I need to use a &quot;</span>
        <span class="o">+</span> <span class="n">gen</span><span class="p">(</span><span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;calculator&quot;</span><span class="p">,</span> <span class="s2">&quot;search engine&quot;</span><span class="p">])</span>
        <span class="o">+</span> <span class="s2">&quot;. &quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="s2">&quot;tool&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;calculator&quot;</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="s2">&quot;The math expression is: &quot;</span> <span class="o">+</span> <span class="n">gen</span><span class="p">(</span><span class="s2">&quot;expression&quot;</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">s</span><span class="p">[</span><span class="s2">&quot;tool&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;search engine&quot;</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="s2">&quot;The key word to search is: &quot;</span> <span class="o">+</span> <span class="n">gen</span><span class="p">(</span><span class="s2">&quot;word&quot;</span><span class="p">))</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">tool_use</span><span class="p">(</span><span class="s2">&quot;What is 2 * 2?&quot;</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;tool&quot;</span><span class="p">])</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;expression&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:08] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 8, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:08] INFO:     127.0.0.1:50154 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-05-14 00:26:08] Prefill batch. #new-seq: 2, #new-token: 5, #cached-token: 62, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:08] INFO:     127.0.0.1:50158 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>calculator</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:08] Decode batch. #running-req: 2, #token: 0, token usage: 0.00, cuda graph: False, gen throughput (token/s): 50.89, #queue-req: 0
[2025-05-14 00:26:08] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 33, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:09] Decode batch. #running-req: 1, #token: 87, token usage: 0.00, cuda graph: False, gen throughput (token/s): 62.70, #queue-req: 0
[2025-05-14 00:26:09] INFO:     127.0.0.1:50174 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>2 * 2. You don't actually need a calculator to solve this as it's a simple multiplication problem, but here's the answer:<br><br>2 * 2 = 4<br><br>So, the result of 2 * 2 is 4.</strong></div>
</div>
</section>
<section id="Parallelism">
<h2>Parallelism<a class="headerlink" href="#Parallelism" title="Link to this heading">#</a></h2>
<p>Use <code class="docutils literal notranslate"><span class="pre">fork</span></code> to launch parallel prompts. Because <code class="docutils literal notranslate"><span class="pre">sgl.gen</span></code> is non-blocking, the for loop below issues two generation calls in parallel.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">tip_suggestion</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span>
        <span class="s2">&quot;Here are two tips for staying healthy: &quot;</span>
        <span class="s2">&quot;1. Balanced Diet. 2. Regular Exercise.</span><span class="se">\n\n</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="n">forks</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">fork</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">forks</span><span class="p">):</span>
        <span class="n">f</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Now, expand tip </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> into a paragraph:</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="o">+</span> <span class="n">gen</span><span class="p">(</span><span class="s2">&quot;detailed_tip&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="s2">&quot;Tip 1:&quot;</span> <span class="o">+</span> <span class="n">forks</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;detailed_tip&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="s2">&quot;Tip 2:&quot;</span> <span class="o">+</span> <span class="n">forks</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;detailed_tip&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span>
        <span class="s2">&quot;To summarize the above two tips, I can say:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">gen</span><span class="p">(</span><span class="s2">&quot;summary&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
    <span class="p">)</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">tip_suggestion</span><span class="p">()</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;summary&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:09] Prefill batch. #new-seq: 2, #new-token: 70, #cached-token: 28, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:09] Decode batch. #running-req: 2, #token: 112, token usage: 0.01, cuda graph: False, gen throughput (token/s): 104.83, #queue-req: 0
[2025-05-14 00:26:10] Decode batch. #running-req: 2, #token: 192, token usage: 0.01, cuda graph: False, gen throughput (token/s): 127.58, #queue-req: 0
[2025-05-14 00:26:10] Decode batch. #running-req: 2, #token: 272, token usage: 0.01, cuda graph: False, gen throughput (token/s): 128.06, #queue-req: 0
[2025-05-14 00:26:11] Decode batch. #running-req: 2, #token: 352, token usage: 0.02, cuda graph: False, gen throughput (token/s): 151.10, #queue-req: 0
[2025-05-14 00:26:11] INFO:     127.0.0.1:50186 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-05-14 00:26:11] INFO:     127.0.0.1:50184 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-05-14 00:26:11] Prefill batch. #new-seq: 1, #new-token: 360, #cached-token: 39, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:12] Decode batch. #running-req: 1, #token: 418, token usage: 0.02, cuda graph: False, gen throughput (token/s): 80.60, #queue-req: 0
[2025-05-14 00:26:12] Decode batch. #running-req: 1, #token: 458, token usage: 0.02, cuda graph: False, gen throughput (token/s): 63.82, #queue-req: 0
[2025-05-14 00:26:13] Decode batch. #running-req: 1, #token: 498, token usage: 0.02, cuda graph: False, gen throughput (token/s): 63.90, #queue-req: 0
[2025-05-14 00:26:14] Decode batch. #running-req: 1, #token: 538, token usage: 0.03, cuda graph: False, gen throughput (token/s): 63.32, #queue-req: 0
[2025-05-14 00:26:14] Decode batch. #running-req: 1, #token: 578, token usage: 0.03, cuda graph: False, gen throughput (token/s): 65.62, #queue-req: 0
[2025-05-14 00:26:15] INFO:     127.0.0.1:50194 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>1. **Balanced Diet**: A balanced diet involves consuming a variety of nutrients from all the food groups, including:<br>   - Carbohydrates for energy<br>   - Proteins for tissue repair and growth<br>   - Healthy fats for brain function and absorption of vitamins<br>   - A wide array of fruits and vegetables for vitamins, minerals, and fiber<br>   - Limiting processed foods, sugars, and saturated fats<br>   - Staying hydrated by drinking plenty of water<br><br>2. **Regular Exercise**: Regular exercise is crucial for maintaining good health and includes:<br>   - At least 30 minutes of moderate physical activity most days of the week (e.g., brisk walking, cycling, swimming)<br>   - Strength training exercises at least two days per week<br>   - Yoga or other flexibility and balance exercises<br>   - Endurance activities that improve cardiovascular health<br>   - Benefits such as weight management, reduced risk of chronic diseases, improved mental health, and better sleep quality<br><br>By combining these two lifestyle habits, you can significantly enhance your overall health and quality of life.</strong></div>
</div>
</section>
<section id="Constrained-Decoding">
<h2>Constrained Decoding<a class="headerlink" href="#Constrained-Decoding" title="Link to this heading">#</a></h2>
<p>Use <code class="docutils literal notranslate"><span class="pre">regex</span></code> to specify a regular expression as a decoding constraint. This is only supported for local models.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">regular_expression_gen</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="s2">&quot;What is the IP address of the Google DNS servers?&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span>
        <span class="n">gen</span><span class="p">(</span>
            <span class="s2">&quot;answer&quot;</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">regex</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;((25[0-5]|2[0-4]\d|[01]?\d\d?).)</span><span class="si">{3}</span><span class="s2">(25[0-5]|2[0-4]\d|[01]?\d\d?)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">regular_expression_gen</span><span class="p">()</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;answer&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:15] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 12, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:16] Decode batch. #running-req: 1, #token: 34, token usage: 0.00, cuda graph: False, gen throughput (token/s): 19.52, #queue-req: 0
[2025-05-14 00:26:16] INFO:     127.0.0.1:50204 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>208.67.222.222</strong></div>
</div>
<p>Use <code class="docutils literal notranslate"><span class="pre">regex</span></code> to define a <code class="docutils literal notranslate"><span class="pre">JSON</span></code> decoding schema.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">character_regex</span> <span class="o">=</span> <span class="p">(</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;\{\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;name&quot;: &quot;[\w\d\s]{1,16}&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;house&quot;: &quot;(Gryffindor|Slytherin|Ravenclaw|Hufflepuff)&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;blood status&quot;: &quot;(Pure-blood|Half-blood|Muggle-born)&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;occupation&quot;: &quot;(student|teacher|auror|ministry of magic|death eater|order of the phoenix)&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;wand&quot;: \{\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;        &quot;wood&quot;: &quot;[\w\d\s]{1,16}&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;        &quot;core&quot;: &quot;[\w\d\s]{1,16}&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;        &quot;length&quot;: [0-9]{1,2}\.[0-9]{0,2}\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    \},\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;alive&quot;: &quot;(Alive|Deceased)&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;patronus&quot;: &quot;[\w\d\s]{1,16}&quot;,\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    &quot;bogart&quot;: &quot;[\w\d\s]{1,16}&quot;\n&quot;&quot;&quot;</span>
    <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;\}&quot;&quot;&quot;</span>
<span class="p">)</span>


<span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">character_gen</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> is a character in Harry Potter. Please fill in the following information about this character.&quot;</span>
    <span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;json_output&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">regex</span><span class="o">=</span><span class="n">character_regex</span><span class="p">))</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">character_gen</span><span class="p">(</span><span class="s2">&quot;Harry Potter&quot;</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;json_output&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:17] Prefill batch. #new-seq: 1, #new-token: 24, #cached-token: 14, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:17] Decode batch. #running-req: 1, #token: 67, token usage: 0.00, cuda graph: False, gen throughput (token/s): 35.87, #queue-req: 0
[2025-05-14 00:26:18] Decode batch. #running-req: 1, #token: 107, token usage: 0.01, cuda graph: False, gen throughput (token/s): 64.74, #queue-req: 0
[2025-05-14 00:26:19] Decode batch. #running-req: 1, #token: 1, token usage: 0.00, cuda graph: False, gen throughput (token/s): 64.70, #queue-req: 0
[2025-05-14 00:26:19] INFO:     127.0.0.1:33588 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>{<br>    "name": "Harry Potter",<br>    "house": "Gryffindor",<br>    "blood status": "Half-blood",<br>    "occupation": "student",<br>    "wand": {<br>        "wood": "Willow",<br>        "core": "Phoenix Feather",<br>        "length": 10.5<br>    },<br>    "alive": "Alive",<br>    "patronus": "stag",<br>    "bogart": "dementor"<br>}</strong></div>
</div>
</section>
<section id="Batching">
<h2>Batching<a class="headerlink" href="#Batching" title="Link to this heading">#</a></h2>
<p>Use <code class="docutils literal notranslate"><span class="pre">run_batch</span></code> to run a batch of prompts.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">text_qa</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">question</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;answer&quot;</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">))</span>


<span class="n">states</span> <span class="o">=</span> <span class="n">text_qa</span><span class="o">.</span><span class="n">run_batch</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of the United Kingdom?&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of Japan?&quot;</span><span class="p">},</span>
    <span class="p">],</span>
    <span class="n">progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">states</span><span class="p">):</span>
    <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Answer </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">states</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;answer&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:19] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 13, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:19] INFO:     127.0.0.1:33602 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
 67%|   | 2/3 [00:00&lt;00:00, 10.74it/s]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:19] Prefill batch. #new-seq: 3, #new-token: 29, #cached-token: 51, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:19] INFO:     127.0.0.1:33616 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-05-14 00:26:19] INFO:     127.0.0.1:33628 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|| 3/3 [00:00&lt;00:00, 16.80it/s]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:19] INFO:     127.0.0.1:33606 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Answer 1: The capital of the United Kingdom is London.</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Answer 2: The capital of France is Paris.</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Answer 3: The capital of Japan is Tokyo.</strong></div>
</div>
</section>
<section id="Streaming">
<h2>Streaming<a class="headerlink" href="#Streaming" title="Link to this heading">#</a></h2>
<p>Use <code class="docutils literal notranslate"><span class="pre">stream</span></code> to stream the output to the user.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">text_qa</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">question</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;answer&quot;</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">))</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">text_qa</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
    <span class="n">question</span><span class="o">=</span><span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">text_iter</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;|im_start|&gt;system
You are a helpful assistant.&lt;|im_end|&gt;
&lt;|im_start|&gt;user
What is the capital of France?&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
[2025-05-14 00:26:19] INFO:     127.0.0.1:33642 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-05-14 00:26:19] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 25, token usage: 0.00, #running-req: 0, #queue-req: 0
The capital of France is Paris.&lt;|im_end|&gt;
</pre></div></div>
</div>
</section>
<section id="Complex-Prompts">
<h2>Complex Prompts<a class="headerlink" href="#Complex-Prompts" title="Link to this heading">#</a></h2>
<p>You may use <code class="docutils literal notranslate"><span class="pre">{system|user|assistant}_{begin|end}</span></code> to define complex prompts.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">chat_example</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">system</span><span class="p">(</span><span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">)</span>
    <span class="c1"># Same as: s += s.system(&quot;You are a helpful assistant.&quot;)</span>

    <span class="k">with</span> <span class="n">s</span><span class="o">.</span><span class="n">user</span><span class="p">():</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;Question: What is the capital of France?&quot;</span>

    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant_begin</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;Answer: &quot;</span> <span class="o">+</span> <span class="n">gen</span><span class="p">(</span><span class="s2">&quot;answer&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant_end</span><span class="p">()</span>


<span class="n">state</span> <span class="o">=</span> <span class="n">chat_example</span><span class="p">()</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;answer&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:19] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 14, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:19] INFO:     127.0.0.1:33652 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'> The capital of France is Paris.</strong></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terminate_process</span><span class="p">(</span><span class="n">server_process</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:19] Child process unexpectedly failed with an exit code 9. pid=1655523
</pre></div></div>
</div>
</section>
<section id="Multi-modal-Generation">
<h2>Multi-modal Generation<a class="headerlink" href="#Multi-modal-Generation" title="Link to this heading">#</a></h2>
<p>You may use SGLang frontend language to define multi-modal prompts. See <a class="reference external" href="https://docs.sglang.ai/supported_models/generative_models.html">here</a> for supported models.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">server_process</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">launch_server_cmd</span><span class="p">(</span>
    <span class="s2">&quot;python -m sglang.launch_server --model-path Qwen/Qwen2.5-VL-7B-Instruct --host 0.0.0.0&quot;</span>
<span class="p">)</span>

<span class="n">wait_for_server</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Server started on http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:26] server_args=ServerArgs(model_path=&#39;Qwen/Qwen2.5-VL-7B-Instruct&#39;, tokenizer_path=&#39;Qwen/Qwen2.5-VL-7B-Instruct&#39;, tokenizer_mode=&#39;auto&#39;, skip_tokenizer_init=False, enable_tokenizer_batch_encode=False, load_format=&#39;auto&#39;, trust_remote_code=False, dtype=&#39;auto&#39;, kv_cache_dtype=&#39;auto&#39;, quantization=None, quantization_param_path=None, context_length=None, device=&#39;cuda&#39;, served_model_name=&#39;Qwen/Qwen2.5-VL-7B-Instruct&#39;, chat_template=None, completion_template=None, is_embedding=False, revision=None, host=&#39;0.0.0.0&#39;, port=38226, mem_fraction_static=0.88, max_running_requests=200, max_total_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy=&#39;fcfs&#39;, schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=487768059, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level=&#39;info&#39;, log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, enable_request_time_stats_logging=False, api_key=None, file_storage_path=&#39;sglang_storage&#39;, enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method=&#39;round_robin&#39;, ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args=&#39;{}&#39;, lora_paths=None, max_loras_per_batch=8, lora_backend=&#39;triton&#39;, attention_backend=None, sampling_backend=&#39;flashinfer&#39;, grammar_backend=&#39;xgrammar&#39;, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type=&#39;qk&#39;, ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode=&#39;auto&#39;, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config=&#39;&#39;, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy=&#39;write_through_selective&#39;, flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, mm_attention_backend=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode=&#39;null&#39;, disaggregation_bootstrap_port=8998, disaggregation_transfer_backend=&#39;mooncake&#39;, disaggregation_ib_device=None, pdlb_url=None)
[2025-05-14 00:26:35] Attention backend not set. Use flashinfer backend by default.
[2025-05-14 00:26:35] Automatically reduce --mem-fraction-static to 0.792 because this is a multimodal model.
[2025-05-14 00:26:35] Automatically turn off --chunked-prefill-size for multimodal model.
[2025-05-14 00:26:35] Init torch distributed begin.
[2025-05-14 00:26:35] Init torch distributed ends. mem usage=0.00 GB
[2025-05-14 00:26:35] Load weight begin. avail mem=61.85 GB
[2025-05-14 00:26:36] Multimodal attention backend not set. Use sdpa.
[2025-05-14 00:26:37] Using model weights format [&#39;*.safetensors&#39;]
Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00&lt;?, ?it/s]
Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00&lt;00:03,  1.21it/s]
Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01&lt;00:02,  1.36it/s]
Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02&lt;00:01,  1.43it/s]
Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:02&lt;00:00,  1.45it/s]
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03&lt;00:00,  1.91it/s]
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03&lt;00:00,  1.64it/s]

[2025-05-14 00:26:40] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=46.06 GB, mem usage=15.79 GB.
[2025-05-14 00:26:41] KV Cache is allocated. #tokens: 20480, K size: 0.55 GB, V size: 0.55 GB
[2025-05-14 00:26:41] Memory pool end. avail mem=44.69 GB
2025-05-14 00:26:41,144 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
[2025-05-14 00:26:42] max_total_num_tokens=20480, chunked_prefill_size=-1, max_prefill_tokens=16384, max_running_requests=200, context_len=128000
[2025-05-14 00:26:42] INFO:     Started server process [1656639]
[2025-05-14 00:26:42] INFO:     Waiting for application startup.
[2025-05-14 00:26:42] INFO:     Application startup complete.
[2025-05-14 00:26:42] INFO:     Uvicorn running on http://0.0.0.0:38226 (Press CTRL+C to quit)
[2025-05-14 00:26:43] INFO:     127.0.0.1:58674 - &#34;GET /v1/models HTTP/1.1&#34; 200 OK
[2025-05-14 00:26:43] INFO:     127.0.0.1:58682 - &#34;GET /get_model_info HTTP/1.1&#34; 200 OK
[2025-05-14 00:26:43] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0
2025-05-14 00:26:45,559 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False
2025-05-14 00:26:45,573 - INFO - flashinfer.jit: Finished loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False
[2025-05-14 00:26:46] INFO:     127.0.0.1:58684 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    We are running those notebooks in a CI parallel environment, so the throughput is not representative of the actual performance.<br>                    </strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Server started on http://localhost:38226
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_default_backend</span><span class="p">(</span><span class="n">RuntimeEndpoint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:48] INFO:     127.0.0.1:39382 - &#34;GET /get_model_info HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<p>Ask a question about an image.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">image_qa</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">image_file</span><span class="p">,</span> <span class="n">question</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">user</span><span class="p">(</span><span class="n">image</span><span class="p">(</span><span class="n">image_file</span><span class="p">)</span> <span class="o">+</span> <span class="n">question</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">assistant</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="s2">&quot;answer&quot;</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">))</span>


<span class="n">image_url</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true&quot;</span>
<span class="n">image_bytes</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_image</span><span class="p">(</span><span class="n">image_url</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">image_qa</span><span class="p">(</span><span class="n">image_bytes</span><span class="p">,</span> <span class="s2">&quot;What is in the image?&quot;</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;answer&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:50] Prefill batch. #new-seq: 1, #new-token: 307, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0
[2025-05-14 00:26:51] Decode batch. #running-req: 1, #token: 340, token usage: 0.02, cuda graph: False, gen throughput (token/s): 4.42, #queue-req: 0
[2025-05-14 00:26:51] INFO:     127.0.0.1:39390 - &#34;POST /generate HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>The image shows a person standing on the bumper roof of a taxi, ironseting (grot) laundry while two taxis drive behind him in an urban street setting. This is likely a humorous or staged, unconventional scene, showcasing unexpected publicarroigne</strong></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terminate_process</span><span class="p">(</span><span class="n">server_process</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-05-14 00:26:51] Child process unexpectedly failed with an exit code 9. pid=1656862
</pre></div></div>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../backend/lora.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">LoRA Serving</p>
      </div>
    </a>
    <a class="right-next"
       href="choices_methods.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Choices Methods in SGLang</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Launch-A-Server">Launch A Server</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Basic-Usage">Basic Usage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Multi-turn-Dialog">Multi-turn Dialog</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Control-flow">Control flow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Parallelism">Parallelism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Constrained-Decoding">Constrained Decoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Batching">Batching</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Streaming">Streaming</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Complex-Prompts">Complex Prompts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Multi-modal-Generation">Multi-modal Generation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By SGLang Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
       Copyright 2023-2025, SGLang.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on May 14, 2025.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  
    <!-- RunLLM Widget Script -->
    <script type="module" id="runllm-widget-script" src="https://widget.runllm.com" crossorigin="true" version="stable" runllm-keyboard-shortcut="Mod+j" runllm-name="SGLang Chatbot" runllm-position="BOTTOM_RIGHT" runllm-assistant-id="629" async></script>
    
</body>
</html>