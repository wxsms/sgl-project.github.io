
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>SGLang Native APIs &#8212; SGLang</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
    <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=b2feb39a"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=ccdb6887"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'basic_usage/native_api';</script>
    <link rel="icon" href="../_static/logo.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sampling Parameters" href="sampling_params.html" />
    <link rel="prev" title="Offline Engine API" href="offline_engine_api.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Dec 12, 2025"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="SGLang - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="SGLang - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../get_started/install.html">Install SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Basic Usage</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="send_request.html">Sending Requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="openai_api.html">OpenAI-Compatible APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="offline_engine_api.html">Offline Engine API</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">SGLang Native APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="sampling_params.html">Sampling Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="popular_model_usage.html">Popular Model Usage (DeepSeek, GPT-OSS, GLM, Llama, Qwen, and more)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/server_arguments.html">Server Arguments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/hyperparameter_tuning.html">Hyperparameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/attention_backend.html">Attention Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/speculative_decoding.html">Speculative Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/structured_outputs.html">Structured Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/structured_outputs_for_reasoning_models.html">Structured Outputs For Reasoning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/tool_parser.html">Tool Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/separate_reasoning.html">Reasoning Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/quantized_kv_cache.html">Quantized KV Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/expert_parallelism.html">Expert Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/lora.html">LoRA Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/pd_disaggregation.html">PD Disaggregation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/pipeline_parallelism.html">Pipeline Parallelism for Long Context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/hicache.html">Hierarchical KV Caching (HiCache)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/vlm_query.html">Query Vision Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/dp_for_multi_modal_encoder.html">DP for Multi-Modal Encoder in SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/router.html">SGLang Model Gateway (formerly SGLang Router)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/deterministic_inference.html">Deterministic Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/observability.html">Observability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/checkpoint_engine.html">Checkpoint Engine Integration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supported Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../supported_models/generative_models.html">Large Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/multimodal_language_models.html">Multimodal Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/diffusion_language_models.html">Diffusion Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/embedding_models.html">Embedding Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/reward_models.html">Reward Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/rerank_models.html">Rerank Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/classify_models.html">Classification API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/support_new_models.html">How to Support New Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/transformers_fallback.html">Transformers fallback in SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/modelscope.html">Use Models From ModelScope</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/mindspore_models.html">MindSpore Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Hardware Platforms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../platforms/amd_gpu.html">AMD GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/cpu_server.html">CPU Servers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/tpu.html">TPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/nvidia_jetson.html">NVIDIA Jetson Orin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/ascend_npu_support.html">Ascend NPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/xpu.html">XPU</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/contribution_guide.html">Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/development_guide_using_docker.html">Development Guide Using Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/benchmark_and_profiling.html">Benchmark and Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/bench_serving.html">Bench Serving Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../references/faq.html">Troubleshooting and Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/environment_variables.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/production_metrics.html">Production Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/production_request_trace.html">Production Request Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/multi_node_deployment/multi_node_index.html">Multi-Node Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/custom_chat_template.html">Custom Chat Template</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/frontend/frontend_index.html">Frontend Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/post_training_integration.html">Post-Training Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/learn_more.html">Learn More and Join the Community</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/blob/main/basic_usage/native_api.ipynb?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/edit/main/basic_usage/native_api.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/issues/new?title=Issue%20on%20page%20%2Fbasic_usage/native_api.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/basic_usage/native_api.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>SGLang Native APIs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Launch-A-Server">Launch A Server</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Generate-(text-generation-model)">Generate (text generation model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Get-Model-Info">Get Model Info</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Get-Server-Info">Get Server Info</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Health-Check">Health Check</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Flush-Cache">Flush Cache</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Update-Weights-From-Disk">Update Weights From Disk</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Encode-(embedding-model)">Encode (embedding model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#v1/rerank-(cross-encoder-rerank-model)">v1/rerank (cross encoder rerank model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Classify-(reward-model)">Classify (reward model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Capture-expert-selection-distribution-in-MoE-models">Capture expert selection distribution in MoE models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Tokenize/Detokenize-Example-(Round-Trip)">Tokenize/Detokenize Example (Round Trip)</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <style>
    .output_area.stderr, .output_area.stdout {
        color: #d3d3d3 !important; /* light gray */
    }
</style><section id="SGLang-Native-APIs">
<h1>SGLang Native APIs<a class="headerlink" href="#SGLang-Native-APIs" title="Link to this heading">#</a></h1>
<p>Apart from the OpenAI compatible APIs, the SGLang Runtime also provides its native server APIs. We introduce the following APIs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">/generate</span></code> (text generation model)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/get_model_info</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/get_server_info</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/health</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/health_generate</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/flush_cache</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/update_weights</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/encode</span></code>(embedding model)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/v1/rerank</span></code>(cross encoder rerank model)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/classify</span></code>(reward model)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/start_expert_distribution_record</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/stop_expert_distribution_record</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/dump_expert_distribution_record</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/tokenize</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/detokenize</span></code></p></li>
<li><p>A full list of these APIs can be found at <a class="reference external" href="https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server.py">http_server.py</a></p></li>
</ul>
<p>We mainly use <code class="docutils literal notranslate"><span class="pre">requests</span></code> to test these APIs in the following examples. You can also use <code class="docutils literal notranslate"><span class="pre">curl</span></code>.</p>
<section id="Launch-A-Server">
<h2>Launch A Server<a class="headerlink" href="#Launch-A-Server" title="Link to this heading">#</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sglang.test.doc_patch</span><span class="w"> </span><span class="kn">import</span> <span class="n">launch_server_cmd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">wait_for_server</span><span class="p">,</span> <span class="n">print_highlight</span><span class="p">,</span> <span class="n">terminate_process</span>

<span class="n">server_process</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">launch_server_cmd</span><span class="p">(</span>
    <span class="s2">&quot;python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --host 0.0.0.0 --log-level warning&quot;</span>
<span class="p">)</span>

<span class="n">wait_for_server</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[2025-12-12 18:15:33] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:15:33] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:15:33] INFO utils.py:164: NumExpr defaulting to 16 threads.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-12-12 18:15:38] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:15:38] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:15:38] INFO utils.py:164: NumExpr defaulting to 16 threads.
[2025-12-12 18:15:40] WARNING server_args.py:1416: Attention backend not explicitly specified. Use fa3 backend by default.
[2025-12-12 18:15:46] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:15:46] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:15:46] INFO utils.py:164: NumExpr defaulting to 16 threads.
[2025-12-12 18:15:46] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:15:46] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:15:46] INFO utils.py:164: NumExpr defaulting to 16 threads.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1212 18:15:50.251122 811495 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1212 18:15:50.251194 811495 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.185.21.146 port: 12001
I1212 18:15:50.251227 811495 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.185.21.146:16628
I1212 18:15:50.251319 811495 transfer_engine.cpp:185] Auto-discovering topology...
I1212 18:15:50.253952 811495 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.
I1212 18:15:50.261747 811495 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/
I1212 18:15:50.262434 811495 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:45:5d
I1212 18:15:50.266322 811495 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/
I1212 18:15:50.267134 811495 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:b9:15:92
I1212 18:15:50.271843 811495 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/
I1212 18:15:50.281204 811495 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:c5:5d
I1212 18:15:50.332839 811495 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/
I1212 18:15:50.342022 811495 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:45:5d
I1212 18:15:50.346875 811495 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/
I1212 18:15:50.347522 811495 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:c5:5d
I1212 18:15:50.356146 811495 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/
I1212 18:15:50.356771 811495 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:45:5d
I1212 18:15:50.363265 811495 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/
I1212 18:15:50.363989 811495 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:c5:5d
I1212 18:15:50.368906 811495 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/
I1212 18:15:50.369612 811495 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:45:5d
I1212 18:15:50.374490 811495 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/
I1212 18:15:50.375133 811495 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:c5:5d
W1212 18:15:51.023370 811495 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f014bfff010, len: 2147483648: Operation not permitted [1]
[2025-12-12 18:15:53] Ignore import error when loading sglang.srt.models.mindspore: name &#39;ms&#39; is not defined
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00&lt;?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  5.94it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  5.94it/s]

Capturing batches (bs=1 avail_mem=76.94 GB): 100%|██████████| 3/3 [00:00&lt;00:00,  7.51it/s]
[2025-12-12 18:15:57] Endpoint &#39;/get_model_info&#39; is deprecated and will be removed in a future version. Please use &#39;/model_info&#39; instead.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong></div>
</div>
</section>
<section id="Generate-(text-generation-model)">
<h2>Generate (text generation model)<a class="headerlink" href="#Generate-(text-generation-model)" title="Link to this heading">#</a></h2>
<p>Generate completions. This is similar to the <code class="docutils literal notranslate"><span class="pre">/v1/completions</span></code> in OpenAI API. Detailed parameters can be found in the <a class="reference internal" href="sampling_params.html"><span class="doc">sampling parameters</span></a>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>

<span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/generate&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>{'text': " France's capital is Paris.\nThis entry was shared on 12/11/2020, 00:00\nShare this:\nImagine the population: Paris boasts a population of 772,511, 276, primarily French, with 284,988 immigrants from over 50 countries, and citizens from over 90 countries. However, within its borders, more than 65 percent is French.\nLast year Paris hosted World Cup rugby and soccer teams, and the French language continues to be the main language to speak across the city. It", 'output_ids': [9625, 594, 6722, 374, 12095, 624, 1986, 4343, 572, 6094, 389, 220, 16, 17, 14, 16, 16, 14, 17, 15, 17, 15, 11, 220, 15, 15, 25, 15, 15, 198, 12115, 419, 510, 51057, 279, 7042, 25, 12095, 37019, 264, 7042, 315, 220, 22, 22, 17, 11, 20, 16, 16, 11, 220, 17, 22, 21, 11, 15503, 8585, 11, 448, 220, 17, 23, 19, 11, 24, 23, 23, 19955, 504, 916, 220, 20, 15, 5837, 11, 323, 10283, 504, 916, 220, 24, 15, 5837, 13, 4354, 11, 2878, 1181, 23806, 11, 803, 1091, 220, 21, 20, 3266, 374, 8585, 624, 5842, 1042, 12095, 21009, 4337, 10861, 46450, 323, 22174, 7263, 11, 323, 279, 8585, 4128, 9539, 311, 387, 279, 1887, 4128, 311, 6468, 3941, 279, 3283, 13, 1084], 'meta_info': {'id': '5436c25b59f04863ac2518634258cea8', 'finish_reason': {'type': 'length', 'length': 128}, 'prompt_tokens': 7, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 128, 'cached_tokens': 0, 'e2e_latency': 0.2857825756072998, 'response_sent_to_client_ts': 1765563363.0307488}}</strong></div>
</div>
</section>
<section id="Get-Model-Info">
<h2>Get Model Info<a class="headerlink" href="#Get-Model-Info" title="Link to this heading">#</a></h2>
<p>Get the information of the model.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model_path</span></code>: The path/name of the model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">is_generation</span></code>: Whether the model is used as generation model or embedding model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer_path</span></code>: The path/name of the tokenizer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">preferred_sampling_params</span></code>: The default sampling params specified via <code class="docutils literal notranslate"><span class="pre">--preferred-sampling-params</span></code>. <code class="docutils literal notranslate"><span class="pre">None</span></code> is returned in this example as we did not explicitly configure it in server args.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_version</span></code>: This field contains the version of the model weights. This is often used to track changes or updates to the model’s trained parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">has_image_understanding</span></code>: Whether the model has image-understanding capability.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">has_audio_understanding</span></code>: Whether the model has audio-understanding capability.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_type</span></code>: The model type from the HuggingFace config (e.g., “qwen2”, “llama”).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">architectures</span></code>: The model architectures from the HuggingFace config (e.g., [“Qwen2ForCausalLM”]).</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/get_model_info&quot;</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">response_json</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">response_json</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">response_json</span><span class="p">[</span><span class="s2">&quot;model_path&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;qwen/qwen2.5-0.5b-instruct&quot;</span>
<span class="k">assert</span> <span class="n">response_json</span><span class="p">[</span><span class="s2">&quot;is_generation&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">True</span>
<span class="k">assert</span> <span class="n">response_json</span><span class="p">[</span><span class="s2">&quot;tokenizer_path&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;qwen/qwen2.5-0.5b-instruct&quot;</span>
<span class="k">assert</span> <span class="n">response_json</span><span class="p">[</span><span class="s2">&quot;preferred_sampling_params&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
<span class="k">assert</span> <span class="n">response_json</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="o">==</span> <span class="p">{</span>
    <span class="s2">&quot;model_path&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_generation&quot;</span><span class="p">,</span>
    <span class="s2">&quot;tokenizer_path&quot;</span><span class="p">,</span>
    <span class="s2">&quot;preferred_sampling_params&quot;</span><span class="p">,</span>
    <span class="s2">&quot;weight_version&quot;</span><span class="p">,</span>
    <span class="s2">&quot;has_image_understanding&quot;</span><span class="p">,</span>
    <span class="s2">&quot;has_audio_understanding&quot;</span><span class="p">,</span>
    <span class="s2">&quot;model_type&quot;</span><span class="p">,</span>
    <span class="s2">&quot;architectures&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-12-12 18:16:03] Endpoint &#39;/get_model_info&#39; is deprecated and will be removed in a future version. Please use &#39;/model_info&#39; instead.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>{'model_path': 'qwen/qwen2.5-0.5b-instruct', 'tokenizer_path': 'qwen/qwen2.5-0.5b-instruct', 'is_generation': True, 'preferred_sampling_params': None, 'weight_version': 'default', 'has_image_understanding': False, 'has_audio_understanding': False, 'model_type': 'qwen2', 'architectures': ['Qwen2ForCausalLM']}</strong></div>
</div>
</section>
<section id="Get-Server-Info">
<h2>Get Server Info<a class="headerlink" href="#Get-Server-Info" title="Link to this heading">#</a></h2>
<p>Gets the server information including CLI arguments, token limits, and memory pool sizes.</p>
<ul class="simple">
<li><p>Note: <code class="docutils literal notranslate"><span class="pre">get_server_info</span></code> merges the following deprecated endpoints:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">get_server_args</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_memory_pool_size</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_max_total_num_tokens</span></code></p></li>
</ul>
</li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/get_server_info&quot;</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-12-12 18:16:03] Endpoint &#39;/get_server_info&#39; is deprecated and will be removed in a future version. Please use &#39;/server_info&#39; instead.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>{"model_path":"qwen/qwen2.5-0.5b-instruct","tokenizer_path":"qwen/qwen2.5-0.5b-instruct","tokenizer_mode":"auto","tokenizer_worker_num":1,"skip_tokenizer_init":false,"load_format":"auto","model_loader_extra_config":"{}","rl_quant_profile":null,"trust_remote_code":false,"context_length":null,"is_embedding":false,"enable_multimodal":null,"revision":null,"model_impl":"auto","host":"0.0.0.0","port":35580,"fastapi_root_path":"","grpc_mode":false,"skip_server_warmup":false,"warmups":null,"nccl_port":null,"checkpoint_engine_wait_weights_before_ready":false,"dtype":"auto","quantization":null,"quantization_param_path":null,"kv_cache_dtype":"auto","enable_fp32_lm_head":false,"modelopt_quant":null,"modelopt_checkpoint_restore_path":null,"modelopt_checkpoint_save_path":null,"modelopt_export_path":null,"quantize_and_serve":false,"mem_fraction_static":0.841,"max_running_requests":128,"max_queued_requests":null,"max_total_tokens":20480,"chunked_prefill_size":8192,"enable_dynamic_chunking":false,"max_prefill_tokens":16384,"schedule_policy":"fcfs","enable_priority_scheduling":false,"abort_on_priority_when_disabled":false,"schedule_low_priority_values_first":false,"priority_scheduling_preemption_threshold":10,"schedule_conservativeness":1.0,"page_size":1,"hybrid_kvcache_ratio":null,"swa_full_tokens_ratio":0.8,"disable_hybrid_swa_memory":false,"radix_eviction_policy":"lru","device":"cuda","tp_size":1,"pp_size":1,"pp_max_micro_batch_size":null,"pp_async_batch_depth":0,"stream_interval":1,"stream_output":false,"random_seed":344047147,"constrained_json_whitespace_pattern":null,"constrained_json_disable_any_whitespace":false,"watchdog_timeout":300,"dist_timeout":null,"download_dir":null,"base_gpu_id":0,"gpu_id_step":1,"sleep_on_idle":false,"mm_process_config":{},"log_level":"warning","log_level_http":null,"log_requests":false,"log_requests_level":2,"crash_dump_folder":null,"show_time_cost":false,"enable_metrics":false,"enable_metrics_for_all_schedulers":false,"tokenizer_metrics_custom_labels_header":"x-custom-labels","tokenizer_metrics_allowed_custom_labels":null,"bucket_time_to_first_token":null,"bucket_inter_token_latency":null,"bucket_e2e_request_latency":null,"collect_tokens_histogram":false,"prompt_tokens_buckets":null,"generation_tokens_buckets":null,"gc_warning_threshold_secs":0.0,"decode_log_interval":40,"enable_request_time_stats_logging":false,"kv_events_config":null,"enable_trace":false,"otlp_traces_endpoint":"localhost:4317","export_metrics_to_file":false,"export_metrics_to_file_dir":null,"api_key":null,"served_model_name":"qwen/qwen2.5-0.5b-instruct","weight_version":"default","chat_template":null,"completion_template":null,"file_storage_path":"sglang_storage","enable_cache_report":false,"reasoning_parser":null,"tool_call_parser":null,"tool_server":null,"sampling_defaults":"model","dp_size":1,"load_balance_method":"round_robin","load_watch_interval":0.1,"prefill_round_robin_balance":false,"dist_init_addr":null,"nnodes":1,"node_rank":0,"json_model_override_args":"{}","preferred_sampling_params":null,"enable_lora":null,"max_lora_rank":null,"lora_target_modules":null,"lora_paths":null,"max_loaded_loras":null,"max_loras_per_batch":8,"lora_eviction_policy":"lru","lora_backend":"csgmv","max_lora_chunk_size":16,"attention_backend":"fa3","decode_attention_backend":null,"prefill_attention_backend":null,"sampling_backend":"flashinfer","grammar_backend":"xgrammar","mm_attention_backend":null,"fp8_gemm_runner_backend":"auto","nsa_prefill_backend":"flashmla_sparse","nsa_decode_backend":"fa3","enable_flashinfer_autotune":false,"speculative_algorithm":null,"speculative_draft_model_path":null,"speculative_draft_model_revision":null,"speculative_draft_load_format":null,"speculative_num_steps":null,"speculative_eagle_topk":null,"speculative_num_draft_tokens":null,"speculative_accept_threshold_single":1.0,"speculative_accept_threshold_acc":1.0,"speculative_token_map":null,"speculative_attention_mode":"prefill","speculative_moe_runner_backend":null,"speculative_moe_a2a_backend":null,"speculative_ngram_min_match_window_size":1,"speculative_ngram_max_match_window_size":12,"speculative_ngram_min_bfs_breadth":1,"speculative_ngram_max_bfs_breadth":10,"speculative_ngram_match_type":"BFS","speculative_ngram_branch_length":18,"speculative_ngram_capacity":10000000,"ep_size":1,"moe_a2a_backend":"none","moe_runner_backend":"auto","flashinfer_mxfp4_moe_precision":"default","enable_flashinfer_allreduce_fusion":false,"deepep_mode":"auto","ep_num_redundant_experts":0,"ep_dispatch_algorithm":null,"init_expert_location":"trivial","enable_eplb":false,"eplb_algorithm":"auto","eplb_rebalance_num_iterations":1000,"eplb_rebalance_layers_per_chunk":null,"eplb_min_rebalancing_utilization_threshold":1.0,"expert_distribution_recorder_mode":null,"expert_distribution_recorder_buffer_size":1000,"enable_expert_distribution_metrics":false,"deepep_config":null,"moe_dense_tp_size":null,"elastic_ep_backend":null,"mooncake_ib_device":null,"max_mamba_cache_size":null,"mamba_ssm_dtype":"float32","mamba_full_memory_ratio":0.9,"enable_hierarchical_cache":false,"hicache_ratio":2.0,"hicache_size":0,"hicache_write_policy":"write_through","hicache_io_backend":"kernel","hicache_mem_layout":"layer_first","hicache_storage_backend":null,"hicache_storage_prefetch_policy":"best_effort","hicache_storage_backend_extra_config":null,"enable_lmcache":false,"kt_weight_path":null,"kt_method":"AMXINT4","kt_cpuinfer":null,"kt_threadpool_count":2,"kt_num_gpu_experts":null,"kt_max_deferred_experts_per_token":null,"dllm_algorithm":null,"dllm_algorithm_config":null,"enable_double_sparsity":false,"ds_channel_config_path":null,"ds_heavy_channel_num":32,"ds_heavy_token_num":256,"ds_heavy_channel_type":"qk","ds_sparse_decode_threshold":4096,"cpu_offload_gb":0,"offload_group_size":-1,"offload_num_in_group":1,"offload_prefetch_step":1,"offload_mode":"cpu","multi_item_scoring_delimiter":null,"disable_radix_cache":false,"cuda_graph_max_bs":4,"cuda_graph_bs":[1,2,4],"disable_cuda_graph":false,"disable_cuda_graph_padding":false,"enable_profile_cuda_graph":false,"enable_cudagraph_gc":false,"enable_layerwise_nvtx_marker":false,"enable_nccl_nvls":false,"enable_symm_mem":false,"disable_flashinfer_cutlass_moe_fp4_allgather":false,"enable_tokenizer_batch_encode":false,"disable_tokenizer_batch_decode":false,"disable_outlines_disk_cache":false,"disable_custom_all_reduce":false,"enable_mscclpp":false,"enable_torch_symm_mem":false,"disable_overlap_schedule":false,"enable_mixed_chunk":false,"enable_dp_attention":false,"enable_dp_lm_head":false,"enable_two_batch_overlap":false,"enable_single_batch_overlap":false,"tbo_token_distribution_threshold":0.48,"enable_torch_compile":false,"enable_piecewise_cuda_graph":false,"enable_torch_compile_debug_mode":false,"torch_compile_max_bs":32,"piecewise_cuda_graph_max_tokens":4096,"piecewise_cuda_graph_tokens":[4,8,12,16,20,24,28,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256,288,320,352,384,416,448,480,512,640,768,896,1024,1152,1280,1408,1536,1664,1792,1920,2048,2176,2304,2432,2560,2688,2816,2944,3072,3200,3328,3456,3584,3712,3840,3968,4096],"piecewise_cuda_graph_compiler":"eager","torchao_config":"","enable_nan_detection":false,"enable_p2p_check":false,"triton_attention_reduce_in_fp32":false,"triton_attention_num_kv_splits":8,"triton_attention_split_tile_size":null,"num_continuous_decode_steps":1,"delete_ckpt_after_loading":false,"enable_memory_saver":false,"enable_weights_cpu_backup":false,"enable_draft_weights_cpu_backup":false,"allow_auto_truncate":false,"enable_custom_logit_processor":false,"flashinfer_mla_disable_ragged":false,"disable_shared_experts_fusion":false,"disable_chunked_prefix_cache":false,"disable_fast_image_processor":false,"keep_mm_feature_on_device":false,"enable_return_hidden_states":false,"scheduler_recv_interval":1,"numa_node":null,"enable_deterministic_inference":false,"rl_on_policy_target":null,"enable_attn_tp_input_scattered":false,"enable_nsa_prefill_context_parallel":false,"enable_fused_qk_norm_rope":false,"enable_dynamic_batch_tokenizer":false,"dynamic_batch_tokenizer_batch_size":32,"dynamic_batch_tokenizer_batch_timeout":0.002,"debug_tensor_dump_output_folder":null,"debug_tensor_dump_layers":null,"debug_tensor_dump_input_file":null,"debug_tensor_dump_inject":false,"disaggregation_mode":"null","disaggregation_transfer_backend":"mooncake","disaggregation_bootstrap_port":8998,"disaggregation_decode_tp":null,"disaggregation_decode_dp":null,"disaggregation_prefill_pp":1,"disaggregation_ib_device":null,"disaggregation_decode_enable_offload_kvcache":false,"num_reserved_decode_tokens":512,"disaggregation_decode_polling_interval":1,"custom_weight_loader":[],"weight_loader_disable_mmap":false,"remote_instance_weight_loader_seed_instance_ip":null,"remote_instance_weight_loader_seed_instance_service_port":null,"remote_instance_weight_loader_send_weights_group_ports":null,"remote_instance_weight_loader_backend":"nccl","remote_instance_weight_loader_support_transfer_engine":true,"enable_pdmux":false,"pdmux_config_path":null,"sm_group_num":8,"mm_max_concurrent_calls":32,"mm_per_request_timeout":10.0,"enable_broadcast_mm_inputs_process":false,"decrypted_config_file":null,"decrypted_draft_config_file":null,"mm_enable_dp_encoder":false,"forward_hooks":null,"status":"ready","max_total_num_tokens":20480,"max_req_input_len":20474,"tp_rank":0,"remote_instance_transfer_engine_session_id":"10.185.21.146:16628","remote_instance_transfer_engine_weights_info_dict":{"model.embed_tokens.weight":[139642238140416,136134656,2],"model.layers.0.self_attn.qkv_proj.weight":[139642510770176,1032192,2],"model.layers.0.self_attn.qkv_proj.bias":[139642531741696,1152,2],"model.layers.0.self_attn.o_proj.weight":[139642512834560,802816,2],"model.layers.0.mlp.gate_up_proj.weight":[139655290814464,8716288,2],"model.layers.0.mlp.down_proj.weight":[139655475363840,4358144,2],"model.layers.0.input_layernorm.weight":[139642531744256,896,2],"model.layers.0.post_attention_layernorm.weight":[139642531746304,896,2],"model.layers.1.self_attn.qkv_proj.weight":[139655484080128,1032192,2],"model.layers.1.self_attn.qkv_proj.bias":[139642531748352,1152,2],"model.layers.1.self_attn.o_proj.weight":[139655486144512,802816,2],"model.layers.1.mlp.gate_up_proj.weight":[139655257260032,8716288,2],"model.layers.1.mlp.down_proj.weight":[139642514440192,4358144,2],"model.layers.1.input_layernorm.weight":[139642531750912,896,2],"model.layers.1.post_attention_layernorm.weight":[139642531752960,896,2],"model.layers.2.self_attn.qkv_proj.weight":[139642523156480,1032192,2],"model.layers.2.self_attn.qkv_proj.bias":[139642531755008,1152,2],"model.layers.2.self_attn.o_proj.weight":[139642525220864,802816,2],"model.layers.2.mlp.gate_up_proj.weight":[139655223705600,8716288,2],"model.layers.2.mlp.down_proj.weight":[139655190151168,4358144,2],"model.layers.2.input_layernorm.weight":[139642531757568,896,2],"model.layers.2.post_attention_layernorm.weight":[139642531759616,896,2],"model.layers.3.self_attn.qkv_proj.weight":[139642526826496,1032192,2],"model.layers.3.self_attn.qkv_proj.bias":[139642531761664,1152,2],"model.layers.3.self_attn.o_proj.weight":[139642528890880,802816,2],"model.layers.3.mlp.gate_up_proj.weight":[139655156596736,8716288,2],"model.layers.3.mlp.down_proj.weight":[139655198867456,4358144,2],"model.layers.3.input_layernorm.weight":[139642531764224,896,2],"model.layers.3.post_attention_layernorm.weight":[139642531766272,896,2],"model.layers.4.self_attn.qkv_proj.weight":[139655207583744,1032192,2],"model.layers.4.self_attn.qkv_proj.bias":[139642531768320,1152,2],"model.layers.4.self_attn.o_proj.weight":[139655123042304,802816,2],"model.layers.4.mlp.gate_up_proj.weight":[139655124647936,8716288,2],"model.layers.4.mlp.down_proj.weight":[139655089487872,4358144,2],"model.layers.4.input_layernorm.weight":[139642531770880,896,2],"model.layers.4.post_attention_layernorm.weight":[139642531772928,896,2],"model.layers.5.self_attn.qkv_proj.weight":[139655098204160,1032192,2],"model.layers.5.self_attn.qkv_proj.bias":[139642531774976,1152,2],"model.layers.5.self_attn.o_proj.weight":[139655142080512,802816,2],"model.layers.5.mlp.gate_up_proj.weight":[139655055933440,8716288,2],"model.layers.5.mlp.down_proj.weight":[139655100268544,4358144,2],"model.layers.5.input_layernorm.weight":[139642531777536,896,2],"model.layers.5.post_attention_layernorm.weight":[139642531779584,896,2],"model.layers.6.self_attn.qkv_proj.weight":[139655022379008,1032192,2],"model.layers.6.self_attn.qkv_proj.bias":[139642531781632,1152,2],"model.layers.6.self_attn.o_proj.weight":[139655024443392,802816,2],"model.layers.6.mlp.gate_up_proj.weight":[139654988824576,8716288,2],"model.layers.6.mlp.down_proj.weight":[139655026049024,4358144,2],"model.layers.6.input_layernorm.weight":[139642531784192,896,2],"model.layers.6.post_attention_layernorm.weight":[139642531786240,896,2],"model.layers.7.self_attn.qkv_proj.weight":[139655034765312,1032192,2],"model.layers.7.self_attn.qkv_proj.bias":[139642531788288,1152,2],"model.layers.7.self_attn.o_proj.weight":[139655036829696,802816,2],"model.layers.7.mlp.gate_up_proj.weight":[139654619725824,8716288,2],"model.layers.7.mlp.down_proj.weight":[139654418399232,4358144,2],"model.layers.7.input_layernorm.weight":[139642531790848,896,2],"model.layers.7.post_attention_layernorm.weight":[139642531792896,896,2],"model.layers.8.self_attn.qkv_proj.weight":[139655038435328,1032192,2],"model.layers.8.self_attn.qkv_proj.bias":[139642531794944,1152,2],"model.layers.8.self_attn.o_proj.weight":[139655040499712,802816,2],"model.layers.8.mlp.gate_up_proj.weight":[139654217072640,8716288,2],"model.layers.8.mlp.down_proj.weight":[139654427115520,4358144,2],"model.layers.8.input_layernorm.weight":[139642531797504,896,2],"model.layers.8.post_attention_layernorm.weight":[139642531799552,896,2],"model.layers.9.self_attn.qkv_proj.weight":[139654435831808,1032192,2],"model.layers.9.self_attn.qkv_proj.bias":[139642531801600,1152,2],"model.layers.9.self_attn.o_proj.weight":[139650727411712,802816,2],"model.layers.9.mlp.gate_up_proj.weight":[139650729017344,8716288,2],"model.layers.9.mlp.down_proj.weight":[139650257649664,4358144,2],"model.layers.9.input_layernorm.weight":[139642531804160,896,2],"model.layers.9.post_attention_layernorm.weight":[139642531806208,896,2],"model.layers.10.self_attn.qkv_proj.weight":[139650266365952,1032192,2],"model.layers.10.self_attn.qkv_proj.bias":[139642531808256,1152,2],"model.layers.10.self_attn.o_proj.weight":[139650746449920,802816,2],"model.layers.10.mlp.gate_up_proj.weight":[139649217462272,8716288,2],"model.layers.10.mlp.down_proj.weight":[139650268430336,4358144,2],"model.layers.10.input_layernorm.weight":[139642531810816,896,2],"model.layers.10.post_attention_layernorm.weight":[139642531812864,896,2],"model.layers.11.self_attn.qkv_proj.weight":[139648714145792,1032192,2],"model.layers.11.self_attn.qkv_proj.bias":[139642531814912,1152,2],"model.layers.11.self_attn.o_proj.weight":[139648716210176,802816,2],"model.layers.11.mlp.gate_up_proj.weight":[139645794910208,8716288,2],"model.layers.11.mlp.down_proj.weight":[139648717815808,4358144,2],"model.layers.11.input_layernorm.weight":[139642531817472,896,2],"model.layers.11.post_attention_layernorm.weight":[139642531819520,896,2],"model.layers.12.self_attn.qkv_proj.weight":[139648726532096,1032192,2],"model.layers.12.self_attn.qkv_proj.bias":[139642531821568,1152,2],"model.layers.12.self_attn.o_proj.weight":[139648728596480,802816,2],"model.layers.12.mlp.gate_up_proj.weight":[139641667715072,8716288,2],"model.layers.12.mlp.down_proj.weight":[139641634160640,4358144,2],"model.layers.12.input_layernorm.weight":[139642531824128,896,2],"model.layers.12.post_attention_layernorm.weight":[139642531826176,896,2],"model.layers.13.self_attn.qkv_proj.weight":[139648730202112,1032192,2],"model.layers.13.self_attn.qkv_proj.bias":[139642531828224,1152,2],"model.layers.13.self_attn.o_proj.weight":[139648732266496,802816,2],"model.layers.13.mlp.gate_up_proj.weight":[139641600606208,8716288,2],"model.layers.13.mlp.down_proj.weight":[139641642876928,4358144,2],"model.layers.13.input_layernorm.weight":[139642531830784,896,2],"model.layers.13.post_attention_layernorm.weight":[139642531832832,896,2],"model.layers.14.self_attn.qkv_proj.weight":[139641651593216,1032192,2],"model.layers.14.self_attn.qkv_proj.bias":[139642531834880,1152,2],"model.layers.14.self_attn.o_proj.weight":[139641567051776,802816,2],"model.layers.14.mlp.gate_up_proj.weight":[139641568657408,8716288,2],"model.layers.14.mlp.down_proj.weight":[139641533497344,4358144,2],"model.layers.14.input_layernorm.weight":[139642531837440,896,2],"model.layers.14.post_attention_layernorm.weight":[139642531839488,896,2],"model.layers.15.self_attn.qkv_proj.weight":[139641542213632,1032192,2],"model.layers.15.self_attn.qkv_proj.bias":[139642531841536,1152,2],"model.layers.15.self_attn.o_proj.weight":[139641586089984,802816,2],"model.layers.15.mlp.gate_up_proj.weight":[139641499942912,8716288,2],"model.layers.15.mlp.down_proj.weight":[139641544278016,4358144,2],"model.layers.15.input_layernorm.weight":[139642531844096,896,2],"model.layers.15.post_attention_layernorm.weight":[139642531846144,896,2],"model.layers.16.self_attn.qkv_proj.weight":[139641466388480,1032192,2],"model.layers.16.self_attn.qkv_proj.bias":[139642531848192,1152,2],"model.layers.16.self_attn.o_proj.weight":[139641468452864,802816,2],"model.layers.16.mlp.gate_up_proj.weight":[139641432834048,8716288,2],"model.layers.16.mlp.down_proj.weight":[139641470058496,4358144,2],"model.layers.16.input_layernorm.weight":[139642531850752,896,2],"model.layers.16.post_attention_layernorm.weight":[139642531852800,896,2],"model.layers.17.self_attn.qkv_proj.weight":[139641478774784,1032192,2],"model.layers.17.self_attn.qkv_proj.bias":[139642531854848,1152,2],"model.layers.17.self_attn.o_proj.weight":[139641480839168,802816,2],"model.layers.17.mlp.gate_up_proj.weight":[139641399279616,8716288,2],"model.layers.17.mlp.down_proj.weight":[139641365725184,4358144,2],"model.layers.17.input_layernorm.weight":[139642531857408,896,2],"model.layers.17.post_attention_layernorm.weight":[139642531859456,896,2],"model.layers.18.self_attn.qkv_proj.weight":[139641482444800,1032192,2],"model.layers.18.self_attn.qkv_proj.bias":[139642531861504,1152,2],"model.layers.18.self_attn.o_proj.weight":[139641484509184,802816,2],"model.layers.18.mlp.gate_up_proj.weight":[139641332170752,8716288,2],"model.layers.18.mlp.down_proj.weight":[139641374441472,4358144,2],"model.layers.18.input_layernorm.weight":[139642531864064,896,2],"model.layers.18.post_attention_layernorm.weight":[139642531866112,896,2],"model.layers.19.self_attn.qkv_proj.weight":[139641383157760,1032192,2],"model.layers.19.self_attn.qkv_proj.bias":[139642531868160,1152,2],"model.layers.19.self_attn.o_proj.weight":[139641298616320,802816,2],"model.layers.19.mlp.gate_up_proj.weight":[139641300221952,8716288,2],"model.layers.19.mlp.down_proj.weight":[139641265061888,4358144,2],"model.layers.19.input_layernorm.weight":[139642531870720,896,2],"model.layers.19.post_attention_layernorm.weight":[139642531872768,896,2],"model.layers.20.self_attn.qkv_proj.weight":[139641273778176,1032192,2],"model.layers.20.self_attn.qkv_proj.bias":[139642531874816,1152,2],"model.layers.20.self_attn.o_proj.weight":[139641317654528,802816,2],"model.layers.20.mlp.gate_up_proj.weight":[139641231507456,8716288,2],"model.layers.20.mlp.down_proj.weight":[139641275842560,4358144,2],"model.layers.20.input_layernorm.weight":[139642531877376,896,2],"model.layers.20.post_attention_layernorm.weight":[139642531879424,896,2],"model.layers.21.self_attn.qkv_proj.weight":[139641197953024,1032192,2],"model.layers.21.self_attn.qkv_proj.bias":[139642531881472,1152,2],"model.layers.21.self_attn.o_proj.weight":[139641200017408,802816,2],"model.layers.21.mlp.gate_up_proj.weight":[139641164398592,8716288,2],"model.layers.21.mlp.down_proj.weight":[139641201623040,4358144,2],"model.layers.21.input_layernorm.weight":[139642531884032,896,2],"model.layers.21.post_attention_layernorm.weight":[139642531886080,896,2],"model.layers.22.self_attn.qkv_proj.weight":[139641210339328,1032192,2],"model.layers.22.self_attn.qkv_proj.bias":[139642531888128,1152,2],"model.layers.22.self_attn.o_proj.weight":[139641212403712,802816,2],"model.layers.22.mlp.gate_up_proj.weight":[139641130844160,8716288,2],"model.layers.22.mlp.down_proj.weight":[139641097289728,4358144,2],"model.layers.22.input_layernorm.weight":[139642531890688,896,2],"model.layers.22.post_attention_layernorm.weight":[139642531892736,896,2],"model.layers.23.self_attn.qkv_proj.weight":[139641214009344,1032192,2],"model.layers.23.self_attn.qkv_proj.bias":[139642531894784,1152,2],"model.layers.23.self_attn.o_proj.weight":[139641216073728,802816,2],"model.layers.23.mlp.gate_up_proj.weight":[139641063735296,8716288,2],"model.layers.23.mlp.down_proj.weight":[139641106006016,4358144,2],"model.layers.23.input_layernorm.weight":[139642531897344,896,2],"model.layers.23.post_attention_layernorm.weight":[139642531899392,896,2],"model.norm.weight":[139642531901440,896,2]},"internal_states":[{"model_path":"qwen/qwen2.5-0.5b-instruct","tokenizer_path":"qwen/qwen2.5-0.5b-instruct","tokenizer_mode":"auto","tokenizer_worker_num":1,"skip_tokenizer_init":false,"load_format":"auto","model_loader_extra_config":"{}","rl_quant_profile":null,"trust_remote_code":false,"context_length":null,"is_embedding":false,"enable_multimodal":null,"revision":null,"model_impl":"auto","host":"0.0.0.0","port":35580,"fastapi_root_path":"","grpc_mode":false,"skip_server_warmup":false,"warmups":null,"nccl_port":null,"checkpoint_engine_wait_weights_before_ready":false,"dtype":"auto","quantization":null,"quantization_param_path":null,"kv_cache_dtype":"auto","enable_fp32_lm_head":false,"modelopt_quant":null,"modelopt_checkpoint_restore_path":null,"modelopt_checkpoint_save_path":null,"modelopt_export_path":null,"quantize_and_serve":false,"mem_fraction_static":0.841,"max_running_requests":128,"max_queued_requests":null,"max_total_tokens":20480,"chunked_prefill_size":8192,"enable_dynamic_chunking":false,"max_prefill_tokens":16384,"schedule_policy":"fcfs","enable_priority_scheduling":false,"abort_on_priority_when_disabled":false,"schedule_low_priority_values_first":false,"priority_scheduling_preemption_threshold":10,"schedule_conservativeness":1.0,"page_size":1,"hybrid_kvcache_ratio":null,"swa_full_tokens_ratio":0.8,"disable_hybrid_swa_memory":false,"radix_eviction_policy":"lru","device":"cuda","tp_size":1,"pp_size":1,"pp_max_micro_batch_size":128,"pp_async_batch_depth":0,"stream_interval":1,"stream_output":false,"random_seed":344047147,"constrained_json_whitespace_pattern":null,"constrained_json_disable_any_whitespace":false,"watchdog_timeout":300,"dist_timeout":null,"download_dir":null,"base_gpu_id":0,"gpu_id_step":1,"sleep_on_idle":false,"mm_process_config":{},"log_level":"warning","log_level_http":null,"log_requests":false,"log_requests_level":2,"crash_dump_folder":null,"show_time_cost":false,"enable_metrics":false,"enable_metrics_for_all_schedulers":false,"tokenizer_metrics_custom_labels_header":"x-custom-labels","tokenizer_metrics_allowed_custom_labels":null,"bucket_time_to_first_token":null,"bucket_inter_token_latency":null,"bucket_e2e_request_latency":null,"collect_tokens_histogram":false,"prompt_tokens_buckets":null,"generation_tokens_buckets":null,"gc_warning_threshold_secs":0.0,"decode_log_interval":40,"enable_request_time_stats_logging":false,"kv_events_config":null,"enable_trace":false,"otlp_traces_endpoint":"localhost:4317","export_metrics_to_file":false,"export_metrics_to_file_dir":null,"api_key":null,"served_model_name":"qwen/qwen2.5-0.5b-instruct","weight_version":"default","chat_template":null,"completion_template":null,"file_storage_path":"sglang_storage","enable_cache_report":false,"reasoning_parser":null,"tool_call_parser":null,"tool_server":null,"sampling_defaults":"model","dp_size":1,"load_balance_method":"round_robin","load_watch_interval":0.1,"prefill_round_robin_balance":false,"dist_init_addr":null,"nnodes":1,"node_rank":0,"json_model_override_args":"{}","preferred_sampling_params":null,"enable_lora":null,"max_lora_rank":null,"lora_target_modules":null,"lora_paths":null,"max_loaded_loras":null,"max_loras_per_batch":8,"lora_eviction_policy":"lru","lora_backend":"csgmv","max_lora_chunk_size":16,"attention_backend":"fa3","decode_attention_backend":"fa3","prefill_attention_backend":"fa3","sampling_backend":"flashinfer","grammar_backend":"xgrammar","mm_attention_backend":null,"fp8_gemm_runner_backend":"auto","nsa_prefill_backend":"flashmla_sparse","nsa_decode_backend":"fa3","enable_flashinfer_autotune":false,"speculative_algorithm":null,"speculative_draft_model_path":null,"speculative_draft_model_revision":null,"speculative_draft_load_format":null,"speculative_num_steps":null,"speculative_eagle_topk":null,"speculative_num_draft_tokens":null,"speculative_accept_threshold_single":1.0,"speculative_accept_threshold_acc":1.0,"speculative_token_map":null,"speculative_attention_mode":"prefill","speculative_moe_runner_backend":null,"speculative_moe_a2a_backend":null,"speculative_ngram_min_match_window_size":1,"speculative_ngram_max_match_window_size":12,"speculative_ngram_min_bfs_breadth":1,"speculative_ngram_max_bfs_breadth":10,"speculative_ngram_match_type":"BFS","speculative_ngram_branch_length":18,"speculative_ngram_capacity":10000000,"ep_size":1,"moe_a2a_backend":"none","moe_runner_backend":"auto","flashinfer_mxfp4_moe_precision":"default","enable_flashinfer_allreduce_fusion":false,"deepep_mode":"auto","ep_num_redundant_experts":0,"ep_dispatch_algorithm":null,"init_expert_location":"trivial","enable_eplb":false,"eplb_algorithm":"auto","eplb_rebalance_num_iterations":1000,"eplb_rebalance_layers_per_chunk":null,"eplb_min_rebalancing_utilization_threshold":1.0,"expert_distribution_recorder_mode":null,"expert_distribution_recorder_buffer_size":1000,"enable_expert_distribution_metrics":false,"deepep_config":null,"moe_dense_tp_size":null,"elastic_ep_backend":null,"mooncake_ib_device":null,"max_mamba_cache_size":null,"mamba_ssm_dtype":"float32","mamba_full_memory_ratio":0.9,"enable_hierarchical_cache":false,"hicache_ratio":2.0,"hicache_size":0,"hicache_write_policy":"write_through","hicache_io_backend":"kernel","hicache_mem_layout":"layer_first","hicache_storage_backend":null,"hicache_storage_prefetch_policy":"best_effort","hicache_storage_backend_extra_config":null,"enable_lmcache":false,"kt_weight_path":null,"kt_method":"AMXINT4","kt_cpuinfer":null,"kt_threadpool_count":2,"kt_num_gpu_experts":null,"kt_max_deferred_experts_per_token":null,"dllm_algorithm":null,"dllm_algorithm_config":null,"enable_double_sparsity":false,"ds_channel_config_path":null,"ds_heavy_channel_num":32,"ds_heavy_token_num":256,"ds_heavy_channel_type":"qk","ds_sparse_decode_threshold":4096,"cpu_offload_gb":0,"offload_group_size":-1,"offload_num_in_group":1,"offload_prefetch_step":1,"offload_mode":"cpu","multi_item_scoring_delimiter":null,"disable_radix_cache":false,"cuda_graph_max_bs":4,"cuda_graph_bs":[1,2,4],"disable_cuda_graph":false,"disable_cuda_graph_padding":false,"enable_profile_cuda_graph":false,"enable_cudagraph_gc":false,"enable_layerwise_nvtx_marker":false,"enable_nccl_nvls":false,"enable_symm_mem":false,"disable_flashinfer_cutlass_moe_fp4_allgather":false,"enable_tokenizer_batch_encode":false,"disable_tokenizer_batch_decode":false,"disable_outlines_disk_cache":false,"disable_custom_all_reduce":false,"enable_mscclpp":false,"enable_torch_symm_mem":false,"disable_overlap_schedule":false,"enable_mixed_chunk":false,"enable_dp_attention":false,"enable_dp_lm_head":false,"enable_two_batch_overlap":false,"enable_single_batch_overlap":false,"tbo_token_distribution_threshold":0.48,"enable_torch_compile":false,"enable_piecewise_cuda_graph":false,"enable_torch_compile_debug_mode":false,"torch_compile_max_bs":32,"piecewise_cuda_graph_max_tokens":4096,"piecewise_cuda_graph_tokens":[4,8,12,16,20,24,28,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256,288,320,352,384,416,448,480,512,640,768,896,1024,1152,1280,1408,1536,1664,1792,1920,2048,2176,2304,2432,2560,2688,2816,2944,3072,3200,3328,3456,3584,3712,3840,3968,4096],"piecewise_cuda_graph_compiler":"eager","torchao_config":"","enable_nan_detection":false,"enable_p2p_check":false,"triton_attention_reduce_in_fp32":false,"triton_attention_num_kv_splits":8,"triton_attention_split_tile_size":null,"num_continuous_decode_steps":1,"delete_ckpt_after_loading":false,"enable_memory_saver":false,"enable_weights_cpu_backup":false,"enable_draft_weights_cpu_backup":false,"allow_auto_truncate":false,"enable_custom_logit_processor":false,"flashinfer_mla_disable_ragged":false,"disable_shared_experts_fusion":false,"disable_chunked_prefix_cache":true,"disable_fast_image_processor":false,"keep_mm_feature_on_device":false,"enable_return_hidden_states":false,"scheduler_recv_interval":1,"numa_node":null,"enable_deterministic_inference":false,"rl_on_policy_target":null,"enable_attn_tp_input_scattered":false,"enable_nsa_prefill_context_parallel":false,"enable_fused_qk_norm_rope":false,"enable_dynamic_batch_tokenizer":false,"dynamic_batch_tokenizer_batch_size":32,"dynamic_batch_tokenizer_batch_timeout":0.002,"debug_tensor_dump_output_folder":null,"debug_tensor_dump_layers":null,"debug_tensor_dump_input_file":null,"debug_tensor_dump_inject":false,"disaggregation_mode":"null","disaggregation_transfer_backend":"mooncake","disaggregation_bootstrap_port":8998,"disaggregation_decode_tp":null,"disaggregation_decode_dp":null,"disaggregation_prefill_pp":1,"disaggregation_ib_device":null,"disaggregation_decode_enable_offload_kvcache":false,"num_reserved_decode_tokens":512,"disaggregation_decode_polling_interval":1,"custom_weight_loader":[],"weight_loader_disable_mmap":false,"remote_instance_weight_loader_seed_instance_ip":null,"remote_instance_weight_loader_seed_instance_service_port":null,"remote_instance_weight_loader_send_weights_group_ports":null,"remote_instance_weight_loader_backend":"nccl","remote_instance_weight_loader_support_transfer_engine":true,"enable_pdmux":false,"pdmux_config_path":null,"sm_group_num":8,"mm_max_concurrent_calls":32,"mm_per_request_timeout":10.0,"enable_broadcast_mm_inputs_process":false,"decrypted_config_file":null,"decrypted_draft_config_file":null,"mm_enable_dp_encoder":false,"forward_hooks":null,"use_mla_backend":false,"last_gen_throughput":729.3875249804393,"memory_usage":{"weight":1.06,"kvcache":0.23,"token_capacity":20480,"graph":0.09}}],"version":"0.5.6.post2"}</strong></div>
</div>
</section>
<section id="Health-Check">
<h2>Health Check<a class="headerlink" href="#Health-Check" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">/health</span></code>: Check the health of the server.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/health_generate</span></code>: Check the health of the server by generating one token.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/health_generate&quot;</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'></strong></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/health&quot;</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'></strong></div>
</div>
</section>
<section id="Flush-Cache">
<h2>Flush Cache<a class="headerlink" href="#Flush-Cache" title="Link to this heading">#</a></h2>
<p>Flush the radix cache. It will be automatically triggered when the model weights are updated by the <code class="docutils literal notranslate"><span class="pre">/update_weights</span></code> API.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/flush_cache&quot;</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Cache flushed.<br>Please check backend logs for more details. (When there are running or waiting requests, the operation will not be performed.)<br></strong></div>
</div>
</section>
<section id="Update-Weights-From-Disk">
<h2>Update Weights From Disk<a class="headerlink" href="#Update-Weights-From-Disk" title="Link to this heading">#</a></h2>
<p>Update model weights from disk without restarting the server. Only applicable for models with the same architecture and parameter size.</p>
<p>SGLang support <code class="docutils literal notranslate"><span class="pre">update_weights_from_disk</span></code> API for continuous evaluation during training (save checkpoint to disk and update weights from disk).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># successful update with same architecture and size</span>

<span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/update_weights_from_disk&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model_path&quot;</span><span class="p">:</span> <span class="s2">&quot;qwen/qwen2.5-0.5b-instruct&quot;</span><span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s2">&quot;success&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">True</span>
<span class="k">assert</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s2">&quot;message&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Succeeded to update model weights.&quot;</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00&lt;?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  6.17it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  6.16it/s]

</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>{"success":true,"message":"Succeeded to update model weights.","num_paused_requests":0}</strong></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># failed update with different parameter size or wrong name</span>

<span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/update_weights_from_disk&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model_path&quot;</span><span class="p">:</span> <span class="s2">&quot;qwen/qwen2.5-0.5b-instruct-wrong&quot;</span><span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">response_json</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">response_json</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">response_json</span><span class="p">[</span><span class="s2">&quot;success&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">False</span>
<span class="k">assert</span> <span class="n">response_json</span><span class="p">[</span><span class="s2">&quot;message&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span>
    <span class="s2">&quot;Failed to get weights iterator: &quot;</span>
    <span class="s2">&quot;qwen/qwen2.5-0.5b-instruct-wrong&quot;</span>
    <span class="s2">&quot; (repository not found).&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-12-12 18:16:05] Failed to get weights iterator: qwen/qwen2.5-0.5b-instruct-wrong (repository not found).
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>{'success': False, 'message': 'Failed to get weights iterator: qwen/qwen2.5-0.5b-instruct-wrong (repository not found).', 'num_paused_requests': 0}</strong></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terminate_process</span><span class="p">(</span><span class="n">server_process</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Encode-(embedding-model)">
<h2>Encode (embedding model)<a class="headerlink" href="#Encode-(embedding-model)" title="Link to this heading">#</a></h2>
<p>Encode text into embeddings. Note that this API is only available for <a class="reference internal" href="openai_api_embeddings.html"><span class="doc">embedding models</span></a> and will raise an error for generation models. Therefore, we launch a new server to server an embedding model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_process</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">launch_server_cmd</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">python3 -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-1.5B-instruct \</span>
<span class="sd">    --host 0.0.0.0 --is-embedding --log-level warning</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>

<span class="n">wait_for_server</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-12-12 18:16:10] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:16:10] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:16:10] INFO utils.py:164: NumExpr defaulting to 16 threads.
[2025-12-12 18:16:13] INFO model_config.py:913: Downcasting torch.float32 to torch.float16.
[2025-12-12 18:16:13] WARNING server_args.py:1416: Attention backend not explicitly specified. Use fa3 backend by default.
[2025-12-12 18:16:19] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:16:19] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:16:19] INFO utils.py:164: NumExpr defaulting to 16 threads.
[2025-12-12 18:16:19] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:16:19] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:16:19] INFO utils.py:164: NumExpr defaulting to 16 threads.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1212 18:16:22.631659 812962 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1212 18:16:22.631772 812962 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.185.21.146 port: 12001
I1212 18:16:22.631809 812962 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.185.21.146:16769
I1212 18:16:22.640211 812962 transfer_engine.cpp:185] Auto-discovering topology...
I1212 18:16:22.642956 812962 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.
I1212 18:16:22.668047 812962 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/
I1212 18:16:22.668751 812962 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:45:5d
I1212 18:16:22.671787 812962 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/
I1212 18:16:22.676601 812962 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:b9:15:92
I1212 18:16:22.684553 812962 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/
I1212 18:16:22.685204 812962 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:c5:5d
I1212 18:16:22.726600 812962 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/
I1212 18:16:22.727298 812962 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:45:5d
I1212 18:16:22.731523 812962 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/
I1212 18:16:22.732177 812962 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:c5:5d
I1212 18:16:22.736258 812962 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/
I1212 18:16:22.736920 812962 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:45:5d
I1212 18:16:22.773092 812962 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/
I1212 18:16:22.776827 812962 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:c5:5d
I1212 18:16:22.792483 812962 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/
I1212 18:16:22.793181 812962 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:45:5d
I1212 18:16:22.830617 812962 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/
I1212 18:16:22.838433 812962 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:c5:5d
W1212 18:16:23.555564 812962 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f9c43fff010, len: 2147483648: Operation not permitted [1]
[2025-12-12 18:16:25] Ignore import error when loading sglang.srt.models.mindspore: name &#39;ms&#39; is not defined
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00&lt;?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01&lt;00:01,  1.19s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02&lt;00:00,  1.07s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02&lt;00:00,  1.09s/it]

[2025-12-12 18:16:30] Endpoint &#39;/get_model_info&#39; is deprecated and will be removed in a future version. Please use &#39;/model_info&#39; instead.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># successful encode for embedding model</span>

<span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/encode&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;Alibaba-NLP/gte-Qwen2-1.5B-instruct&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Once upon a time&quot;</span><span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">response_json</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text embedding (first 10): </span><span class="si">{</span><span class="n">response_json</span><span class="p">[</span><span class="s1">&#39;embedding&#39;</span><span class="p">][:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Text embedding (first 10): [-0.00023102760314941406, -0.04986572265625, -0.0032711029052734375, 0.011077880859375, -0.0140533447265625, 0.0159912109375, -0.01441192626953125, 0.0059051513671875, -0.0228424072265625, 0.0272979736328125]</strong></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terminate_process</span><span class="p">(</span><span class="n">embedding_process</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="v1/rerank-(cross-encoder-rerank-model)">
<h2>v1/rerank (cross encoder rerank model)<a class="headerlink" href="#v1/rerank-(cross-encoder-rerank-model)" title="Link to this heading">#</a></h2>
<p>Rerank a list of documents given a query using a cross-encoder model. Note that this API is only available for cross encoder model like <a class="reference external" href="https://huggingface.co/BAAI/bge-reranker-v2-m3">BAAI/bge-reranker-v2-m3</a> with <code class="docutils literal notranslate"><span class="pre">attention-backend</span></code> <code class="docutils literal notranslate"><span class="pre">triton</span></code> and <code class="docutils literal notranslate"><span class="pre">torch_native</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reranker_process</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">launch_server_cmd</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">python3 -m sglang.launch_server --model-path BAAI/bge-reranker-v2-m3 \</span>
<span class="sd">    --host 0.0.0.0 --disable-radix-cache --chunked-prefill-size -1 --attention-backend triton --is-embedding --log-level warning</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>

<span class="n">wait_for_server</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-12-12 18:16:40] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:16:40] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:16:40] INFO utils.py:164: NumExpr defaulting to 16 threads.
[2025-12-12 18:16:42] INFO model_config.py:913: Downcasting torch.float32 to torch.float16.
[2025-12-12 18:16:48] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:16:48] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:16:48] INFO utils.py:164: NumExpr defaulting to 16 threads.
[2025-12-12 18:16:48] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:16:48] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:16:48] INFO utils.py:164: NumExpr defaulting to 16 threads.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1212 18:16:51.271759 814158 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1212 18:16:51.271775 814158 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.185.21.146 port: 12001
I1212 18:16:51.271795 814158 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.185.21.146:15147
I1212 18:16:51.271868 814158 transfer_engine.cpp:185] Auto-discovering topology...
I1212 18:16:51.274756 814158 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.
I1212 18:16:51.301571 814158 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/
I1212 18:16:51.302413 814158 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:45:5d
I1212 18:16:51.329919 814158 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/
I1212 18:16:51.330977 814158 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:b9:15:92
I1212 18:16:51.353550 814158 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/
I1212 18:16:51.354349 814158 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:c5:5d
I1212 18:16:51.442592 814158 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/
I1212 18:16:51.443434 814158 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:45:5d
I1212 18:16:51.469790 814158 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/
I1212 18:16:51.470692 814158 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:c5:5d
I1212 18:16:51.486897 814158 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/
I1212 18:16:51.498601 814158 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:45:5d
I1212 18:16:51.555081 814158 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/
I1212 18:16:51.557013 814158 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:c5:5d
I1212 18:16:51.561529 814158 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/
I1212 18:16:51.562299 814158 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:45:5d
I1212 18:16:51.566623 814158 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/
I1212 18:16:51.567382 814158 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:c5:5d
W1212 18:16:52.203954 814158 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f7283fff010, len: 2147483648: Operation not permitted [1]
[2025-12-12 18:16:54] Ignore import error when loading sglang.srt.models.mindspore: name &#39;ms&#39; is not defined
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00&lt;?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  1.11it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  1.11it/s]

[2025-12-12 18:16:59] Endpoint &#39;/get_model_info&#39; is deprecated and will be removed in a future version. Please use &#39;/model_info&#39; instead.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compute rerank scores for query and documents</span>

<span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/v1/rerank&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;BAAI/bge-reranker-v2-m3&quot;</span><span class="p">,</span>
    <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;what is panda?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;hi&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.&quot;</span><span class="p">,</span>
    <span class="p">],</span>
<span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">response_json</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">response_json</span><span class="p">:</span>
    <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Score: </span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;score&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> - Document: &#39;</span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;document&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Score: 5.26 - Document: 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Score: -8.19 - Document: 'hi'</strong></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terminate_process</span><span class="p">(</span><span class="n">reranker_process</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Classify-(reward-model)">
<h2>Classify (reward model)<a class="headerlink" href="#Classify-(reward-model)" title="Link to this heading">#</a></h2>
<p>SGLang Runtime also supports reward models. Here we use a reward model to classify the quality of pairwise generations.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note that SGLang now treats embedding models and reward models as the same type of models.</span>
<span class="c1"># This will be updated in the future.</span>

<span class="n">reward_process</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">launch_server_cmd</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">python3 -m sglang.launch_server --model-path Skywork/Skywork-Reward-Llama-3.1-8B-v0.2 --host 0.0.0.0 --is-embedding --log-level warning</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>

<span class="n">wait_for_server</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-12-12 18:17:09] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:17:09] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:17:09] INFO utils.py:164: NumExpr defaulting to 16 threads.
[2025-12-12 18:17:11] WARNING server_args.py:1416: Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-12-12 18:17:16] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:17:16] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:17:16] INFO utils.py:164: NumExpr defaulting to 16 threads.
[2025-12-12 18:17:17] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:17:17] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:17:17] INFO utils.py:164: NumExpr defaulting to 16 threads.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1212 18:17:20.232815 814939 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1212 18:17:20.232832 814939 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.185.21.146 port: 12001
I1212 18:17:20.232854 814939 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.185.21.146:16623
I1212 18:17:20.232939 814939 transfer_engine.cpp:185] Auto-discovering topology...
I1212 18:17:20.235797 814939 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.
I1212 18:17:20.261883 814939 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/
I1212 18:17:20.262552 814939 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:45:5d
I1212 18:17:20.290088 814939 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/
I1212 18:17:20.306588 814939 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:b9:15:92
I1212 18:17:20.329679 814939 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/
I1212 18:17:20.330403 814939 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:c5:5d
I1212 18:17:20.422582 814939 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/
I1212 18:17:20.423278 814939 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:45:5d
I1212 18:17:20.449571 814939 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/
I1212 18:17:20.450213 814939 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:c5:5d
I1212 18:17:20.477519 814939 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/
I1212 18:17:20.478164 814939 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:45:5d
I1212 18:17:20.505539 814939 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/
I1212 18:17:20.506206 814939 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:c5:5d
I1212 18:17:20.533499 814939 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/
I1212 18:17:20.534137 814939 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:45:5d
I1212 18:17:20.562512 814939 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/
I1212 18:17:20.563156 814939 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:c5:5d
W1212 18:17:21.476490 814939 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f01a7fff010, len: 2147483648: Operation not permitted [1]
[2025-12-12 18:17:23] Ignore import error when loading sglang.srt.models.mindspore: name &#39;ms&#39; is not defined
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00&lt;?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02&lt;00:07,  2.61s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05&lt;00:05,  2.80s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08&lt;00:02,  2.67s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08&lt;00:00,  1.73s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08&lt;00:00,  2.09s/it]

[2025-12-12 18:17:35] Endpoint &#39;/get_model_info&#39; is deprecated and will be removed in a future version. Please use &#39;/model_info&#39; instead.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">PROMPT</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;What is the range of the numeric output of a sigmoid node in a neural network?&quot;</span>
<span class="p">)</span>

<span class="n">RESPONSE1</span> <span class="o">=</span> <span class="s2">&quot;The output of a sigmoid node is bounded between -1 and 1.&quot;</span>
<span class="n">RESPONSE2</span> <span class="o">=</span> <span class="s2">&quot;The output of a sigmoid node is bounded between 0 and 1.&quot;</span>

<span class="n">CONVS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">PROMPT</span><span class="p">},</span> <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">RESPONSE1</span><span class="p">}],</span>
    <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">PROMPT</span><span class="p">},</span> <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">RESPONSE2</span><span class="p">}],</span>
<span class="p">]</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Skywork/Skywork-Reward-Llama-3.1-8B-v0.2&quot;</span><span class="p">)</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">CONVS</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/classify&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;Skywork/Skywork-Reward-Llama-3.1-8B-v0.2&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">prompts</span><span class="p">}</span>

<span class="n">responses</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
<span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">responses</span><span class="p">:</span>
    <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;reward: </span><span class="si">{</span><span class="n">response</span><span class="p">[</span><span class="s1">&#39;embedding&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>reward: -24.125</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>reward: 1.0703125</strong></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terminate_process</span><span class="p">(</span><span class="n">reward_process</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Capture-expert-selection-distribution-in-MoE-models">
<h2>Capture expert selection distribution in MoE models<a class="headerlink" href="#Capture-expert-selection-distribution-in-MoE-models" title="Link to this heading">#</a></h2>
<p>SGLang Runtime supports recording the number of times an expert is selected in a MoE model run for each expert in the model. This is useful when analyzing the throughput of the model and plan for optimization.</p>
<p><em>Note: We only print out the first 10 lines of the csv below for better readability. Please adjust accordingly if you want to analyze the results more deeply.</em></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">expert_record_server_process</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">launch_server_cmd</span><span class="p">(</span>
    <span class="s2">&quot;python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0 --expert-distribution-recorder-mode stat --log-level warning&quot;</span>
<span class="p">)</span>

<span class="n">wait_for_server</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-12-12 18:17:46] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:17:46] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:17:46] INFO utils.py:164: NumExpr defaulting to 16 threads.
[2025-12-12 18:17:48] WARNING server_args.py:1416: Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-12-12 18:17:54] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:17:54] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:17:54] INFO utils.py:164: NumExpr defaulting to 16 threads.
[2025-12-12 18:17:54] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:17:54] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:17:54] INFO utils.py:164: NumExpr defaulting to 16 threads.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1212 18:17:57.909489 815573 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1212 18:17:57.909511 815573 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.185.21.146 port: 12001
I1212 18:17:57.909534 815573 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.185.21.146:16316
I1212 18:17:57.909613 815573 transfer_engine.cpp:185] Auto-discovering topology...
I1212 18:17:57.912387 815573 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.
I1212 18:17:57.937692 815573 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/
I1212 18:17:57.938418 815573 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:45:5d
I1212 18:17:57.966089 815573 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/
I1212 18:17:57.966940 815573 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:b9:15:92
I1212 18:17:57.993615 815573 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/
I1212 18:17:57.994262 815573 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:c5:5d
I1212 18:17:58.322602 815573 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/
I1212 18:17:58.323370 815573 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:45:5d
I1212 18:17:58.349644 815573 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/
I1212 18:17:58.350297 815573 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:c5:5d
I1212 18:17:58.355019 815573 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/
I1212 18:17:58.355661 815573 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:45:5d
I1212 18:17:58.381548 815573 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/
I1212 18:17:58.382200 815573 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:c5:5d
I1212 18:17:58.409847 815573 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/
I1212 18:17:58.410501 815573 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:45:5d
I1212 18:17:58.437559 815573 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/
I1212 18:17:58.438212 815573 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:c5:5d
W1212 18:17:59.193955 815573 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f2db3fff010, len: 2147483648: Operation not permitted [1]
[2025-12-12 18:18:01] Ignore import error when loading sglang.srt.models.mindspore: name &#39;ms&#39; is not defined
Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00&lt;?, ?it/s]
Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00&lt;00:05,  1.27it/s]
Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01&lt;00:05,  1.11it/s]
Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:02&lt;00:04,  1.06it/s]
Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:03&lt;00:03,  1.04it/s]
Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:04&lt;00:02,  1.01it/s]
Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:05&lt;00:01,  1.00it/s]
Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:06&lt;00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:07&lt;00:00,  1.30it/s]
Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:07&lt;00:00,  1.13it/s]

Capturing batches (bs=4 avail_mem=47.28 GB):   0%|          | 0/3 [00:00&lt;?, ?it/s][2025-12-12 18:18:11] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /public_sglang_ci/runner-l2b-gpu-0/_work/sglang/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=60,N=1408,device_name=NVIDIA_H100_80GB_HBM3.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton
[2025-12-12 18:18:11] Using MoE kernel config with down_moe=False. Performance might be sub-optimal! Config file not found at /public_sglang_ci/runner-l2b-gpu-0/_work/sglang/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=60,N=1408,device_name=NVIDIA_H100_80GB_HBM3_down.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton
Capturing batches (bs=1 avail_mem=47.15 GB): 100%|██████████| 3/3 [00:02&lt;00:00,  1.06it/s]
[2025-12-12 18:18:15] Endpoint &#39;/get_model_info&#39; is deprecated and will be removed in a future version. Please use &#39;/model_info&#39; instead.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/start_expert_distribution_record&quot;</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

<span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/generate&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">())</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/stop_expert_distribution_record&quot;</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">/dump_expert_distribution_record&quot;</span><span class="p">)</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><Response [200]></strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>{'text': ' The capital of France is Paris. The official and most common name in French is "La Ville Lumière", or "City of Light".', 'output_ids': [576, 6722, 315, 9625, 374, 12095, 13, 576, 3946, 323, 1429, 4185, 829, 304, 8585, 374, 330, 8747, 89130, 42601, 20108, 497, 476, 330, 12730, 315, 8658, 3263, 151643], 'meta_info': {'id': '2c21ff9352e14275bf9d291771d51656', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 7, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 29, 'cached_tokens': 0, 'e2e_latency': 0.20828676223754883, 'response_sent_to_client_ts': 1765563500.2836733}}</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><Response [200]></strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><Response [200]></strong></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terminate_process</span><span class="p">(</span><span class="n">expert_record_server_process</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Tokenize/Detokenize-Example-(Round-Trip)">
<h2>Tokenize/Detokenize Example (Round Trip)<a class="headerlink" href="#Tokenize/Detokenize-Example-(Round-Trip)" title="Link to this heading">#</a></h2>
<p>This example demonstrates how to use the /tokenize and /detokenize endpoints together. We first tokenize a string, then detokenize the resulting IDs to reconstruct the original text. This workflow is useful when you need to handle tokenization externally but still leverage the server for detokenization.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer_free_server_process</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">launch_server_cmd</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>

<span class="n">wait_for_server</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-12-12 18:18:25] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:18:25] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:18:25] INFO utils.py:164: NumExpr defaulting to 16 threads.
[2025-12-12 18:18:28] WARNING server_args.py:1416: Attention backend not explicitly specified. Use fa3 backend by default.
[2025-12-12 18:18:28] server_args=ServerArgs(model_path=&#39;qwen/qwen2.5-0.5b-instruct&#39;, tokenizer_path=&#39;qwen/qwen2.5-0.5b-instruct&#39;, tokenizer_mode=&#39;auto&#39;, tokenizer_worker_num=1, skip_tokenizer_init=False, load_format=&#39;auto&#39;, model_loader_extra_config=&#39;{}&#39;, rl_quant_profile=None, trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl=&#39;auto&#39;, host=&#39;127.0.0.1&#39;, port=37610, fastapi_root_path=&#39;&#39;, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype=&#39;auto&#39;, quantization=None, quantization_param_path=None, kv_cache_dtype=&#39;auto&#39;, enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.841, max_running_requests=128, max_queued_requests=None, max_total_tokens=20480, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, schedule_policy=&#39;fcfs&#39;, enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy=&#39;lru&#39;, device=&#39;cuda&#39;, tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=180993644, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, mm_process_config={}, log_level=&#39;info&#39;, log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header=&#39;x-custom-labels&#39;, tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint=&#39;localhost:4317&#39;, export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name=&#39;qwen/qwen2.5-0.5b-instruct&#39;, weight_version=&#39;default&#39;, chat_template=None, completion_template=None, file_storage_path=&#39;sglang_storage&#39;, enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults=&#39;model&#39;, dp_size=1, load_balance_method=&#39;round_robin&#39;, load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args=&#39;{}&#39;, preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy=&#39;lru&#39;, lora_backend=&#39;csgmv&#39;, max_lora_chunk_size=16, attention_backend=&#39;fa3&#39;, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend=&#39;flashinfer&#39;, grammar_backend=&#39;xgrammar&#39;, mm_attention_backend=None, fp8_gemm_runner_backend=&#39;auto&#39;, nsa_prefill_backend=&#39;flashmla_sparse&#39;, nsa_decode_backend=&#39;fa3&#39;, enable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode=&#39;prefill&#39;, speculative_moe_runner_backend=None, speculative_moe_a2a_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type=&#39;BFS&#39;, speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend=&#39;none&#39;, moe_runner_backend=&#39;auto&#39;, flashinfer_mxfp4_moe_precision=&#39;default&#39;, enable_flashinfer_allreduce_fusion=False, deepep_mode=&#39;auto&#39;, ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location=&#39;trivial&#39;, enable_eplb=False, eplb_algorithm=&#39;auto&#39;, eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype=&#39;float32&#39;, mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy=&#39;write_through&#39;, hicache_io_backend=&#39;kernel&#39;, hicache_mem_layout=&#39;layer_first&#39;, hicache_storage_backend=None, hicache_storage_prefetch_policy=&#39;best_effort&#39;, hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=&#39;AMXINT4&#39;, kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type=&#39;qk&#39;, ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode=&#39;cpu&#39;, multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler=&#39;eager&#39;, torchao_config=&#39;&#39;, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode=&#39;null&#39;, disaggregation_transfer_backend=&#39;mooncake&#39;, disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend=&#39;nccl&#39;, remote_instance_weight_loader_support_transfer_engine=True, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None, mm_enable_dp_encoder=False, forward_hooks=None)
[2025-12-12 18:18:28] Using default HuggingFace chat template with detected content format: string
[2025-12-12 18:18:34] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:18:34] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:18:34] INFO utils.py:164: NumExpr defaulting to 16 threads.
[2025-12-12 18:18:34] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check &#34;NUMEXPR_MAX_THREADS&#34; environment variable.
[2025-12-12 18:18:34] INFO utils.py:151: Note: NumExpr detected 112 cores but &#34;NUMEXPR_MAX_THREADS&#34; not set, so enforcing safe limit of 16.
[2025-12-12 18:18:34] INFO utils.py:164: NumExpr defaulting to 16 threads.
[2025-12-12 18:18:36] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2025-12-12 18:18:37] Init torch distributed ends. mem usage=0.00 GB
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1212 18:18:37.146385 816425 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1212 18:18:37.146405 816425 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.185.21.146 port: 12001
I1212 18:18:37.146430 816425 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.185.21.146:16570
I1212 18:18:37.146498 816425 transfer_engine.cpp:185] Auto-discovering topology...
I1212 18:18:37.149384 816425 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.
I1212 18:18:37.177690 816425 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/
I1212 18:18:37.178369 816425 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:45:5d
I1212 18:18:37.206231 816425 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/
I1212 18:18:37.207208 816425 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:b9:15:92
I1212 18:18:37.233615 816425 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/
I1212 18:18:37.234242 816425 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:c5:5d
I1212 18:18:37.318571 816425 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/
I1212 18:18:37.319594 816425 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:45:5d
I1212 18:18:37.345615 816425 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/
I1212 18:18:37.346633 816425 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:c5:5d
I1212 18:18:37.373554 816425 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/
I1212 18:18:37.374541 816425 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:45:5d
I1212 18:18:37.401592 816425 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/
I1212 18:18:37.402578 816425 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:c5:5d
I1212 18:18:37.429566 816425 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/
I1212 18:18:37.430202 816425 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:45:5d
I1212 18:18:37.457579 816425 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/
I1212 18:18:37.458192 816425 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:c5:5d
W1212 18:18:38.194594 816425 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7fa74ffff010, len: 2147483648: Operation not permitted [1]
[2025-12-12 18:18:38] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-12-12 18:18:40] Ignore import error when loading sglang.srt.models.mindspore: name &#39;ms&#39; is not defined
[2025-12-12 18:18:40] Load weight begin. avail mem=78.58 GB
[2025-12-12 18:18:40] Found local HF snapshot for qwen/qwen2.5-0.5b-instruct at /hf_home/hub/models--qwen--qwen2.5-0.5b-instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775; skipping download.
[2025-12-12 18:18:40] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00&lt;?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  5.14it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  5.13it/s]

[2025-12-12 18:18:41] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=77.52 GB, mem usage=1.06 GB.
[2025-12-12 18:18:41] Using KV cache dtype: torch.bfloat16
[2025-12-12 18:18:41] KV Cache is allocated. #tokens: 20480, K size: 0.12 GB, V size: 0.12 GB
[2025-12-12 18:18:41] Memory pool end. avail mem=77.12 GB
[2025-12-12 18:18:41] Capture cuda graph begin. This can take up to several minutes. avail mem=77.02 GB
[2025-12-12 18:18:41] Capture cuda graph bs [1, 2, 4]
Capturing batches (bs=1 avail_mem=76.96 GB): 100%|██████████| 3/3 [00:00&lt;00:00,  9.01it/s]
[2025-12-12 18:18:42] Capture cuda graph end. Time elapsed: 0.70 s. mem usage=0.07 GB. avail mem=76.95 GB.
[2025-12-12 18:18:42] max_total_num_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=128, context_len=32768, available_gpu_mem=76.95 GB
[2025-12-12 18:18:43] INFO:     Started server process [816277]
[2025-12-12 18:18:43] INFO:     Waiting for application startup.
[2025-12-12 18:18:43] Using default chat sampling params from model generation config: {&#39;repetition_penalty&#39;: 1.1, &#39;temperature&#39;: 0.7, &#39;top_k&#39;: 20, &#39;top_p&#39;: 0.8}
[2025-12-12 18:18:43] Using default chat sampling params from model generation config: {&#39;repetition_penalty&#39;: 1.1, &#39;temperature&#39;: 0.7, &#39;top_k&#39;: 20, &#39;top_p&#39;: 0.8}
[2025-12-12 18:18:43] INFO:     Application startup complete.
[2025-12-12 18:18:43] INFO:     Uvicorn running on http://127.0.0.1:37610 (Press CTRL+C to quit)
[2025-12-12 18:18:44] INFO:     127.0.0.1:33000 - &#34;GET /v1/models HTTP/1.1&#34; 200 OK
[2025-12-12 18:18:44] Endpoint &#39;/get_model_info&#39; is deprecated and will be removed in a future version. Please use &#39;/model_info&#39; instead.
[2025-12-12 18:18:44] INFO:     127.0.0.1:33006 - &#34;GET /get_model_info HTTP/1.1&#34; 200 OK
[2025-12-12 18:18:44] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2025-12-12 18:18:44] INFO:     127.0.0.1:33014 - &#34;POST /generate HTTP/1.1&#34; 200 OK
[2025-12-12 18:18:44] The server is fired up and ready to roll!
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sglang.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">print_highlight</span>

<span class="n">base_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://localhost:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">tokenize_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/tokenize&quot;</span>
<span class="n">detokenize_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/detokenize&quot;</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;qwen/qwen2.5-0.5b-instruct&quot;</span>
<span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;SGLang provides efficient tokenization endpoints.&quot;</span>
<span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original Input Text:</span><span class="se">\n</span><span class="s2">&#39;</span><span class="si">{</span><span class="n">input_text</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>

<span class="c1"># --- tokenize the input text ---</span>
<span class="n">tokenize_payload</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model_name</span><span class="p">,</span>
    <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">input_text</span><span class="p">,</span>
    <span class="s2">&quot;add_special_tokens&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">}</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">tokenize_response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">tokenize_url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">tokenize_payload</span><span class="p">)</span>
    <span class="n">tokenize_response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
    <span class="n">tokenization_result</span> <span class="o">=</span> <span class="n">tokenize_response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenization_result</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tokens&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">token_ids</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tokenization returned empty tokens.&quot;</span><span class="p">)</span>

    <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Tokenized Output (IDs):</span><span class="se">\n</span><span class="si">{</span><span class="n">token_ids</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Token Count: </span><span class="si">{</span><span class="n">tokenization_result</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;count&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max Model Length: </span><span class="si">{</span><span class="n">tokenization_result</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_model_len&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># --- detokenize the obtained token IDs ---</span>
    <span class="n">detokenize_payload</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model_name</span><span class="p">,</span>
        <span class="s2">&quot;tokens&quot;</span><span class="p">:</span> <span class="n">token_ids</span><span class="p">,</span>
        <span class="s2">&quot;skip_special_tokens&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">detokenize_response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">detokenize_url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">detokenize_payload</span><span class="p">)</span>
    <span class="n">detokenize_response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
    <span class="n">detokenization_result</span> <span class="o">=</span> <span class="n">detokenize_response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
    <span class="n">reconstructed_text</span> <span class="o">=</span> <span class="n">detokenization_result</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">)</span>

    <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Detokenized Output (Text):</span><span class="se">\n</span><span class="s2">&#39;</span><span class="si">{</span><span class="n">reconstructed_text</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">input_text</span> <span class="o">==</span> <span class="n">reconstructed_text</span><span class="p">:</span>
        <span class="n">print_highlight</span><span class="p">(</span>
            <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Round Trip Successful: Original and reconstructed text match.&quot;</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">print_highlight</span><span class="p">(</span>
            <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Round Trip Mismatch: Original and reconstructed text differ.&quot;</span>
        <span class="p">)</span>

<span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">HTTP Request Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="n">print_highlight</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">An error occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Original Input Text:<br>'SGLang provides efficient tokenization endpoints.'</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-12-12 18:18:49] INFO:     127.0.0.1:56482 - &#34;POST /tokenize HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br>Tokenized Output (IDs):<br>[50, 3825, 524, 5707, 11050, 3950, 2022, 36342, 13]</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Token Count: 9</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'>Max Model Length: 131072</strong></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[2025-12-12 18:18:49] INFO:     127.0.0.1:56492 - &#34;POST /detokenize HTTP/1.1&#34; 200 OK
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br>Detokenized Output (Text):<br>'SGLang provides efficient tokenization endpoints.'</strong></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<strong style='color: #00008B;'><br>Round Trip Successful: Original and reconstructed text match.</strong></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terminate_process</span><span class="p">(</span><span class="n">tokenizer_free_server_process</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="offline_engine_api.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Offline Engine API</p>
      </div>
    </a>
    <a class="right-next"
       href="sampling_params.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Sampling Parameters</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Launch-A-Server">Launch A Server</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Generate-(text-generation-model)">Generate (text generation model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Get-Model-Info">Get Model Info</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Get-Server-Info">Get Server Info</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Health-Check">Health Check</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Flush-Cache">Flush Cache</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Update-Weights-From-Disk">Update Weights From Disk</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Encode-(embedding-model)">Encode (embedding model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#v1/rerank-(cross-encoder-rerank-model)">v1/rerank (cross encoder rerank model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Classify-(reward-model)">Classify (reward model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Capture-expert-selection-distribution-in-MoE-models">Capture expert selection distribution in MoE models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Tokenize/Detokenize-Example-(Round-Trip)">Tokenize/Detokenize Example (Round Trip)</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By SGLang Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023-2025, SGLang.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Dec 12, 2025.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  
    <!-- RunLLM Widget Script -->
    <script type="module" id="runllm-widget-script" src="https://widget.runllm.com" crossorigin="true" version="stable" runllm-keyboard-shortcut="Mod+j" runllm-name="SGLang Chatbot" runllm-position="BOTTOM_RIGHT" runllm-assistant-id="629" async></script>
    
</body>
</html>