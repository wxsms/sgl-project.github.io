{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Engine API\n",
    "\n",
    "SGLang provides a direct inference engine without the need for an HTTP server, especially for use cases where additional HTTP server adds unnecessary complexity or overhead. Here are two general use cases:\n",
    "\n",
    "- Offline Batch Inference\n",
    "- Custom Server on Top of the Engine\n",
    "\n",
    "This document focuses on the offline batch inference, demonstrating four different inference modes:\n",
    "\n",
    "- Non-streaming synchronous generation\n",
    "- Streaming synchronous generation\n",
    "- Non-streaming asynchronous generation\n",
    "- Streaming asynchronous generation\n",
    "\n",
    "Additionally, you can easily build a custom server on top of the SGLang offline engine. A detailed example working in a python script can be found in [custom_server](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/custom_server.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nest Asyncio\n",
    "Note that if you want to use **Offline Engine** in ipython or some other nested loop code, you need to add the following code:\n",
    "```python\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "The engine supports [vlm inference](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/offline_batch_inference_vlm.py) as well as [extracting hidden states](https://github.com/sgl-project/sglang/blob/main/examples/runtime/hidden_states). \n",
    "\n",
    "Please see [the examples](https://github.com/sgl-project/sglang/tree/main/examples/runtime/engine) for further use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Batch Inference\n",
    "\n",
    "SGLang offline engine supports batch inference with efficient scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:06:08.524562Z",
     "iopub.status.busy": "2026-01-23T08:06:08.524464Z",
     "iopub.status.idle": "2026-01-23T08:06:32.863504Z",
     "shell.execute_reply": "2026-01-23T08:06:32.862606Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-23 08:06:13] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-23 08:06:13] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-23 08:06:13] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-23 08:06:16] INFO server_args.py:1724: Attention backend not specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-23 08:06:16] INFO server_args.py:2618: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-23 08:06:16] INFO engine.py:154: server_args=ServerArgs(model_path='qwen/qwen2.5-0.5b-instruct', tokenizer_path='qwen/qwen2.5-0.5b-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.835, max_running_requests=128, max_queued_requests=None, max_total_tokens=20480, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', enable_prefill_delayer=False, prefill_delayer_max_delay_passes=30, prefill_delayer_token_usage_low_watermark=None, prefill_delayer_forward_passes_buckets=None, prefill_delayer_wait_seconds_buckets=None, device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=154419661, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=300, dist_timeout=None, download_dir=None, model_checksum=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, log_requests_format='text', log_requests_target=None, uvicorn_access_log_exclude_prefixes=[], crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, admin_api_key=None, served_model_name='qwen/qwen2.5-0.5b-instruct', weight_version='default', chat_template=None, hf_chat_template_name=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, enable_lora_overlap_loading=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='fa3', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', fp4_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, enable_multi_layer_eagle=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', disable_hicache_numa_detect=False, hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, hierarchical_sparse_attention_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=8192, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 704, 768, 832, 896, 960, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816, 3072, 3328, 3584, 3840, 4096, 4608, 5120, 5632, 6144, 6656, 7168, 7680, 8192], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, nsa_prefill_cp_mode='in-seq-split', enable_fused_qk_norm_rope=False, enable_precise_embedding_interpolation=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, disaggregation_decode_enable_fake_auto=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, limit_mm_data_per_request=None, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.61it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=128 avail_mem=76.34 GB):   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=128 avail_mem=76.34 GB):   5%|▌         | 1/20 [00:00<00:09,  1.97it/s]\r",
      "Capturing batches (bs=120 avail_mem=76.23 GB):   5%|▌         | 1/20 [00:00<00:09,  1.97it/s]\r",
      "Capturing batches (bs=112 avail_mem=76.23 GB):   5%|▌         | 1/20 [00:00<00:09,  1.97it/s]\r",
      "Capturing batches (bs=104 avail_mem=76.22 GB):   5%|▌         | 1/20 [00:00<00:09,  1.97it/s]\r",
      "Capturing batches (bs=96 avail_mem=76.22 GB):   5%|▌         | 1/20 [00:00<00:09,  1.97it/s] \r",
      "Capturing batches (bs=96 avail_mem=76.22 GB):  25%|██▌       | 5/20 [00:00<00:01,  9.58it/s]\r",
      "Capturing batches (bs=88 avail_mem=76.21 GB):  25%|██▌       | 5/20 [00:00<00:01,  9.58it/s]\r",
      "Capturing batches (bs=80 avail_mem=76.21 GB):  25%|██▌       | 5/20 [00:00<00:01,  9.58it/s]\r",
      "Capturing batches (bs=72 avail_mem=76.20 GB):  25%|██▌       | 5/20 [00:00<00:01,  9.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=64 avail_mem=76.20 GB):  25%|██▌       | 5/20 [00:00<00:01,  9.58it/s]\r",
      "Capturing batches (bs=64 avail_mem=76.20 GB):  45%|████▌     | 9/20 [00:00<00:00, 15.40it/s]\r",
      "Capturing batches (bs=56 avail_mem=76.19 GB):  45%|████▌     | 9/20 [00:00<00:00, 15.40it/s]\r",
      "Capturing batches (bs=48 avail_mem=76.19 GB):  45%|████▌     | 9/20 [00:00<00:00, 15.40it/s]\r",
      "Capturing batches (bs=40 avail_mem=76.18 GB):  45%|████▌     | 9/20 [00:00<00:00, 15.40it/s]\r",
      "Capturing batches (bs=40 avail_mem=76.18 GB):  60%|██████    | 12/20 [00:00<00:00, 17.11it/s]\r",
      "Capturing batches (bs=32 avail_mem=76.18 GB):  60%|██████    | 12/20 [00:00<00:00, 17.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=24 avail_mem=76.17 GB):  60%|██████    | 12/20 [00:00<00:00, 17.11it/s]\r",
      "Capturing batches (bs=16 avail_mem=76.17 GB):  60%|██████    | 12/20 [00:01<00:00, 17.11it/s]\r",
      "Capturing batches (bs=16 avail_mem=76.17 GB):  75%|███████▌  | 15/20 [00:01<00:00, 14.74it/s]\r",
      "Capturing batches (bs=12 avail_mem=76.16 GB):  75%|███████▌  | 15/20 [00:01<00:00, 14.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=8 avail_mem=76.16 GB):  75%|███████▌  | 15/20 [00:01<00:00, 14.74it/s] \r",
      "Capturing batches (bs=4 avail_mem=76.16 GB):  75%|███████▌  | 15/20 [00:01<00:00, 14.74it/s]\r",
      "Capturing batches (bs=4 avail_mem=76.16 GB):  90%|█████████ | 18/20 [00:01<00:00, 16.96it/s]\r",
      "Capturing batches (bs=2 avail_mem=76.15 GB):  90%|█████████ | 18/20 [00:01<00:00, 16.96it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.15 GB):  90%|█████████ | 18/20 [00:01<00:00, 16.96it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.15 GB): 100%|██████████| 20/20 [00:01<00:00, 14.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# launch the offline engine\n",
    "import asyncio\n",
    "\n",
    "import sglang as sgl\n",
    "import sglang.test.doc_patch\n",
    "from sglang.utils import async_stream_and_merge, stream_and_merge\n",
    "\n",
    "llm = sgl.Engine(model_path=\"qwen/qwen2.5-0.5b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:06:32.866241Z",
     "iopub.status.busy": "2026-01-23T08:06:32.865681Z",
     "iopub.status.idle": "2026-01-23T08:06:34.066327Z",
     "shell.execute_reply": "2026-01-23T08:06:34.065045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Hello, my name is\n",
      "Generated text:  Ian and I live in West Perth, Perth, Western Australia. I have been training and playing on a golf course for 13 years. I enjoy keeping fit and was awarded a handicap of 2 during my first year at Perth Golf Club.\n",
      "I started to play golf on my birthday in 2016. My handicap was 2 during my first year at Perth Golf Club and I started at 22. I have never played on a handicap 1 or handicap 3 course before. I was also very young when I first started playing golf and I am now 33. I have been playing golf for a\n",
      "===============================\n",
      "Prompt: The president of the United States is\n",
      "Generated text:  trying to decide between two different policies. The first policy is to cut 20% of the federal budget. The second policy is to cut 10% of the federal budget. Which policy will result in a smaller total reduction in the federal budget?\n",
      "To determine which policy will result in a smaller total reduction in the federal budget, we need to calculate the total reduction for each policy and compare them.\n",
      "\n",
      "Let's denote the initial federal budget as \\( B \\).\n",
      "\n",
      "1. **First Policy: Cut 20% of the budget**\n",
      "   - Reduction amount: \\( 0.20 \\times B \\)\n",
      "   - Total\n",
      "===============================\n",
      "Prompt: The capital of France is\n",
      "Generated text:  ( ).\n",
      "\n",
      "A: Paris\n",
      "\n",
      "B: Rennes\n",
      "\n",
      "C: Nice\n",
      "\n",
      "D: Bordeaux\n",
      "To determine the capital of France, we need to recall the capital cities of France. France is a European country, and its capital is typically the largest city or the largest metropolis in the country. Let's examine the options:\n",
      "\n",
      "A: Paris\n",
      "B: Rennes\n",
      "C: Nice\n",
      "D: Bordeaux\n",
      "\n",
      "The capital of France is typically the largest city in the country, which is Paris. This is because Paris is the largest city in France, and it serves as the capital of France.\n",
      "\n",
      "Therefore, the correct answer is \\boxed{\n",
      "===============================\n",
      "Prompt: The future of AI is\n",
      "Generated text:  uncertain, but the future of the brain is certain.\n",
      "Fascinating concepts like artificial intelligence (AI) and brain science are often presented in very broad terms. The field of AI is expanding at an incredible pace, but the field of neuroscience is largely overlooked. The brain is just as important to the future of AI as the brain is to the future of neurology. In fact, the brain is now the single largest piece of hardware in the world, and we are only just starting to learn how it works. The brain is a formidable, fascinating puzzle that holds secrets we are only beginning to understand.\n",
      "The human brain contains millions of neurons\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:06:34.073944Z",
     "iopub.status.busy": "2026-01-23T08:06:34.073763Z",
     "iopub.status.idle": "2026-01-23T08:06:34.873103Z",
     "shell.execute_reply": "2026-01-23T08:06:34.861623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing synchronous streaming generation with overlap removal ===\n",
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  [Name], and I'm a [Job Title] at [Company Name]. I'm a [Number] year old, [Gender] and [Country]. I'm a [Skill] with [Number] years of experience in [Field of Work]. I'm passionate about [Why You're Interested in the Industry]. I'm always looking for ways to [What You Can Do for the Company]. I'm [What You Can Do for the Company]. I'm excited to [What You Can Do for the Company]. I'm looking forward to [What You Can Do for the Company]. I'm [What You Can Do for the Company\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  Paris, known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum. It is also a major center for art, culture, and politics, and is home to many world-renowned museums and theaters. Paris is a popular tourist destination, with millions of visitors each year. The city is also known for its rich history, including the influence of the French Revolution and the influence of the French language. Paris is a vibrant and dynamic city, with a diverse population and a rich cultural heritage. The city is also home to many international organizations and institutions, including the French Academy of Sciences and\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  likely to be characterized by rapid advancements in areas such as machine learning, natural language processing, and computer vision. These technologies will continue to improve, leading to more sophisticated and accurate AI systems that can perform a wide range of tasks with increasing accuracy and efficiency. Some possible future trends in AI include:\n",
      "\n",
      "1. Increased focus on ethical considerations: As AI systems become more sophisticated, there will be a growing emphasis on ethical considerations and responsible use of AI. This will include issues such as bias, transparency, accountability, and fairness.\n",
      "\n",
      "2. Greater integration with human decision-making: AI systems will become more integrated with human decision-making processes, allowing for more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "print(\"\\n=== Testing synchronous streaming generation with overlap removal ===\\n\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    merged_output = stream_and_merge(llm, prompt, sampling_params)\n",
    "    print(\"Generated text:\", merged_output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:06:34.875488Z",
     "iopub.status.busy": "2026-01-23T08:06:34.875306Z",
     "iopub.status.idle": "2026-01-23T08:06:35.138847Z",
     "shell.execute_reply": "2026-01-23T08:06:35.138214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing asynchronous batch generation ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n",
      "Generated text:  [insert name] and I am a [insert occupation or profession]. I bring a unique blend of knowledge, experience, and a deep respect for the natural world to the table. In my free time, I like to explore the outdoors and enjoy hiking, camping, and gardening. I enjoy creating and sharing my art, from painting to sculpture. Outside of my creative pursuits, I have a strong interest in sustainability and eco-tourism. I am always looking for new ways to bring people closer to nature and learn more about it. I am looking forward to meeting you and learning more about your interests. Good luck! [Insert name]\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n",
      "Generated text:  Paris, known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral.\n",
      "The city is also famous for its history, with UNESCO listing it as a World Heritage Site. It has a rich cultural heritage and is a major economic center, with a bustling downtown area and a diverse population. Paris is a popular tourist destination and a cultural melting pot of diverse traditions and foods. Its status as a major global city and its status as the political and cultural capital of France make it a key economic and political player in Europe. Paris has a unique blend of historical and modern culture, making it\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n",
      "Generated text:  exciting and varied, with a range of trends and developments that are likely to shape the way we use and interact with AI-powered technologies. Here are some possible trends in the field:\n",
      "\n",
      "1. Increased automation and artificial general intelligence (AGI): As AI continues to improve and become more integrated into our daily lives, it is possible that we will see more widespread automation of tasks and tasks that previously required human expertise. This could include tasks such as driving, manufacturing, and customer service, among others.\n",
      "\n",
      "2. Increased focus on ethical AI: As AI becomes more integrated into our lives, there will be a growing focus on how it can be used\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous batch generation ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    outputs = await llm.async_generate(prompts, sampling_params)\n",
    "\n",
    "    for prompt, output in zip(prompts, outputs):\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated text: {output['text']}\")\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:06:35.144968Z",
     "iopub.status.busy": "2026-01-23T08:06:35.144511Z",
     "iopub.status.idle": "2026-01-23T08:06:35.829428Z",
     "shell.execute_reply": "2026-01-23T08:06:35.825153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing asynchronous streaming generation (no repeats) ===\n",
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]. I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [age"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "] year"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " old girl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " who"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loves to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [what"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to do"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "], and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " currently ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " do"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a living"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]. I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m currently"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [current"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " location"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I enjoy"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reason"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " liking"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " here"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " believe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " like your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " place"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "],"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " excited"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to see"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " what I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " can"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " do"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " here."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Looking forward"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " meeting"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Name"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " like to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " introduce yourself"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". That"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s great"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name],"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Name"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "] is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [last"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " name],"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " who"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loves"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " historic and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vibrant city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a UNESCO"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " World"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Heritage"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " site,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the largest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in France"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the seat"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " government for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " France"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " located"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " region"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " metropolitan area"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Île"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " de"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " la"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ité"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " which"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is part"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Î"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-de-F"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rance region"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " larger metropolitan"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " region known"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Basin."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is known"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for its"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rich history"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " beautiful"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " architecture,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lively"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " culture."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It's"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " one of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the world"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " most"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " visited cities"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " major"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " economic and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cultural"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hub,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " many"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " world-ren"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owned attractions"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " events"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " taking place"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " throughout the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " year."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It's"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " also the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " world's"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " most"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " highly uncertain"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " but"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " some"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " potential"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trends that"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " are emerging"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and being"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " explored"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " include"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Increased"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " integration"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " natural"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " language"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (N"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " N"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the ability"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " understand"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and interpret"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " human language"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " becoming more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " advanced with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the introduction"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " advanced models"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and data"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Enhanced"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI ethics"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and accountability"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - As"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI becomes"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " integrated into"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " society"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " there"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " will"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a greater"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " need"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ethical"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " guidelines and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accountability systems"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to ensure"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is being"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a responsible"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " way"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Increased"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " use of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " healthcare"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " becomes"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sophisticated"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " becoming"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " increasingly"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " useful"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " healthcare"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " applications"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " such"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predicting"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " disease"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progression"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous streaming generation (no repeats) ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(\"Generated text: \", end=\"\", flush=True)\n",
    "\n",
    "        # Replace direct calls to async_generate with our custom overlap-aware version\n",
    "        async for cleaned_chunk in async_stream_and_merge(llm, prompt, sampling_params):\n",
    "            print(cleaned_chunk, end=\"\", flush=True)\n",
    "\n",
    "        print()  # New line after each prompt\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T08:06:35.833842Z",
     "iopub.status.busy": "2026-01-23T08:06:35.833425Z",
     "iopub.status.idle": "2026-01-23T08:06:35.857002Z",
     "shell.execute_reply": "2026-01-23T08:06:35.846842Z"
    }
   },
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
