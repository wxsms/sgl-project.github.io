{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Engine API\n",
    "\n",
    "SGLang provides a direct inference engine without the need for an HTTP server, especially for use cases where additional HTTP server adds unnecessary complexity or overhead. Here are two general use cases:\n",
    "\n",
    "- Offline Batch Inference\n",
    "- Custom Server on Top of the Engine\n",
    "\n",
    "This document focuses on the offline batch inference, demonstrating four different inference modes:\n",
    "\n",
    "- Non-streaming synchronous generation\n",
    "- Streaming synchronous generation\n",
    "- Non-streaming asynchronous generation\n",
    "- Streaming asynchronous generation\n",
    "\n",
    "Additionally, you can easily build a custom server on top of the SGLang offline engine. A detailed example working in a python script can be found in [custom_server](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/custom_server.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nest Asyncio\n",
    "Note that if you want to use **Offline Engine** in ipython or some other nested loop code, you need to add the following code:\n",
    "```python\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "The engine supports [vlm inference](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/offline_batch_inference_vlm.py) as well as [extracting hidden states](https://github.com/sgl-project/sglang/blob/main/examples/runtime/hidden_states). \n",
    "\n",
    "Please see [the examples](https://github.com/sgl-project/sglang/tree/main/examples/runtime/engine) for further use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Batch Inference\n",
    "\n",
    "SGLang offline engine supports batch inference with efficient scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T07:46:57.226329Z",
     "iopub.status.busy": "2025-11-15T07:46:57.226208Z",
     "iopub.status.idle": "2025-11-15T07:47:37.916703Z",
     "shell.execute_reply": "2025-11-15T07:47:37.915829Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-15 07:47:02] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-15 07:47:02] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-15 07:47:02] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-15 07:47:05] WARNING server_args.py:1210: Attention backend not explicitly specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-15 07:47:05] INFO engine.py:123: server_args=ServerArgs(model_path='qwen/qwen2.5-0.5b-instruct', tokenizer_path='qwen/qwen2.5-0.5b-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.835, max_running_requests=128, max_queued_requests=None, max_total_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=279563816, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='qwen/qwen2.5-0.5b-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='fa3', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-15 07:47:10] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-11-15 07:47:10] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-11-15 07:47:10] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.37it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.37it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=128 avail_mem=76.92 GB):   0%|          | 0/20 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=128 avail_mem=76.92 GB):   5%|▌         | 1/20 [00:00<00:03,  5.71it/s]\r",
      "Capturing batches (bs=120 avail_mem=76.81 GB):   5%|▌         | 1/20 [00:00<00:03,  5.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=112 avail_mem=76.81 GB):   5%|▌         | 1/20 [00:00<00:03,  5.71it/s]\r",
      "Capturing batches (bs=104 avail_mem=76.80 GB):   5%|▌         | 1/20 [00:00<00:03,  5.71it/s]\r",
      "Capturing batches (bs=104 avail_mem=76.80 GB):  20%|██        | 4/20 [00:00<00:01, 15.67it/s]\r",
      "Capturing batches (bs=96 avail_mem=76.80 GB):  20%|██        | 4/20 [00:00<00:01, 15.67it/s] \r",
      "Capturing batches (bs=88 avail_mem=76.79 GB):  20%|██        | 4/20 [00:00<00:01, 15.67it/s]\r",
      "Capturing batches (bs=80 avail_mem=76.79 GB):  20%|██        | 4/20 [00:00<00:01, 15.67it/s]\r",
      "Capturing batches (bs=80 avail_mem=76.79 GB):  35%|███▌      | 7/20 [00:00<00:00, 20.28it/s]\r",
      "Capturing batches (bs=72 avail_mem=76.78 GB):  35%|███▌      | 7/20 [00:00<00:00, 20.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=64 avail_mem=76.78 GB):  35%|███▌      | 7/20 [00:00<00:00, 20.28it/s]\r",
      "Capturing batches (bs=56 avail_mem=76.77 GB):  35%|███▌      | 7/20 [00:00<00:00, 20.28it/s]\r",
      "Capturing batches (bs=56 avail_mem=76.77 GB):  50%|█████     | 10/20 [00:00<00:00, 21.26it/s]\r",
      "Capturing batches (bs=48 avail_mem=76.77 GB):  50%|█████     | 10/20 [00:00<00:00, 21.26it/s]\r",
      "Capturing batches (bs=40 avail_mem=76.76 GB):  50%|█████     | 10/20 [00:00<00:00, 21.26it/s]\r",
      "Capturing batches (bs=32 avail_mem=76.76 GB):  50%|█████     | 10/20 [00:00<00:00, 21.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=32 avail_mem=76.76 GB):  65%|██████▌   | 13/20 [00:00<00:00, 14.69it/s]\r",
      "Capturing batches (bs=24 avail_mem=76.76 GB):  65%|██████▌   | 13/20 [00:00<00:00, 14.69it/s]\r",
      "Capturing batches (bs=16 avail_mem=76.75 GB):  65%|██████▌   | 13/20 [00:00<00:00, 14.69it/s]\r",
      "Capturing batches (bs=16 avail_mem=76.75 GB):  75%|███████▌  | 15/20 [00:01<00:00, 13.25it/s]\r",
      "Capturing batches (bs=12 avail_mem=76.75 GB):  75%|███████▌  | 15/20 [00:01<00:00, 13.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=8 avail_mem=76.74 GB):  75%|███████▌  | 15/20 [00:01<00:00, 13.25it/s] \r",
      "Capturing batches (bs=4 avail_mem=76.73 GB):  75%|███████▌  | 15/20 [00:01<00:00, 13.25it/s]\r",
      "Capturing batches (bs=4 avail_mem=76.73 GB):  90%|█████████ | 18/20 [00:01<00:00, 16.30it/s]\r",
      "Capturing batches (bs=2 avail_mem=76.73 GB):  90%|█████████ | 18/20 [00:01<00:00, 16.30it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.73 GB):  90%|█████████ | 18/20 [00:01<00:00, 16.30it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.73 GB): 100%|██████████| 20/20 [00:01<00:00, 16.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# launch the offline engine\n",
    "import asyncio\n",
    "\n",
    "import sglang as sgl\n",
    "import sglang.test.doc_patch\n",
    "from sglang.utils import async_stream_and_merge, stream_and_merge\n",
    "\n",
    "llm = sgl.Engine(model_path=\"qwen/qwen2.5-0.5b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T07:47:37.919679Z",
     "iopub.status.busy": "2025-11-15T07:47:37.919205Z",
     "iopub.status.idle": "2025-11-15T07:47:38.708221Z",
     "shell.execute_reply": "2025-11-15T07:47:38.707494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Hello, my name is\n",
      "Generated text:  Taz and I am a 15 year old female student at a public high school in the United States. I have been a keen climber since I was a child, and I am currently training for the 2019-2020 school year. I have training in hiking, rock climbing, and alpine climbing. My personal goals are to reach 4000m (13,000ft) on the rock wall of the 10K Pro World Championships in 2020, and to become an alpine climber in 2022.\n",
      "I have\n",
      "===============================\n",
      "Prompt: The president of the United States is\n",
      "Generated text:  trying to decide how many military tanks to have on the country. He knows that there are two options, Option A and Option B. He has done some research and determined that Option A will have 2 tanks per soldier, and Option B will have 3 tanks per soldier. If the United States has 200 soldiers, how many more tanks will Option B provide compared to Option A?\n",
      "To determine how many more tanks Option B will provide compared to Option A, we need to calculate the total number of tanks for each option and then find the difference.\n",
      "\n",
      "First, let's calculate the number of tanks Option A will provide:\n",
      "\\\n",
      "===============================\n",
      "Prompt: The capital of France is\n",
      "Generated text:  in which country? The capital of France is Paris, which is located in France. Paris is the largest city in France and serves as the capital of the country. Paris is known for its beautiful architecture, rich history, and world-class museums and monuments. The country of France is divided into 26 provinces, each governed by a regional government. Paris is the capital of France and is the largest city in the country.\n",
      "===============================\n",
      "Prompt: The future of AI is\n",
      "Generated text:  rapidly changing, and the field is expected to continue to evolve in many ways. While AI is primarily designed to make things easier and more efficient, it is also being used to create new technologies and solutions to the challenges of the present and future. So, what does the future of AI look like?\n",
      "The future of AI is expected to continue to be shaped by new and emerging technologies, such as quantum computing and machine learning. It is also expected to be influenced by a changing regulatory landscape, including the increasing use of AI in healthcare, finance, and beyond.\n",
      "There are a number of potential areas where AI can make a significant impact, including\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T07:47:38.710029Z",
     "iopub.status.busy": "2025-11-15T07:47:38.709873Z",
     "iopub.status.idle": "2025-11-15T07:47:39.422627Z",
     "shell.execute_reply": "2025-11-15T07:47:39.421840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing synchronous streaming generation with overlap removal ===\n",
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  [Name], and I'm a [job title] at [company name]. I'm excited to meet you and learn more about you. What can you tell me about yourself? I'm a [job title] at [company name], and I'm excited to meet you and learn more about you. What can you tell me about yourself? I'm a [job title] at [company name], and I'm excited to meet you and learn more about you. What can you tell me about yourself? I'm a [job title] at [company name], and I'm excited to meet you and learn more about you. What\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  Paris, also known as the City of Light. It is the largest city in France and the second-largest city in the European Union. Paris is known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and the Louvre Museum. It is also home to many famous museums, including the Musée d'Orsay, the Musée Rodin, and the Musée d'Orsay. Paris is a cultural and historical center with a rich history dating back to the Roman Empire and the Middle Ages. It is a popular tourist destination and a major economic center in Europe. The city\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  likely to be characterized by a number of trends that are expected to shape the technology's direction. Here are some of the most likely trends:\n",
      "\n",
      "1. Increased focus on ethical considerations: As AI becomes more integrated into our daily lives, there will be a greater emphasis on ethical considerations. This includes issues such as bias, transparency, accountability, and privacy. AI developers will need to take these concerns into account as they design and deploy AI systems.\n",
      "\n",
      "2. Greater use of AI in healthcare: AI is already being used in healthcare to improve patient outcomes and reduce costs. As AI technology continues to advance, we can expect to see even more widespread adoption\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "print(\"\\n=== Testing synchronous streaming generation with overlap removal ===\\n\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    merged_output = stream_and_merge(llm, prompt, sampling_params)\n",
    "    print(\"Generated text:\", merged_output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T07:47:39.424378Z",
     "iopub.status.busy": "2025-11-15T07:47:39.424221Z",
     "iopub.status.idle": "2025-11-15T07:47:39.654275Z",
     "shell.execute_reply": "2025-11-15T07:47:39.653466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing asynchronous batch generation ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n",
      "Generated text:  [insert character's name here] and I'm a [insert character's age and gender here]. I'm currently [insert character's current age or occupation here]. I was born in [insert birthplace here] and I live in [insert current location here]. I'm passionate about [insert one or two things that excite me or make me happy] and I enjoy [insert one or two things that I love to do]. I am also a [insert one or two things that you'd want to know about me]. I've always been [insert something about your character here] and I believe that my values are [insert any\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n",
      "Generated text:  Paris, an iconic city known for its beautiful cathedrals, charming districts, and vibrant festivals throughout the year. \n",
      "\n",
      "City Facts:\n",
      "\n",
      "- Location: Paris, France, is the capital and largest city of France.\n",
      "- Population: Paris has a population of approximately 2.2 million people.\n",
      "- Capital of France: Paris is the seat of government, the King's residence, and the major city of France.\n",
      "- Official language: French.\n",
      "- Time zone: GMT (Greenwich Mean Time) - 1 hour ahead of the eastern European time zone.\n",
      "- Economy: The city's economy is dominated by tourism, fashion, and\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n",
      "Generated text:  highly unpredictable and uncertain. However, there are some general trends that can be expected to shape the development of AI in the years to come. Some of the possible future trends in AI include:\n",
      "\n",
      "1. Increased personalization: AI will become more personalized and tailored to individual needs and preferences, enabling more efficient and effective use of resources.\n",
      "\n",
      "2. Autonomous agents: AI will become more autonomous, with robots and other intelligent systems being able to make decisions and take actions on their own without human intervention.\n",
      "\n",
      "3. AI will be integrated into everyday life: AI will become more integrated into everyday life, enabling people to access information, make decisions, and perform\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous batch generation ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    outputs = await llm.async_generate(prompts, sampling_params)\n",
    "\n",
    "    for prompt, output in zip(prompts, outputs):\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated text: {output['text']}\")\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T07:47:39.655936Z",
     "iopub.status.busy": "2025-11-15T07:47:39.655771Z",
     "iopub.status.idle": "2025-11-15T07:47:40.257269Z",
     "shell.execute_reply": "2025-11-15T07:47:40.256382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing asynchronous streaming generation (no repeats) ===\n",
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "],"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " story"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " craft"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " background"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specific"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " skill"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " interest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " My"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ention"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " purpose"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mention"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " any"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " specific"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " goal"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " objective"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " have"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'d"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " like"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " share"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " about"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " yourself"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "],"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " looking"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " forward"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " day"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " when"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " can"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " goal"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mission"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " And"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " always"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " looking"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ways"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'d"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " like"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " do"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " contribute"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " world"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "],"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " always"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " happy"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " help"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " where"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " can"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'d"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " like"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " share"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " about"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " yourself"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " response"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " should"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " brief"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " but"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " engaging"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " historic"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " located"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " River"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ine"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " southern"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " part"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " country"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " world"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " second"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-largest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " by"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " population"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " third"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-largest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " by"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " area"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " serves"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " seat"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " government"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " major"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cultural"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " economic"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hub"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " founded"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " by"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Char"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lem"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agne"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " world"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " oldest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuously"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " inhabited"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " home"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lou"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vre"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Museum"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Notre"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-D"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ame"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cathedral"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " E"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iff"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tower"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arc"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " de"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tri"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " also"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " known"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " its"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vibrant"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nightlife"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fashion"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " industry"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " culinary"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " scene"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cosm"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opolitan"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " diverse"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " highly"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dynamic"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " constantly"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " evolving"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " difficult"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predict"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " certainty"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " what"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trends"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " will"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " However"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " some"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " potential"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " future"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trends"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " include"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Increased"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " emphasis"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ethical"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " people"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " become"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aware"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " potential"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " risks"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ethical"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " issues"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " associated"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " there"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " growing"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " push"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " development"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ethical"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " transparent"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Greater"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " integration"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " human"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " expertise"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " likely"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " become"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " integrated"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " human"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " expertise"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " particularly"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fields"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " such"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " healthcare"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " transportation"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " manufacturing"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " integration"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " could"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lead"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " efficient"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accurate"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " human"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-like"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " systems"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Increased"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " robotics"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " automation"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous streaming generation (no repeats) ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(\"Generated text: \", end=\"\", flush=True)\n",
    "\n",
    "        # Replace direct calls to async_generate with our custom overlap-aware version\n",
    "        async for cleaned_chunk in async_stream_and_merge(llm, prompt, sampling_params):\n",
    "            print(cleaned_chunk, end=\"\", flush=True)\n",
    "\n",
    "        print()  # New line after each prompt\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T07:47:40.259868Z",
     "iopub.status.busy": "2025-11-15T07:47:40.259710Z",
     "iopub.status.idle": "2025-11-15T07:47:40.302614Z",
     "shell.execute_reply": "2025-11-15T07:47:40.301633Z"
    }
   },
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
