{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Engine API\n",
    "\n",
    "SGLang provides a direct inference engine without the need for an HTTP server, especially for use cases where additional HTTP server adds unnecessary complexity or overhead. Here are two general use cases:\n",
    "\n",
    "- Offline Batch Inference\n",
    "- Custom Server on Top of the Engine\n",
    "\n",
    "This document focuses on the offline batch inference, demonstrating four different inference modes:\n",
    "\n",
    "- Non-streaming synchronous generation\n",
    "- Streaming synchronous generation\n",
    "- Non-streaming asynchronous generation\n",
    "- Streaming asynchronous generation\n",
    "\n",
    "Additionally, you can easily build a custom server on top of the SGLang offline engine. A detailed example working in a python script can be found in [custom_server](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/custom_server.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nest Asyncio\n",
    "Note that if you want to use **Offline Engine** in ipython or some other nested loop code, you need to add the following code:\n",
    "```python\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "The engine supports [vlm inference](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/offline_batch_inference_vlm.py) as well as [extracting hidden states](https://github.com/sgl-project/sglang/blob/main/examples/runtime/hidden_states). \n",
    "\n",
    "Please see [the examples](https://github.com/sgl-project/sglang/tree/main/examples/runtime/engine) for further use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Batch Inference\n",
    "\n",
    "SGLang offline engine supports batch inference with efficient scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T20:39:12.621645Z",
     "iopub.status.busy": "2026-01-20T20:39:12.621548Z",
     "iopub.status.idle": "2026-01-20T20:39:36.385794Z",
     "shell.execute_reply": "2026-01-20T20:39:36.385117Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-20 20:39:18] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-20 20:39:18] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-20 20:39:18] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-20 20:39:20] INFO server_args.py:1670: Attention backend not specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-20 20:39:20] INFO server_args.py:2570: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-20 20:39:20] INFO engine.py:154: server_args=ServerArgs(model_path='qwen/qwen2.5-0.5b-instruct', tokenizer_path='qwen/qwen2.5-0.5b-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.835, max_running_requests=128, max_queued_requests=None, max_total_tokens=20480, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', enable_prefill_delayer=False, prefill_delayer_max_delay_passes=30, prefill_delayer_token_usage_low_watermark=None, prefill_delayer_forward_passes_buckets=None, prefill_delayer_wait_seconds_buckets=None, device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=265395066, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=300, dist_timeout=None, download_dir=None, model_checksum=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, log_requests_format='text', log_requests_target=None, uvicorn_access_log_exclude_prefixes=[], crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, admin_api_key=None, served_model_name='qwen/qwen2.5-0.5b-instruct', weight_version='default', chat_template=None, hf_chat_template_name=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, enable_lora_overlap_loading=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='fa3', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', fp4_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, enable_multi_layer_eagle=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', disable_hicache_numa_detect=False, hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, hierarchical_sparse_attention_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=8192, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 704, 768, 832, 896, 960, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816, 3072, 3328, 3584, 3840, 4096, 4608, 5120, 5632, 6144, 6656, 7168, 7680, 8192], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, nsa_prefill_cp_mode='in-seq-split', enable_fused_qk_norm_rope=False, enable_precise_embedding_interpolation=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, disaggregation_decode_enable_fake_auto=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, limit_mm_data_per_request=None, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.25it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.24it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=128 avail_mem=76.93 GB):   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=128 avail_mem=76.93 GB):   5%|â–Œ         | 1/20 [00:00<00:13,  1.44it/s]\r",
      "Capturing batches (bs=120 avail_mem=76.83 GB):   5%|â–Œ         | 1/20 [00:00<00:13,  1.44it/s]\r",
      "Capturing batches (bs=112 avail_mem=76.82 GB):   5%|â–Œ         | 1/20 [00:00<00:13,  1.44it/s]\r",
      "Capturing batches (bs=104 avail_mem=76.82 GB):   5%|â–Œ         | 1/20 [00:00<00:13,  1.44it/s]\r",
      "Capturing batches (bs=104 avail_mem=76.82 GB):  20%|â–ˆâ–ˆ        | 4/20 [00:00<00:02,  6.09it/s]\r",
      "Capturing batches (bs=96 avail_mem=76.82 GB):  20%|â–ˆâ–ˆ        | 4/20 [00:00<00:02,  6.09it/s] \r",
      "Capturing batches (bs=88 avail_mem=76.81 GB):  20%|â–ˆâ–ˆ        | 4/20 [00:00<00:02,  6.09it/s]\r",
      "Capturing batches (bs=80 avail_mem=76.81 GB):  20%|â–ˆâ–ˆ        | 4/20 [00:00<00:02,  6.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=80 avail_mem=76.81 GB):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:00<00:01,  9.78it/s]\r",
      "Capturing batches (bs=72 avail_mem=76.80 GB):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:00<00:01,  9.78it/s]\r",
      "Capturing batches (bs=64 avail_mem=76.80 GB):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:01<00:01,  9.78it/s]\r",
      "Capturing batches (bs=56 avail_mem=76.79 GB):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:01<00:01,  9.78it/s]\r",
      "Capturing batches (bs=56 avail_mem=76.79 GB):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:01<00:00, 12.95it/s]\r",
      "Capturing batches (bs=48 avail_mem=76.79 GB):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:01<00:00, 12.95it/s]\r",
      "Capturing batches (bs=40 avail_mem=76.78 GB):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:01<00:00, 12.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=32 avail_mem=76.78 GB):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:01<00:00, 12.95it/s]\r",
      "Capturing batches (bs=32 avail_mem=76.78 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:01<00:00, 15.18it/s]\r",
      "Capturing batches (bs=24 avail_mem=76.77 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:01<00:00, 15.18it/s]\r",
      "Capturing batches (bs=16 avail_mem=76.77 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:01<00:00, 15.18it/s]\r",
      "Capturing batches (bs=12 avail_mem=76.76 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:01<00:00, 15.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=12 avail_mem=76.76 GB):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:01<00:00, 16.14it/s]\r",
      "Capturing batches (bs=8 avail_mem=76.76 GB):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:01<00:00, 16.14it/s] \r",
      "Capturing batches (bs=4 avail_mem=76.75 GB):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:01<00:00, 16.14it/s]\r",
      "Capturing batches (bs=2 avail_mem=76.75 GB):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:01<00:00, 16.14it/s]\r",
      "Capturing batches (bs=2 avail_mem=76.75 GB):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:01<00:00, 18.60it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.74 GB):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:01<00:00, 18.60it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.74 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 12.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# launch the offline engine\n",
    "import asyncio\n",
    "\n",
    "import sglang as sgl\n",
    "import sglang.test.doc_patch\n",
    "from sglang.utils import async_stream_and_merge, stream_and_merge\n",
    "\n",
    "llm = sgl.Engine(model_path=\"qwen/qwen2.5-0.5b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T20:39:36.389452Z",
     "iopub.status.busy": "2026-01-20T20:39:36.388520Z",
     "iopub.status.idle": "2026-01-20T20:39:37.523986Z",
     "shell.execute_reply": "2026-01-20T20:39:37.523148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Hello, my name is\n",
      "Generated text:  Sam and I have lived in Portugal since 1997. I am a devoted fan of the Portuguese football team, FC Porto, and the Portuguese football league, La Liga. I have hosted the Portuguese soccer league from 2000 until 2013, and have hosted the UEFA Champions League since 2014.\n",
      "I have been awarded a Cardiff School of Law degree in the UK, and I am a British citizen. I have a Master's degree in Global Development, and I have experience as a Senior Policy Officer, in a think tank and as a consultant in the area of global health.\n",
      "I\n",
      "===============================\n",
      "Prompt: The president of the United States is\n",
      "Generated text:  a ____ of the country.\n",
      "A. leader\n",
      "B. leader\n",
      "C. officer\n",
      "D. military\n",
      "Answer:\n",
      "A\n",
      "\n",
      "Under normal temperature, the volume of a gas molecule is 0.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "===============================\n",
      "Prompt: The capital of France is\n",
      "Generated text:  Paris. There are three main avenues that run through Paris. They are Boulevard, Arc de Triomphe, and Arc de la Fauve. You can see them from the top of Montmartre, a hill in the French countryside. The streets that run through Paris have special names. The street where the Arc de Triomphe is located is called Rue de Rivoli. And the road where the Arc de la Fauve is located is called Rue de Rivoli. Both streets are important to Paris, and they are very famous. Parisians love to get lost in these streets as they go about their daily lives. When\n",
      "===============================\n",
      "Prompt: The future of AI is\n",
      "Generated text:  complex and unpredictable. It is not a technology that will be able to work alone, but rather a set of tools that can work together to make real changes in the world. It is the growth of AI that is sweeping our lives and transforming the world, from the way we work, to the way we live, to the way we communicate. In this blog post, we'll look at the various ways in which AI is transforming the way we live and work, and how we can harness this technology to our advantage.\n",
      "What are some of the ways in which AI is transforming the way we live and work?\n",
      "AI is revolutionizing the way\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T20:39:37.528946Z",
     "iopub.status.busy": "2026-01-20T20:39:37.528669Z",
     "iopub.status.idle": "2026-01-20T20:39:38.336736Z",
     "shell.execute_reply": "2026-01-20T20:39:38.329101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing synchronous streaming generation with overlap removal ===\n",
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  [Name] and I'm a [occupation] who has been [number of years] in the industry. I'm passionate about [reason for passion], and I'm always looking for ways to [action or goal]. I'm a [type of person] who is [character trait or quality] and I'm always [positive or negative] about [something]. I'm [character description] and I'm [character goal]. I'm excited to meet you and learn more about you. What's your name? What's your occupation? What's your passion? What's your action or goal? What's your type of person? What\n",
      "\n",
      "Prompt: Provide a concise factual statement about Franceâ€™s capital city. The capital of France is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  Paris, the city known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum. It is also home to the French Parliament, the French National Library, and the French National Museum of Modern Art. Paris is a bustling metropolis with a rich cultural heritage and is a popular tourist destination. It is also known for its fashion industry, with Paris Fashion Week being one of the largest and most prestigious fashion events in the world. The city is also home to the French Parliament, the French National Library, and the French National Museum of Modern Art. Paris is a vibrant and diverse city with\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  likely to be characterized by a number of trends that are expected to shape the technology's direction and impact on society. Here are some of the most likely trends:\n",
      "\n",
      "1. Increased automation: As AI becomes more advanced, it is likely to become more capable of performing tasks that were previously done by humans. This could lead to a significant increase in automation, which could have a wide range of implications for employment and society as a whole.\n",
      "\n",
      "2. Enhanced privacy and security: As AI becomes more sophisticated, it is likely to require more data to function effectively. This could lead to increased concerns about privacy and security, as AI systems may be more vulnerable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about Franceâ€™s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "print(\"\\n=== Testing synchronous streaming generation with overlap removal ===\\n\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    merged_output = stream_and_merge(llm, prompt, sampling_params)\n",
    "    print(\"Generated text:\", merged_output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T20:39:38.342282Z",
     "iopub.status.busy": "2026-01-20T20:39:38.342133Z",
     "iopub.status.idle": "2026-01-20T20:39:38.596571Z",
     "shell.execute_reply": "2026-01-20T20:39:38.594614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing asynchronous batch generation ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n",
      "Generated text:  [Your Name], and Iâ€™m a professional photographer with a passion for capturing the beauty of the world around us. Iâ€™m a versatile artist who can take photos on all types of subjects, from portraits to landscapes to candid shots. I have a keen eye for detail and enjoy using the right equipment to achieve the best results. Iâ€™m a team player and enjoy working with a wide range of clients, from businesses to individuals. I have a good knowledge of Adobe Photoshop and I'm excited to learn more about the latest tools and techniques in photography. Thank you for considering me for a job. I look forward to the possibility of working with you\n",
      "\n",
      "Prompt: Provide a concise factual statement about Franceâ€™s capital city. The capital of France is\n",
      "Generated text:  Paris, which is known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, Louvre Museum, and SacrÃ©-CÅ“ur Basilica. The city is also famous for its cuisine, including the famous \"la viande\" (meat dishes) and the popular \"ponniÃ¨re\" (pork shoulder steak). Paris is a cultural melting pot of different traditions and languages, and it has a rich history dating back to ancient times. The city has a wide range of activities for tourists to enjoy, including museums, art galleries, and shopping. Overall, Paris is an unforgettable destination for anyone interested in French\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n",
      "Generated text:  rapidly evolving, and there are several potential trends that we can expect to see in the years ahead. Here are some of the most important trends that are shaping the AI landscape:\n",
      "\n",
      "1. Increased focus on ethical AI: One of the most important trends in AI is the increasing focus on ethical AI. With the potential to cause harm, AI developers will need to prioritize ethical considerations in their work. This will involve developing more transparent AI systems, ensuring that AI is not used for harmful purposes, and developing ethical guidelines for AI development.\n",
      "\n",
      "2. Improved AI performance: Another important trend is the continued advancement of AI performance. With advancements in machine learning and\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about Franceâ€™s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous batch generation ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    outputs = await llm.async_generate(prompts, sampling_params)\n",
    "\n",
    "    for prompt, output in zip(prompts, outputs):\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated text: {output['text']}\")\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T20:39:38.599991Z",
     "iopub.status.busy": "2026-01-20T20:39:38.599301Z",
     "iopub.status.idle": "2026-01-20T20:39:39.371531Z",
     "shell.execute_reply": "2026-01-20T20:39:39.352398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing asynchronous streaming generation (no repeats) ===\n",
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Name"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]. I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " am a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " natural-born"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " storyteller"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " have a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " knack for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " creating captivating"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " narratives."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I believe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that storytelling"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " key to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " unlocking one"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s potential"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m eager"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to help"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " others do"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the same"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Whether"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'re an"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aspiring writer"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " seasoned professional"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " looking to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " improve your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " storytelling skills"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m here"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to share"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my insights"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and help"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you achieve"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " your goals"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". So"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", if"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'re"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ready"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " embark on"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " your creative"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " journey,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " join me"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on this"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " journey"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " self-dis"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covery"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " self-expression"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡âœ¨"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #Story"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elling"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #Creative"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crad"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le #"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aut"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obi"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ographical"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ess"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ence"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Feel free"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " adjust"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " greeting"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: Provide a concise factual statement about Franceâ€™s capital city. The capital of France is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acts"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " capital"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " France"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " located in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the north"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "western region"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " country"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the largest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " city in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Europe and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the sixth"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-largest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " world"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " by"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " population"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " million"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " inhabitants"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " known"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " its historical"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " significance,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " renowned for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " its many"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " monuments,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " museums"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " museums"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " also known"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " its"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fashion industry"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", cultural"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " attractions"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " its role"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " global"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " center"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " education"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " research"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " most"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " populous city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in France"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " population"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " has"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fluct"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uated"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " likely"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to be"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " characterized"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " by"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rapid"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " technological advancements"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ability to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " increasingly"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sophisticated and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " complex"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " artificial intelligence"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " systems."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " are"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " some potential"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trends that"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " could shape"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the future"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "\n",
      "1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Increased"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Integration of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " into"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Our"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Daily Lives"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": As"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI becomes"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " integrated into"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " our daily"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lives,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we may"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " see more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " interactions with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI-powered"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " devices and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " services,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " such as"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " virtual assistants"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", self"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-driving cars"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " robotic assistants"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". These"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " interactions could"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lead to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " greater"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " efficiency and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " convenience,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as well"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as new"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " opportunities for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " businesses to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " automate and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " streamline their"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " operations.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI Ethics"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and Privacy"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": As"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI systems"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " become more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sophisticated,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " may"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " see increased"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " scrutiny of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " their ethical"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " behavior"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about Franceâ€™s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous streaming generation (no repeats) ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(\"Generated text: \", end=\"\", flush=True)\n",
    "\n",
    "        # Replace direct calls to async_generate with our custom overlap-aware version\n",
    "        async for cleaned_chunk in async_stream_and_merge(llm, prompt, sampling_params):\n",
    "            print(cleaned_chunk, end=\"\", flush=True)\n",
    "\n",
    "        print()  # New line after each prompt\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T20:39:39.374347Z",
     "iopub.status.busy": "2026-01-20T20:39:39.374002Z",
     "iopub.status.idle": "2026-01-20T20:39:39.393694Z",
     "shell.execute_reply": "2026-01-20T20:39:39.383148Z"
    }
   },
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
