{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Engine API\n",
    "\n",
    "SGLang provides a direct inference engine without the need for an HTTP server, especially for use cases where additional HTTP server adds unnecessary complexity or overhead. Here are two general use cases:\n",
    "\n",
    "- Offline Batch Inference\n",
    "- Custom Server on Top of the Engine\n",
    "\n",
    "This document focuses on the offline batch inference, demonstrating four different inference modes:\n",
    "\n",
    "- Non-streaming synchronous generation\n",
    "- Streaming synchronous generation\n",
    "- Non-streaming asynchronous generation\n",
    "- Streaming asynchronous generation\n",
    "\n",
    "Additionally, you can easily build a custom server on top of the SGLang offline engine. A detailed example working in a python script can be found in [custom_server](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/custom_server.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nest Asyncio\n",
    "Note that if you want to use **Offline Engine** in ipython or some other nested loop code, you need to add the following code:\n",
    "```python\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "The engine supports [vlm inference](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/offline_batch_inference_vlm.py) as well as [extracting hidden states](https://github.com/sgl-project/sglang/blob/main/examples/runtime/hidden_states). \n",
    "\n",
    "Please see [the examples](https://github.com/sgl-project/sglang/tree/main/examples/runtime/engine) for further use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Batch Inference\n",
    "\n",
    "SGLang offline engine supports batch inference with efficient scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T02:14:01.440533Z",
     "iopub.status.busy": "2026-01-08T02:14:01.440377Z",
     "iopub.status.idle": "2026-01-08T02:14:24.443655Z",
     "shell.execute_reply": "2026-01-08T02:14:24.442715Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-08 02:14:06] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-08 02:14:06] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-08 02:14:06] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-08 02:14:09] INFO server_args.py:1615: Attention backend not specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-08 02:14:09] INFO server_args.py:2502: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-08 02:14:09] INFO engine.py:153: server_args=ServerArgs(model_path='qwen/qwen2.5-0.5b-instruct', tokenizer_path='qwen/qwen2.5-0.5b-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.835, max_running_requests=128, max_queued_requests=None, max_total_tokens=20480, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=319272417, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, log_requests_format='text', log_requests_target=None, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='qwen/qwen2.5-0.5b-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='fa3', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, enable_multi_layer_eagle=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, hierarchical_sparse_attention_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=8192, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096, 4352, 4608, 4864, 5120, 5376, 5632, 5888, 6144, 6400, 6656, 6912, 7168, 7424, 7680, 7936, 8192], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, nsa_prefill_cp_mode='in-seq-split', enable_fused_qk_norm_rope=False, enable_precise_embedding_interpolation=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, disaggregation_decode_enable_fake_auto=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, limit_mm_data_per_request=None, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.66it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.66it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=128 avail_mem=55.35 GB):   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=128 avail_mem=55.35 GB):   5%|▌         | 1/20 [00:00<00:04,  4.68it/s]\r",
      "Capturing batches (bs=120 avail_mem=55.25 GB):   5%|▌         | 1/20 [00:00<00:04,  4.68it/s]\r",
      "Capturing batches (bs=112 avail_mem=55.25 GB):   5%|▌         | 1/20 [00:00<00:04,  4.68it/s]\r",
      "Capturing batches (bs=104 avail_mem=55.24 GB):   5%|▌         | 1/20 [00:00<00:04,  4.68it/s]\r",
      "Capturing batches (bs=104 avail_mem=55.24 GB):  20%|██        | 4/20 [00:00<00:01, 13.67it/s]\r",
      "Capturing batches (bs=96 avail_mem=55.24 GB):  20%|██        | 4/20 [00:00<00:01, 13.67it/s] \r",
      "Capturing batches (bs=88 avail_mem=55.23 GB):  20%|██        | 4/20 [00:00<00:01, 13.67it/s]\r",
      "Capturing batches (bs=80 avail_mem=55.23 GB):  20%|██        | 4/20 [00:00<00:01, 13.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=80 avail_mem=55.23 GB):  35%|███▌      | 7/20 [00:00<00:00, 18.19it/s]\r",
      "Capturing batches (bs=72 avail_mem=55.22 GB):  35%|███▌      | 7/20 [00:00<00:00, 18.19it/s]\r",
      "Capturing batches (bs=64 avail_mem=55.22 GB):  35%|███▌      | 7/20 [00:00<00:00, 18.19it/s]\r",
      "Capturing batches (bs=56 avail_mem=55.21 GB):  35%|███▌      | 7/20 [00:00<00:00, 18.19it/s]\r",
      "Capturing batches (bs=56 avail_mem=55.21 GB):  50%|█████     | 10/20 [00:00<00:00, 20.34it/s]\r",
      "Capturing batches (bs=48 avail_mem=55.21 GB):  50%|█████     | 10/20 [00:00<00:00, 20.34it/s]\r",
      "Capturing batches (bs=40 avail_mem=55.20 GB):  50%|█████     | 10/20 [00:00<00:00, 20.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=32 avail_mem=55.20 GB):  50%|█████     | 10/20 [00:00<00:00, 20.34it/s]\r",
      "Capturing batches (bs=32 avail_mem=55.20 GB):  65%|██████▌   | 13/20 [00:00<00:00, 21.68it/s]\r",
      "Capturing batches (bs=24 avail_mem=55.19 GB):  65%|██████▌   | 13/20 [00:00<00:00, 21.68it/s]\r",
      "Capturing batches (bs=16 avail_mem=55.19 GB):  65%|██████▌   | 13/20 [00:00<00:00, 21.68it/s]\r",
      "Capturing batches (bs=12 avail_mem=55.18 GB):  65%|██████▌   | 13/20 [00:00<00:00, 21.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=12 avail_mem=55.18 GB):  80%|████████  | 16/20 [00:00<00:00, 20.01it/s]\r",
      "Capturing batches (bs=8 avail_mem=55.18 GB):  80%|████████  | 16/20 [00:00<00:00, 20.01it/s] \r",
      "Capturing batches (bs=4 avail_mem=55.17 GB):  80%|████████  | 16/20 [00:00<00:00, 20.01it/s]\r",
      "Capturing batches (bs=2 avail_mem=55.17 GB):  80%|████████  | 16/20 [00:00<00:00, 20.01it/s]\r",
      "Capturing batches (bs=2 avail_mem=55.17 GB):  95%|█████████▌| 19/20 [00:00<00:00, 22.38it/s]\r",
      "Capturing batches (bs=1 avail_mem=55.16 GB):  95%|█████████▌| 19/20 [00:00<00:00, 22.38it/s]\r",
      "Capturing batches (bs=1 avail_mem=55.16 GB): 100%|██████████| 20/20 [00:01<00:00, 19.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# launch the offline engine\n",
    "import asyncio\n",
    "\n",
    "import sglang as sgl\n",
    "import sglang.test.doc_patch\n",
    "from sglang.utils import async_stream_and_merge, stream_and_merge\n",
    "\n",
    "llm = sgl.Engine(model_path=\"qwen/qwen2.5-0.5b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T02:14:24.448381Z",
     "iopub.status.busy": "2026-01-08T02:14:24.447402Z",
     "iopub.status.idle": "2026-01-08T02:14:25.530841Z",
     "shell.execute_reply": "2026-01-08T02:14:25.529908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Hello, my name is\n",
      "Generated text:  Alex and I'm a 15-year-old Scottish girl who is really good at talking about some of the things that make me happy and sad. I used to be really good at drawing and I love to write poetry. I have a really neat little interest in vintage music and I often have to use my imagination to come up with new ideas. \n",
      "\n",
      "I also enjoy playing frisbee and having parties with my friends and I love to write some of the songs on the concert music I make. \n",
      "\n",
      "Now, I am going to ask you questions, but I'm not sure if I'm ready to answer them yet. I'm sorry\n",
      "===============================\n",
      "Prompt: The president of the United States is\n",
      "Generated text:  seeking an answer to the question: \"What is the capital city of a country with a population of 2,500,000?\" What is the answer?\n",
      "\n",
      "The capital city of a country with a population of 2,500,000 is Washington, D.C. \n",
      "\n",
      "To break it down:\n",
      "\n",
      "1. The question is asking for the capital city of a country with a population of 2,500,000.\n",
      "2. The capital city of the United States is Washington, D.C.\n",
      "3. The question is asking for the capital city of a country with a population of\n",
      "===============================\n",
      "Prompt: The capital of France is\n",
      "Generated text:  Paris, a city with a rich history, fascinating architecture, and extensive museums. It's a popular tourist destination, with many attractions to see and experience. Some of the most notable landmarks in Paris include the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Champs-Élysées.\n",
      "\n",
      "The city is known for its beautiful gardens, including the Palace of Versailles and the Tuileries Gardens, which are famous for their opulent gardens and opulent interiors. The city is also famous for its fashion industry, including the famed couture houses of Versace and Balenciaga.\n",
      "\n",
      "Paris is a city\n",
      "===============================\n",
      "Prompt: The future of AI is\n",
      "Generated text:  poised to be revolutionized by big data and AI. To prepare for this, businesses need to develop a solid understanding of the potential for AI in their organization. This guide provides a roadmap for identifying the potential benefits and risks of AI and offers practical tips for implementing AI in business. Additionally, this guide offers a list of key resources for learning about AI, its applications, and best practices.\n",
      "Big data is a term that is often used interchangeably with AI. While they are related, they are not the same thing. AI is the process of developing and applying algorithms and machine learning to automate decision-making processes. Big data, on the other\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T02:14:25.533032Z",
     "iopub.status.busy": "2026-01-08T02:14:25.532813Z",
     "iopub.status.idle": "2026-01-08T02:14:26.366256Z",
     "shell.execute_reply": "2026-01-08T02:14:26.358529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing synchronous streaming generation with overlap removal ===\n",
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  [Name], and I'm a [Job Title] at [Company Name]. I'm excited to meet you and learn more about your interests and passions. What can you tell me about yourself? I'm a [insert a short, positive description of your personality or skills]. And what's your favorite hobby or activity? I love [insert a short, positive description of your hobby or activity]. And what's your favorite book or movie? I love [insert a short, positive description of your favorite book or movie]. And what's your favorite place to go? I love [insert a short, positive description of your favorite place]. And\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  Paris, the city known for its iconic Eiffel Tower and its rich history dating back to the Middle Ages. It is the largest city in France and the third-largest city in the world by population. Paris is also the capital of the French department of Paris and the seat of the French government. The city is famous for its art, music, and cuisine, and is home to many world-renowned landmarks such as the Louvre Museum and the Notre-Dame Cathedral. Paris is a cultural and economic hub of France and a major tourist destination. It is also known for its fashion industry, with many famous designers and fashion houses operating in\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  likely to be characterized by rapid advancements in areas such as machine learning, natural language processing, and computer vision. These technologies will continue to improve, leading to more sophisticated and accurate AI systems that can perform a wide range of tasks with increasing accuracy and efficiency. Some potential future trends in AI include:\n",
      "\n",
      "1. Increased integration with human intelligence: As AI becomes more sophisticated, it is likely to become more integrated with human intelligence, allowing for more complex and nuanced interactions between humans and machines.\n",
      "\n",
      "2. Greater emphasis on ethical considerations: As AI systems become more sophisticated, there will be a greater emphasis on ethical considerations, including issues such as bias, privacy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "print(\"\\n=== Testing synchronous streaming generation with overlap removal ===\\n\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    merged_output = stream_and_merge(llm, prompt, sampling_params)\n",
    "    print(\"Generated text:\", merged_output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T02:14:26.371807Z",
     "iopub.status.busy": "2026-01-08T02:14:26.371570Z",
     "iopub.status.idle": "2026-01-08T02:14:26.597657Z",
     "shell.execute_reply": "2026-01-08T02:14:26.596843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing asynchronous batch generation ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n",
      "Generated text:  Jane. I'm a freelance writer and editor, with a degree in English and a passion for storytelling. I love exploring the world through writing and have written numerous books and articles. I'm a listener and listener to listeners, and I love helping others find their voice and express themselves in their own unique way. I'm always looking for new opportunities to learn and grow as a writer and editor. Thanks for listening. [Your Name] [Your Address] [Your Phone Number] [Your Email Address] [Your LinkedIn Profile Link] [Your Portfolio Link] [Your Twitter Profile Link] [Your Instagram Profile Link] [Your Facebook Profile\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n",
      "Generated text:  Paris. \n",
      "\n",
      "(A) The capital of France is Paris. \n",
      "(B) Paris is the capital of France. \n",
      "(C) The capital of France is Paris. \n",
      "(D) Paris is the capital.\n",
      "\n",
      "To determine the correct answer, let's analyze each option step by step:\n",
      "\n",
      "1. **Option A: The capital of France is Paris.**\n",
      "   - This statement is accurate because the capital city of France is indeed Paris.\n",
      "\n",
      "2. **Option B: Paris is the capital of France.**\n",
      "   - This statement is not entirely correct. While Paris is the capital, it is not the only capital city. Other important cities include:\n",
      "    \n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n",
      "Generated text:  likely to involve a wide range of trends and developments, including:\n",
      "\n",
      "1. Increased integration with other technologies: AI is likely to be integrated with other technologies such as blockchain, IoT, and the Internet of Things (IoT) to create new possibilities.\n",
      "\n",
      "2. Enhanced privacy and security: As AI is becoming more sophisticated, it is likely to require more advanced privacy and security measures to protect user data.\n",
      "\n",
      "3. Enhanced emotional intelligence: AI is likely to become more capable of understanding and empathizing with human emotions.\n",
      "\n",
      "4. Greater automation and efficiency: AI is likely to become more prevalent in industry and businesses, leading to greater automation and efficiency.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous batch generation ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    outputs = await llm.async_generate(prompts, sampling_params)\n",
    "\n",
    "    for prompt, output in zip(prompts, outputs):\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated text: {output['text']}\")\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T02:14:26.600497Z",
     "iopub.status.busy": "2026-01-08T02:14:26.600199Z",
     "iopub.status.idle": "2026-01-08T02:14:27.216008Z",
     "shell.execute_reply": "2026-01-08T02:14:27.215256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing asynchronous streaming generation (no repeats) ===\n",
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " name"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " profession"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " occupation"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " who has"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " always had"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " an interest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " personal interest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hobby"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特长"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I've"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " been"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " working hard"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to develop"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my skills"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and expertise"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " field"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " am"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " always"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eager"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " learn"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " always"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " looking"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " opportunities"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " improve"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " abilities"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " contribute"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " community"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " So"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", what"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hobby"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hobby"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特长"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " And"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " how"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " do"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " plan"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " skills"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " knowledge"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " make"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " positive"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " impact"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " world"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " answer"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " question"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hope"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " can"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " talk"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " further"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " about"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Do"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " have any"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hobbies or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " interests?"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " often"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " referred"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Light"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " due"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " its"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vibrant"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " colorful"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " atmosphere"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". It"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " located"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ine"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " River in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the south"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of France"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " heart"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of French"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " culture and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " government."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " home to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " several UNESCO"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " World Heritage"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sites"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " including"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Notre"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-D"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ame"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cathedral"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the Lou"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vre Museum"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the E"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iff"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tower"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is also"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " one of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " most populous"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cities in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the world"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " home"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " many"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " France"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s wealthiest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " residents"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " its"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rich"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " history"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " beautiful"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " architecture"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vibrant culture"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " truly unique"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fascinating"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " second-largest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " city in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Europe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " full"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " potential"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " exciting"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " possibilities"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " but"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " there"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " are"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " also"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " concerns about"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the impact"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " society"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " environment"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " economy."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " are some"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " possible future"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trends"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " artificial"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " intelligence"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Increased automation"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": As"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " becomes"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " advanced"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and widespread"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", we"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " may"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " see"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more jobs"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " automated"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " leading"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a re"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " our work"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " environment and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " how we"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create value"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for education"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " being"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " develop"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " effective"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " teaching tools"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and learning"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " technologies that"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " can"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " help"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " students"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " learn"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " at"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " their"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " own"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pace and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pace.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " healthcare"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " already"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " being"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " develop"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accurate"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " personalized"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " diagnostic"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tools,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " but"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " has"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous streaming generation (no repeats) ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(\"Generated text: \", end=\"\", flush=True)\n",
    "\n",
    "        # Replace direct calls to async_generate with our custom overlap-aware version\n",
    "        async for cleaned_chunk in async_stream_and_merge(llm, prompt, sampling_params):\n",
    "            print(cleaned_chunk, end=\"\", flush=True)\n",
    "\n",
    "        print()  # New line after each prompt\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T02:14:27.217674Z",
     "iopub.status.busy": "2026-01-08T02:14:27.217506Z",
     "iopub.status.idle": "2026-01-08T02:14:27.228252Z",
     "shell.execute_reply": "2026-01-08T02:14:27.227619Z"
    }
   },
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
