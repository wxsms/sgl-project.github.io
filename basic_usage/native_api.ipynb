{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGLang Native APIs\n",
    "\n",
    "Apart from the OpenAI compatible APIs, the SGLang Runtime also provides its native server APIs. We introduce the following APIs:\n",
    "\n",
    "- `/generate` (text generation model)\n",
    "- `/get_model_info`\n",
    "- `/get_server_info`\n",
    "- `/health`\n",
    "- `/health_generate`\n",
    "- `/flush_cache`\n",
    "- `/update_weights`\n",
    "- `/encode`(embedding model)\n",
    "- `/v1/rerank`(cross encoder rerank model)\n",
    "- `/v1/score`(decoder-only scoring)\n",
    "- `/classify`(reward model)\n",
    "- `/start_expert_distribution_record`\n",
    "- `/stop_expert_distribution_record`\n",
    "- `/dump_expert_distribution_record`\n",
    "- `/tokenize`\n",
    "- `/detokenize`\n",
    "- A full list of these APIs can be found at [http_server.py](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server.py)\n",
    "\n",
    "We mainly use `requests` to test these APIs in the following examples. You can also use `curl`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch A Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:05:23.552075Z",
     "iopub.status.busy": "2026-01-31T08:05:23.551961Z",
     "iopub.status.idle": "2026-01-31T08:05:57.878584Z",
     "shell.execute_reply": "2026-01-31T08:05:57.877772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:05:28] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:05:28] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:05:28] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:05:34] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:05:34] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:05:34] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:05:36] INFO server_args.py:1774: Attention backend not specified. Use fa3 backend by default.\n",
      "[2026-01-31 08:05:36] INFO server_args.py:2700: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:05:42] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:05:42] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:05:42] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:05:43] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:05:43] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:05:43] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:05:49] Ignore import error when loading sglang.srt.models.glm_ocr: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:05:49] Ignore import error when loading sglang.srt.models.glm_ocr_nextn: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:05:49] Ignore import error when loading sglang.srt.models.glmasr: cannot import name 'GlmAsrConfig' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.45it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.45it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=77.02 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=77.02 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.23it/s]\r",
      "Capturing batches (bs=2 avail_mem=76.96 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.23it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.96 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.23it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.96 GB): 100%|██████████| 3/3 [00:00<00:00,  6.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang.test.doc_patch import launch_server_cmd\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --host 0.0.0.0 --log-level warning\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate (text generation model)\n",
    "Generate completions. This is similar to the `/v1/completions` in OpenAI API. Detailed parameters can be found in the [sampling parameters](sampling_params.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:05:57.880875Z",
     "iopub.status.busy": "2026-01-31T08:05:57.880491Z",
     "iopub.status.idle": "2026-01-31T08:05:58.205998Z",
     "shell.execute_reply": "2026-01-31T08:05:58.205372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': ' Paris\\n\\nThe capital of France is Paris. Saint-Sulpice Church, which serves as the cathedral of Paris, is the largest church building in Europe, and it is situated in the Eiffel Tower. The city offers a tourist destination in France which is known as a stopover destination.\\nPlease note that the size of certain architectural landmarks is subject to change. Paris is quite a different city from other places of interest. There are several museums Paris such as Musée Eiffel (Sculpture Tower), Musée Rodin (Louvre), and Musée Matisse (Puteaux).\\nThe Louis XIV, King', 'output_ids': [12095, 271, 785, 6722, 315, 9625, 374, 12095, 13, 14205, 6222, 12840, 558, 9257, 11, 892, 17045, 438, 279, 79150, 315, 12095, 11, 374, 279, 7772, 8817, 4752, 304, 4505, 11, 323, 432, 374, 30083, 304, 279, 468, 3092, 301, 21938, 13, 576, 3283, 6081, 264, 29970, 9106, 304, 9625, 892, 374, 3881, 438, 264, 2936, 1975, 9106, 624, 5501, 5185, 429, 279, 1379, 315, 3654, 42463, 59924, 374, 3832, 311, 2297, 13, 12095, 374, 5008, 264, 2155, 3283, 504, 1008, 7482, 315, 2734, 13, 2619, 525, 3807, 50577, 12095, 1741, 438, 5331, 7888, 468, 3092, 301, 320, 50, 3314, 417, 552, 21938, 701, 5331, 7888, 13308, 258, 320, 92806, 48506, 701, 323, 5331, 7888, 386, 3605, 325, 320, 47, 1070, 11981, 4292, 785, 11876, 93415, 11, 6210], 'meta_info': {'id': 'cd60f9b7ee80497c8404bc41d164ab6b', 'finish_reason': {'type': 'length', 'length': 128}, 'prompt_tokens': 7, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 128, 'cached_tokens': 0, 'e2e_latency': 0.318087100982666, 'response_sent_to_client_ts': 1769846758.2022471}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = f\"http://localhost:{port}/generate\"\n",
    "data = {\"text\": \"What is the capital of France?\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Info\n",
    "\n",
    "Get the information of the model.\n",
    "\n",
    "- `model_path`: The path/name of the model.\n",
    "- `is_generation`: Whether the model is used as generation model or embedding model.\n",
    "- `tokenizer_path`: The path/name of the tokenizer.\n",
    "- `preferred_sampling_params`: The default sampling params specified via `--preferred-sampling-params`. `None` is returned in this example as we did not explicitly configure it in server args.\n",
    "- `weight_version`: This field contains the version of the model weights. This is often used to track changes or updates to the model’s trained parameters.\n",
    "- `has_image_understanding`: Whether the model has image-understanding capability.\n",
    "- `has_audio_understanding`: Whether the model has audio-understanding capability.\n",
    "- `model_type`: The model type from the HuggingFace config (e.g., \"qwen2\", \"llama\").\n",
    "- `architectures`: The model architectures from the HuggingFace config (e.g., [\"Qwen2ForCausalLM\"])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:05:58.207392Z",
     "iopub.status.busy": "2026-01-31T08:05:58.207275Z",
     "iopub.status.idle": "2026-01-31T08:05:58.214103Z",
     "shell.execute_reply": "2026-01-31T08:05:58.213431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:05:58] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'model_path': 'qwen/qwen2.5-0.5b-instruct', 'tokenizer_path': 'qwen/qwen2.5-0.5b-instruct', 'is_generation': True, 'preferred_sampling_params': None, 'weight_version': 'default', 'has_image_understanding': False, 'has_audio_understanding': False, 'model_type': 'qwen2', 'architectures': ['Qwen2ForCausalLM']}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/get_model_info\"\n",
    "\n",
    "response = requests.get(url)\n",
    "response_json = response.json()\n",
    "print_highlight(response_json)\n",
    "assert response_json[\"model_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\n",
    "assert response_json[\"is_generation\"] is True\n",
    "assert response_json[\"tokenizer_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\n",
    "assert response_json[\"preferred_sampling_params\"] is None\n",
    "assert response_json.keys() == {\n",
    "    \"model_path\",\n",
    "    \"is_generation\",\n",
    "    \"tokenizer_path\",\n",
    "    \"preferred_sampling_params\",\n",
    "    \"weight_version\",\n",
    "    \"has_image_understanding\",\n",
    "    \"has_audio_understanding\",\n",
    "    \"model_type\",\n",
    "    \"architectures\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Server Info\n",
    "Gets the server information including CLI arguments, token limits, and memory pool sizes.\n",
    "- Note: `get_server_info` merges the following deprecated endpoints:\n",
    "  - `get_server_args`\n",
    "  - `get_memory_pool_size`\n",
    "  - `get_max_total_num_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:05:58.215392Z",
     "iopub.status.busy": "2026-01-31T08:05:58.215275Z",
     "iopub.status.idle": "2026-01-31T08:05:58.223489Z",
     "shell.execute_reply": "2026-01-31T08:05:58.222865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:05:58] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"model_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_mode\":\"auto\",\"tokenizer_worker_num\":1,\"skip_tokenizer_init\":false,\"load_format\":\"auto\",\"model_loader_extra_config\":\"{}\",\"trust_remote_code\":false,\"context_length\":null,\"is_embedding\":false,\"enable_multimodal\":null,\"revision\":null,\"model_impl\":\"auto\",\"host\":\"0.0.0.0\",\"port\":38822,\"fastapi_root_path\":\"\",\"grpc_mode\":false,\"skip_server_warmup\":false,\"warmups\":null,\"nccl_port\":null,\"checkpoint_engine_wait_weights_before_ready\":false,\"dtype\":\"auto\",\"quantization\":null,\"quantization_param_path\":null,\"kv_cache_dtype\":\"auto\",\"enable_fp32_lm_head\":false,\"modelopt_quant\":null,\"modelopt_checkpoint_restore_path\":null,\"modelopt_checkpoint_save_path\":null,\"modelopt_export_path\":null,\"quantize_and_serve\":false,\"rl_quant_profile\":null,\"mem_fraction_static\":0.841,\"max_running_requests\":128,\"max_queued_requests\":null,\"max_total_tokens\":20480,\"chunked_prefill_size\":8192,\"enable_dynamic_chunking\":false,\"max_prefill_tokens\":16384,\"prefill_max_requests\":null,\"schedule_policy\":\"fcfs\",\"enable_priority_scheduling\":false,\"abort_on_priority_when_disabled\":false,\"schedule_low_priority_values_first\":false,\"priority_scheduling_preemption_threshold\":10,\"schedule_conservativeness\":1.0,\"page_size\":1,\"swa_full_tokens_ratio\":0.8,\"disable_hybrid_swa_memory\":false,\"radix_eviction_policy\":\"lru\",\"enable_prefill_delayer\":false,\"prefill_delayer_max_delay_passes\":30,\"prefill_delayer_token_usage_low_watermark\":null,\"prefill_delayer_forward_passes_buckets\":null,\"prefill_delayer_wait_seconds_buckets\":null,\"device\":\"cuda\",\"tp_size\":1,\"pp_size\":1,\"pp_max_micro_batch_size\":null,\"pp_async_batch_depth\":0,\"stream_interval\":1,\"stream_output\":false,\"random_seed\":476796351,\"constrained_json_whitespace_pattern\":null,\"constrained_json_disable_any_whitespace\":false,\"watchdog_timeout\":300,\"soft_watchdog_timeout\":300,\"dist_timeout\":null,\"download_dir\":null,\"model_checksum\":null,\"base_gpu_id\":0,\"gpu_id_step\":1,\"sleep_on_idle\":false,\"custom_sigquit_handler\":null,\"log_level\":\"warning\",\"log_level_http\":null,\"log_requests\":false,\"log_requests_level\":2,\"log_requests_format\":\"text\",\"log_requests_target\":null,\"uvicorn_access_log_exclude_prefixes\":[],\"crash_dump_folder\":null,\"show_time_cost\":false,\"enable_metrics\":false,\"enable_metrics_for_all_schedulers\":false,\"tokenizer_metrics_custom_labels_header\":\"x-custom-labels\",\"tokenizer_metrics_allowed_custom_labels\":null,\"bucket_time_to_first_token\":null,\"bucket_inter_token_latency\":null,\"bucket_e2e_request_latency\":null,\"collect_tokens_histogram\":false,\"prompt_tokens_buckets\":null,\"generation_tokens_buckets\":null,\"gc_warning_threshold_secs\":0.0,\"decode_log_interval\":40,\"enable_request_time_stats_logging\":false,\"kv_events_config\":null,\"enable_trace\":false,\"otlp_traces_endpoint\":\"localhost:4317\",\"export_metrics_to_file\":false,\"export_metrics_to_file_dir\":null,\"api_key\":null,\"admin_api_key\":null,\"served_model_name\":\"qwen/qwen2.5-0.5b-instruct\",\"weight_version\":\"default\",\"chat_template\":null,\"hf_chat_template_name\":null,\"completion_template\":null,\"file_storage_path\":\"sglang_storage\",\"enable_cache_report\":false,\"reasoning_parser\":null,\"tool_call_parser\":null,\"tool_server\":null,\"sampling_defaults\":\"model\",\"dp_size\":1,\"load_balance_method\":\"round_robin\",\"dist_init_addr\":null,\"nnodes\":1,\"node_rank\":0,\"json_model_override_args\":\"{}\",\"preferred_sampling_params\":null,\"enable_lora\":null,\"enable_lora_overlap_loading\":null,\"max_lora_rank\":null,\"lora_target_modules\":null,\"lora_paths\":null,\"max_loaded_loras\":null,\"max_loras_per_batch\":8,\"lora_eviction_policy\":\"lru\",\"lora_backend\":\"csgmv\",\"max_lora_chunk_size\":16,\"attention_backend\":\"fa3\",\"decode_attention_backend\":null,\"prefill_attention_backend\":null,\"sampling_backend\":\"flashinfer\",\"grammar_backend\":\"xgrammar\",\"mm_attention_backend\":null,\"fp8_gemm_runner_backend\":\"auto\",\"fp4_gemm_runner_backend\":\"auto\",\"nsa_prefill_backend\":null,\"nsa_decode_backend\":null,\"disable_flashinfer_autotune\":false,\"speculative_algorithm\":null,\"speculative_draft_model_path\":null,\"speculative_draft_model_revision\":null,\"speculative_draft_load_format\":null,\"speculative_num_steps\":null,\"speculative_eagle_topk\":null,\"speculative_num_draft_tokens\":null,\"speculative_accept_threshold_single\":1.0,\"speculative_accept_threshold_acc\":1.0,\"speculative_token_map\":null,\"speculative_attention_mode\":\"prefill\",\"speculative_draft_attention_backend\":null,\"speculative_moe_runner_backend\":\"auto\",\"speculative_moe_a2a_backend\":null,\"speculative_draft_model_quantization\":null,\"speculative_ngram_min_match_window_size\":1,\"speculative_ngram_max_match_window_size\":12,\"speculative_ngram_min_bfs_breadth\":1,\"speculative_ngram_max_bfs_breadth\":10,\"speculative_ngram_match_type\":\"BFS\",\"speculative_ngram_branch_length\":18,\"speculative_ngram_capacity\":10000000,\"enable_multi_layer_eagle\":false,\"ep_size\":1,\"moe_a2a_backend\":\"none\",\"moe_runner_backend\":\"auto\",\"flashinfer_mxfp4_moe_precision\":\"default\",\"enable_flashinfer_allreduce_fusion\":false,\"deepep_mode\":\"auto\",\"ep_num_redundant_experts\":0,\"ep_dispatch_algorithm\":null,\"init_expert_location\":\"trivial\",\"enable_eplb\":false,\"eplb_algorithm\":\"auto\",\"eplb_rebalance_num_iterations\":1000,\"eplb_rebalance_layers_per_chunk\":null,\"eplb_min_rebalancing_utilization_threshold\":1.0,\"expert_distribution_recorder_mode\":null,\"expert_distribution_recorder_buffer_size\":1000,\"enable_expert_distribution_metrics\":false,\"deepep_config\":null,\"moe_dense_tp_size\":null,\"elastic_ep_backend\":null,\"mooncake_ib_device\":null,\"max_mamba_cache_size\":null,\"mamba_ssm_dtype\":\"float32\",\"mamba_full_memory_ratio\":0.9,\"mamba_scheduler_strategy\":\"no_buffer\",\"mamba_track_interval\":256,\"enable_hierarchical_cache\":false,\"hicache_ratio\":2.0,\"hicache_size\":0,\"hicache_write_policy\":\"write_through\",\"hicache_io_backend\":\"kernel\",\"hicache_mem_layout\":\"layer_first\",\"disable_hicache_numa_detect\":false,\"hicache_storage_backend\":null,\"hicache_storage_prefetch_policy\":\"best_effort\",\"hicache_storage_backend_extra_config\":null,\"hierarchical_sparse_attention_extra_config\":null,\"enable_lmcache\":false,\"kt_weight_path\":null,\"kt_method\":\"AMXINT4\",\"kt_cpuinfer\":null,\"kt_threadpool_count\":2,\"kt_num_gpu_experts\":null,\"kt_max_deferred_experts_per_token\":null,\"dllm_algorithm\":null,\"dllm_algorithm_config\":null,\"enable_double_sparsity\":false,\"ds_channel_config_path\":null,\"ds_heavy_channel_num\":32,\"ds_heavy_token_num\":256,\"ds_heavy_channel_type\":\"qk\",\"ds_sparse_decode_threshold\":4096,\"cpu_offload_gb\":0,\"offload_group_size\":-1,\"offload_num_in_group\":1,\"offload_prefetch_step\":1,\"offload_mode\":\"cpu\",\"multi_item_scoring_delimiter\":null,\"disable_radix_cache\":false,\"cuda_graph_max_bs\":4,\"cuda_graph_bs\":[1,2,4],\"disable_cuda_graph\":false,\"disable_cuda_graph_padding\":false,\"enable_profile_cuda_graph\":false,\"enable_cudagraph_gc\":false,\"enable_layerwise_nvtx_marker\":false,\"enable_nccl_nvls\":false,\"enable_symm_mem\":false,\"disable_flashinfer_cutlass_moe_fp4_allgather\":false,\"enable_tokenizer_batch_encode\":false,\"disable_tokenizer_batch_decode\":false,\"disable_outlines_disk_cache\":false,\"disable_custom_all_reduce\":false,\"enable_mscclpp\":false,\"enable_torch_symm_mem\":false,\"disable_overlap_schedule\":false,\"enable_mixed_chunk\":false,\"enable_dp_attention\":false,\"enable_dp_lm_head\":false,\"enable_two_batch_overlap\":false,\"enable_single_batch_overlap\":false,\"tbo_token_distribution_threshold\":0.48,\"enable_torch_compile\":false,\"enable_piecewise_cuda_graph\":false,\"enable_torch_compile_debug_mode\":false,\"torch_compile_max_bs\":32,\"piecewise_cuda_graph_max_tokens\":8192,\"piecewise_cuda_graph_tokens\":[4,8,12,16,20,24,28,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256,288,320,352,384,416,448,480,512,576,640,704,768,832,896,960,1024,1280,1536,1792,2048,2304,2560,2816,3072,3328,3584,3840,4096,4608,5120,5632,6144,6656,7168,7680,8192],\"piecewise_cuda_graph_compiler\":\"eager\",\"torchao_config\":\"\",\"enable_nan_detection\":false,\"enable_p2p_check\":false,\"triton_attention_reduce_in_fp32\":false,\"triton_attention_num_kv_splits\":8,\"triton_attention_split_tile_size\":null,\"num_continuous_decode_steps\":1,\"delete_ckpt_after_loading\":false,\"enable_memory_saver\":false,\"enable_weights_cpu_backup\":false,\"enable_draft_weights_cpu_backup\":false,\"allow_auto_truncate\":false,\"enable_custom_logit_processor\":false,\"flashinfer_mla_disable_ragged\":false,\"disable_shared_experts_fusion\":false,\"disable_chunked_prefix_cache\":false,\"disable_fast_image_processor\":false,\"keep_mm_feature_on_device\":false,\"enable_return_hidden_states\":false,\"enable_return_routed_experts\":false,\"scheduler_recv_interval\":1,\"numa_node\":null,\"enable_deterministic_inference\":false,\"rl_on_policy_target\":null,\"enable_attn_tp_input_scattered\":false,\"enable_nsa_prefill_context_parallel\":false,\"nsa_prefill_cp_mode\":\"in-seq-split\",\"enable_fused_qk_norm_rope\":false,\"enable_precise_embedding_interpolation\":false,\"enable_dynamic_batch_tokenizer\":false,\"dynamic_batch_tokenizer_batch_size\":32,\"dynamic_batch_tokenizer_batch_timeout\":0.002,\"debug_tensor_dump_output_folder\":null,\"debug_tensor_dump_layers\":null,\"debug_tensor_dump_input_file\":null,\"debug_tensor_dump_inject\":false,\"disaggregation_mode\":\"null\",\"disaggregation_transfer_backend\":\"mooncake\",\"disaggregation_bootstrap_port\":8998,\"disaggregation_decode_tp\":null,\"disaggregation_decode_dp\":null,\"disaggregation_prefill_pp\":1,\"disaggregation_ib_device\":null,\"disaggregation_decode_enable_offload_kvcache\":false,\"disaggregation_decode_enable_fake_auto\":false,\"num_reserved_decode_tokens\":512,\"disaggregation_decode_polling_interval\":1,\"encoder_only\":false,\"language_only\":false,\"encoder_transfer_backend\":\"zmq_to_scheduler\",\"encoder_urls\":[],\"custom_weight_loader\":[],\"weight_loader_disable_mmap\":false,\"remote_instance_weight_loader_seed_instance_ip\":null,\"remote_instance_weight_loader_seed_instance_service_port\":null,\"remote_instance_weight_loader_send_weights_group_ports\":null,\"remote_instance_weight_loader_backend\":\"nccl\",\"remote_instance_weight_loader_start_seed_via_transfer_engine\":false,\"enable_pdmux\":false,\"pdmux_config_path\":null,\"sm_group_num\":8,\"mm_max_concurrent_calls\":32,\"mm_per_request_timeout\":10.0,\"enable_broadcast_mm_inputs_process\":false,\"enable_prefix_mm_cache\":false,\"mm_enable_dp_encoder\":false,\"mm_process_config\":{},\"limit_mm_data_per_request\":null,\"decrypted_config_file\":null,\"decrypted_draft_config_file\":null,\"forward_hooks\":null,\"status\":\"ready\",\"max_total_num_tokens\":20480,\"max_req_input_len\":20474,\"internal_states\":[{\"model_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_mode\":\"auto\",\"tokenizer_worker_num\":1,\"skip_tokenizer_init\":false,\"load_format\":\"auto\",\"model_loader_extra_config\":\"{}\",\"trust_remote_code\":false,\"context_length\":null,\"is_embedding\":false,\"enable_multimodal\":null,\"revision\":null,\"model_impl\":\"auto\",\"host\":\"0.0.0.0\",\"port\":38822,\"fastapi_root_path\":\"\",\"grpc_mode\":false,\"skip_server_warmup\":false,\"warmups\":null,\"nccl_port\":null,\"checkpoint_engine_wait_weights_before_ready\":false,\"dtype\":\"auto\",\"quantization\":null,\"quantization_param_path\":null,\"kv_cache_dtype\":\"auto\",\"enable_fp32_lm_head\":false,\"modelopt_quant\":null,\"modelopt_checkpoint_restore_path\":null,\"modelopt_checkpoint_save_path\":null,\"modelopt_export_path\":null,\"quantize_and_serve\":false,\"rl_quant_profile\":null,\"mem_fraction_static\":0.841,\"max_running_requests\":128,\"max_queued_requests\":null,\"max_total_tokens\":20480,\"chunked_prefill_size\":8192,\"enable_dynamic_chunking\":false,\"max_prefill_tokens\":16384,\"prefill_max_requests\":null,\"schedule_policy\":\"fcfs\",\"enable_priority_scheduling\":false,\"abort_on_priority_when_disabled\":false,\"schedule_low_priority_values_first\":false,\"priority_scheduling_preemption_threshold\":10,\"schedule_conservativeness\":1.0,\"page_size\":1,\"swa_full_tokens_ratio\":0.8,\"disable_hybrid_swa_memory\":false,\"radix_eviction_policy\":\"lru\",\"enable_prefill_delayer\":false,\"prefill_delayer_max_delay_passes\":30,\"prefill_delayer_token_usage_low_watermark\":null,\"prefill_delayer_forward_passes_buckets\":null,\"prefill_delayer_wait_seconds_buckets\":null,\"device\":\"cuda\",\"tp_size\":1,\"pp_size\":1,\"pp_max_micro_batch_size\":128,\"pp_async_batch_depth\":0,\"stream_interval\":1,\"stream_output\":false,\"random_seed\":476796351,\"constrained_json_whitespace_pattern\":null,\"constrained_json_disable_any_whitespace\":false,\"watchdog_timeout\":300,\"soft_watchdog_timeout\":300,\"dist_timeout\":null,\"download_dir\":null,\"model_checksum\":null,\"base_gpu_id\":0,\"gpu_id_step\":1,\"sleep_on_idle\":false,\"custom_sigquit_handler\":null,\"log_level\":\"warning\",\"log_level_http\":null,\"log_requests\":false,\"log_requests_level\":2,\"log_requests_format\":\"text\",\"log_requests_target\":null,\"uvicorn_access_log_exclude_prefixes\":[],\"crash_dump_folder\":null,\"show_time_cost\":false,\"enable_metrics\":false,\"enable_metrics_for_all_schedulers\":false,\"tokenizer_metrics_custom_labels_header\":\"x-custom-labels\",\"tokenizer_metrics_allowed_custom_labels\":null,\"bucket_time_to_first_token\":null,\"bucket_inter_token_latency\":null,\"bucket_e2e_request_latency\":null,\"collect_tokens_histogram\":false,\"prompt_tokens_buckets\":null,\"generation_tokens_buckets\":null,\"gc_warning_threshold_secs\":0.0,\"decode_log_interval\":40,\"enable_request_time_stats_logging\":false,\"kv_events_config\":null,\"enable_trace\":false,\"otlp_traces_endpoint\":\"localhost:4317\",\"export_metrics_to_file\":false,\"export_metrics_to_file_dir\":null,\"api_key\":null,\"admin_api_key\":null,\"served_model_name\":\"qwen/qwen2.5-0.5b-instruct\",\"weight_version\":\"default\",\"chat_template\":null,\"hf_chat_template_name\":null,\"completion_template\":null,\"file_storage_path\":\"sglang_storage\",\"enable_cache_report\":false,\"reasoning_parser\":null,\"tool_call_parser\":null,\"tool_server\":null,\"sampling_defaults\":\"model\",\"dp_size\":1,\"load_balance_method\":\"round_robin\",\"dist_init_addr\":null,\"nnodes\":1,\"node_rank\":0,\"json_model_override_args\":\"{}\",\"preferred_sampling_params\":null,\"enable_lora\":null,\"enable_lora_overlap_loading\":null,\"max_lora_rank\":null,\"lora_target_modules\":null,\"lora_paths\":null,\"max_loaded_loras\":null,\"max_loras_per_batch\":8,\"lora_eviction_policy\":\"lru\",\"lora_backend\":\"csgmv\",\"max_lora_chunk_size\":16,\"attention_backend\":\"fa3\",\"decode_attention_backend\":\"fa3\",\"prefill_attention_backend\":\"fa3\",\"sampling_backend\":\"flashinfer\",\"grammar_backend\":\"xgrammar\",\"mm_attention_backend\":null,\"fp8_gemm_runner_backend\":\"auto\",\"fp4_gemm_runner_backend\":\"auto\",\"nsa_prefill_backend\":null,\"nsa_decode_backend\":null,\"disable_flashinfer_autotune\":false,\"speculative_algorithm\":null,\"speculative_draft_model_path\":null,\"speculative_draft_model_revision\":null,\"speculative_draft_load_format\":null,\"speculative_num_steps\":null,\"speculative_eagle_topk\":null,\"speculative_num_draft_tokens\":null,\"speculative_accept_threshold_single\":1.0,\"speculative_accept_threshold_acc\":1.0,\"speculative_token_map\":null,\"speculative_attention_mode\":\"prefill\",\"speculative_draft_attention_backend\":null,\"speculative_moe_runner_backend\":\"auto\",\"speculative_moe_a2a_backend\":null,\"speculative_draft_model_quantization\":null,\"speculative_ngram_min_match_window_size\":1,\"speculative_ngram_max_match_window_size\":12,\"speculative_ngram_min_bfs_breadth\":1,\"speculative_ngram_max_bfs_breadth\":10,\"speculative_ngram_match_type\":\"BFS\",\"speculative_ngram_branch_length\":18,\"speculative_ngram_capacity\":10000000,\"enable_multi_layer_eagle\":false,\"ep_size\":1,\"moe_a2a_backend\":\"none\",\"moe_runner_backend\":\"auto\",\"flashinfer_mxfp4_moe_precision\":\"default\",\"enable_flashinfer_allreduce_fusion\":false,\"deepep_mode\":\"auto\",\"ep_num_redundant_experts\":0,\"ep_dispatch_algorithm\":null,\"init_expert_location\":\"trivial\",\"enable_eplb\":false,\"eplb_algorithm\":\"auto\",\"eplb_rebalance_num_iterations\":1000,\"eplb_rebalance_layers_per_chunk\":null,\"eplb_min_rebalancing_utilization_threshold\":1.0,\"expert_distribution_recorder_mode\":null,\"expert_distribution_recorder_buffer_size\":1000,\"enable_expert_distribution_metrics\":false,\"deepep_config\":null,\"moe_dense_tp_size\":null,\"elastic_ep_backend\":null,\"mooncake_ib_device\":null,\"max_mamba_cache_size\":null,\"mamba_ssm_dtype\":\"float32\",\"mamba_full_memory_ratio\":0.9,\"mamba_scheduler_strategy\":\"no_buffer\",\"mamba_track_interval\":256,\"enable_hierarchical_cache\":false,\"hicache_ratio\":2.0,\"hicache_size\":0,\"hicache_write_policy\":\"write_through\",\"hicache_io_backend\":\"kernel\",\"hicache_mem_layout\":\"layer_first\",\"disable_hicache_numa_detect\":false,\"hicache_storage_backend\":null,\"hicache_storage_prefetch_policy\":\"best_effort\",\"hicache_storage_backend_extra_config\":null,\"hierarchical_sparse_attention_extra_config\":null,\"enable_lmcache\":false,\"kt_weight_path\":null,\"kt_method\":\"AMXINT4\",\"kt_cpuinfer\":null,\"kt_threadpool_count\":2,\"kt_num_gpu_experts\":null,\"kt_max_deferred_experts_per_token\":null,\"dllm_algorithm\":null,\"dllm_algorithm_config\":null,\"enable_double_sparsity\":false,\"ds_channel_config_path\":null,\"ds_heavy_channel_num\":32,\"ds_heavy_token_num\":256,\"ds_heavy_channel_type\":\"qk\",\"ds_sparse_decode_threshold\":4096,\"cpu_offload_gb\":0,\"offload_group_size\":-1,\"offload_num_in_group\":1,\"offload_prefetch_step\":1,\"offload_mode\":\"cpu\",\"multi_item_scoring_delimiter\":null,\"disable_radix_cache\":false,\"cuda_graph_max_bs\":4,\"cuda_graph_bs\":[1,2,4],\"disable_cuda_graph\":false,\"disable_cuda_graph_padding\":false,\"enable_profile_cuda_graph\":false,\"enable_cudagraph_gc\":false,\"enable_layerwise_nvtx_marker\":false,\"enable_nccl_nvls\":false,\"enable_symm_mem\":false,\"disable_flashinfer_cutlass_moe_fp4_allgather\":false,\"enable_tokenizer_batch_encode\":false,\"disable_tokenizer_batch_decode\":false,\"disable_outlines_disk_cache\":false,\"disable_custom_all_reduce\":false,\"enable_mscclpp\":false,\"enable_torch_symm_mem\":false,\"disable_overlap_schedule\":false,\"enable_mixed_chunk\":false,\"enable_dp_attention\":false,\"enable_dp_lm_head\":false,\"enable_two_batch_overlap\":false,\"enable_single_batch_overlap\":false,\"tbo_token_distribution_threshold\":0.48,\"enable_torch_compile\":false,\"enable_piecewise_cuda_graph\":false,\"enable_torch_compile_debug_mode\":false,\"torch_compile_max_bs\":32,\"piecewise_cuda_graph_max_tokens\":8192,\"piecewise_cuda_graph_tokens\":[4,8,12,16,20,24,28,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256,288,320,352,384,416,448,480,512,576,640,704,768,832,896,960,1024,1280,1536,1792,2048,2304,2560,2816,3072,3328,3584,3840,4096,4608,5120,5632,6144,6656,7168,7680,8192],\"piecewise_cuda_graph_compiler\":\"eager\",\"torchao_config\":\"\",\"enable_nan_detection\":false,\"enable_p2p_check\":false,\"triton_attention_reduce_in_fp32\":false,\"triton_attention_num_kv_splits\":8,\"triton_attention_split_tile_size\":null,\"num_continuous_decode_steps\":1,\"delete_ckpt_after_loading\":false,\"enable_memory_saver\":false,\"enable_weights_cpu_backup\":false,\"enable_draft_weights_cpu_backup\":false,\"allow_auto_truncate\":false,\"enable_custom_logit_processor\":false,\"flashinfer_mla_disable_ragged\":false,\"disable_shared_experts_fusion\":false,\"disable_chunked_prefix_cache\":true,\"disable_fast_image_processor\":false,\"keep_mm_feature_on_device\":false,\"enable_return_hidden_states\":false,\"enable_return_routed_experts\":false,\"scheduler_recv_interval\":1,\"numa_node\":null,\"enable_deterministic_inference\":false,\"rl_on_policy_target\":null,\"enable_attn_tp_input_scattered\":false,\"enable_nsa_prefill_context_parallel\":false,\"nsa_prefill_cp_mode\":\"in-seq-split\",\"enable_fused_qk_norm_rope\":false,\"enable_precise_embedding_interpolation\":false,\"enable_dynamic_batch_tokenizer\":false,\"dynamic_batch_tokenizer_batch_size\":32,\"dynamic_batch_tokenizer_batch_timeout\":0.002,\"debug_tensor_dump_output_folder\":null,\"debug_tensor_dump_layers\":null,\"debug_tensor_dump_input_file\":null,\"debug_tensor_dump_inject\":false,\"disaggregation_mode\":\"null\",\"disaggregation_transfer_backend\":\"mooncake\",\"disaggregation_bootstrap_port\":8998,\"disaggregation_decode_tp\":null,\"disaggregation_decode_dp\":null,\"disaggregation_prefill_pp\":1,\"disaggregation_ib_device\":null,\"disaggregation_decode_enable_offload_kvcache\":false,\"disaggregation_decode_enable_fake_auto\":false,\"num_reserved_decode_tokens\":512,\"disaggregation_decode_polling_interval\":1,\"encoder_only\":false,\"language_only\":false,\"encoder_transfer_backend\":\"zmq_to_scheduler\",\"encoder_urls\":[],\"custom_weight_loader\":[],\"weight_loader_disable_mmap\":false,\"remote_instance_weight_loader_seed_instance_ip\":null,\"remote_instance_weight_loader_seed_instance_service_port\":null,\"remote_instance_weight_loader_send_weights_group_ports\":null,\"remote_instance_weight_loader_backend\":\"nccl\",\"remote_instance_weight_loader_start_seed_via_transfer_engine\":false,\"enable_pdmux\":false,\"pdmux_config_path\":null,\"sm_group_num\":8,\"mm_max_concurrent_calls\":32,\"mm_per_request_timeout\":10.0,\"enable_broadcast_mm_inputs_process\":false,\"enable_prefix_mm_cache\":false,\"mm_enable_dp_encoder\":false,\"mm_process_config\":{},\"limit_mm_data_per_request\":null,\"decrypted_config_file\":null,\"decrypted_draft_config_file\":null,\"forward_hooks\":null,\"use_mla_backend\":false,\"last_gen_throughput\":561.403162206633,\"memory_usage\":{\"weight\":0.98,\"kvcache\":0.23,\"token_capacity\":20480,\"graph\":0.07},\"effective_max_running_requests_per_dp\":128}],\"version\":\"0.0.0.dev1+g22498e10c\"}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/get_server_info\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Check\n",
    "- `/health`: Check the health of the server.\n",
    "- `/health_generate`: Check the health of the server by generating one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:05:58.224725Z",
     "iopub.status.busy": "2026-01-31T08:05:58.224608Z",
     "iopub.status.idle": "2026-01-31T08:05:59.231345Z",
     "shell.execute_reply": "2026-01-31T08:05:59.230686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/health_generate\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:05:59.232770Z",
     "iopub.status.busy": "2026-01-31T08:05:59.232653Z",
     "iopub.status.idle": "2026-01-31T08:06:00.237457Z",
     "shell.execute_reply": "2026-01-31T08:06:00.236793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/health\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flush Cache\n",
    "\n",
    "Flush the radix cache. It will be automatically triggered when the model weights are updated by the `/update_weights` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:06:00.238920Z",
     "iopub.status.busy": "2026-01-31T08:06:00.238803Z",
     "iopub.status.idle": "2026-01-31T08:06:00.246597Z",
     "shell.execute_reply": "2026-01-31T08:06:00.245930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Cache flushed.<br>Please check backend logs for more details. (When there are running or waiting requests, the operation will not be performed.)<br></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/flush_cache\"\n",
    "\n",
    "response = requests.post(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Weights From Disk\n",
    "\n",
    "Update model weights from disk without restarting the server. Only applicable for models with the same architecture and parameter size.\n",
    "\n",
    "SGLang support `update_weights_from_disk` API for continuous evaluation during training (save checkpoint to disk and update weights from disk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:06:00.247862Z",
     "iopub.status.busy": "2026-01-31T08:06:00.247747Z",
     "iopub.status.idle": "2026-01-31T08:06:00.693097Z",
     "shell.execute_reply": "2026-01-31T08:06:00.692440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.32it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.32it/s]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"success\":true,\"message\":\"Succeeded to update model weights.\",\"num_paused_requests\":0}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# successful update with same architecture and size\n",
    "\n",
    "url = f\"http://localhost:{port}/update_weights_from_disk\"\n",
    "data = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.text)\n",
    "assert response.json()[\"success\"] is True\n",
    "assert response.json()[\"message\"] == \"Succeeded to update model weights.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:06:00.694470Z",
     "iopub.status.busy": "2026-01-31T08:06:00.694355Z",
     "iopub.status.idle": "2026-01-31T08:06:00.795456Z",
     "shell.execute_reply": "2026-01-31T08:06:00.794854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:06:00] Failed to get weights iterator: qwen/qwen2.5-0.5b-instruct-wrong (repository not found).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'success': False, 'message': 'Failed to get weights iterator: qwen/qwen2.5-0.5b-instruct-wrong (repository not found).', 'num_paused_requests': 0}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# failed update with different parameter size or wrong name\n",
    "\n",
    "url = f\"http://localhost:{port}/update_weights_from_disk\"\n",
    "data = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct-wrong\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "print_highlight(response_json)\n",
    "assert response_json[\"success\"] is False\n",
    "assert response_json[\"message\"] == (\n",
    "    \"Failed to get weights iterator: \"\n",
    "    \"qwen/qwen2.5-0.5b-instruct-wrong\"\n",
    "    \" (repository not found).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:06:00.796844Z",
     "iopub.status.busy": "2026-01-31T08:06:00.796730Z",
     "iopub.status.idle": "2026-01-31T08:06:00.814181Z",
     "shell.execute_reply": "2026-01-31T08:06:00.813514Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode (embedding model)\n",
    "\n",
    "Encode text into embeddings. Note that this API is only available for [embedding models](openai_api_embeddings.ipynb) and will raise an error for generation models.\n",
    "Therefore, we launch a new server to server an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:06:00.815644Z",
     "iopub.status.busy": "2026-01-31T08:06:00.815532Z",
     "iopub.status.idle": "2026-01-31T08:06:35.916745Z",
     "shell.execute_reply": "2026-01-31T08:06:35.915912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:06:05] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:06:05] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:06:05] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:06:08] INFO model_config.py:1106: Downcasting torch.float32 to torch.float16.\n",
      "[2026-01-31 08:06:08] INFO server_args.py:1774: Attention backend not specified. Use fa3 backend by default.\n",
      "[2026-01-31 08:06:08] INFO server_args.py:2700: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:06:16] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:06:16] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:06:16] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2026-01-31 08:06:16] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:06:16] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:06:16] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:06:22] Ignore import error when loading sglang.srt.models.glm_ocr: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:06:22] Ignore import error when loading sglang.srt.models.glm_ocr_nextn: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:06:22] Ignore import error when loading sglang.srt.models.glmasr: cannot import name 'GlmAsrConfig' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.76s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.98s/it]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-1.5B-instruct \\\n",
    "    --host 0.0.0.0 --is-embedding --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:06:35.918563Z",
     "iopub.status.busy": "2026-01-31T08:06:35.918422Z",
     "iopub.status.idle": "2026-01-31T08:06:35.941906Z",
     "shell.execute_reply": "2026-01-31T08:06:35.941329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Text embedding (first 10): [-0.00023102760314941406, -0.04986572265625, -0.0032711029052734375, 0.011077880859375, -0.0140533447265625, 0.0159912109375, -0.01441192626953125, 0.0059051513671875, -0.0228424072265625, 0.0272979736328125]</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# successful encode for embedding model\n",
    "\n",
    "url = f\"http://localhost:{port}/encode\"\n",
    "data = {\"model\": \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", \"text\": \"Once upon a time\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "print_highlight(f\"Text embedding (first 10): {response_json['embedding'][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:06:35.943183Z",
     "iopub.status.busy": "2026-01-31T08:06:35.943059Z",
     "iopub.status.idle": "2026-01-31T08:06:35.952263Z",
     "shell.execute_reply": "2026-01-31T08:06:35.951738Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(embedding_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1/rerank (cross encoder rerank model)\n",
    "Rerank a list of documents given a query using a cross-encoder model. Note that this API is only available for cross encoder model like [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) with `attention-backend` `triton` and `torch_native`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:06:35.953716Z",
     "iopub.status.busy": "2026-01-31T08:06:35.953598Z",
     "iopub.status.idle": "2026-01-31T08:07:05.024313Z",
     "shell.execute_reply": "2026-01-31T08:07:05.023619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:06:40] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:06:40] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:06:40] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:06:43] INFO model_config.py:1106: Downcasting torch.float32 to torch.float16.\n",
      "[2026-01-31 08:06:43] INFO server_args.py:2700: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:06:45] No HuggingFace chat template found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:06:50] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:06:50] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:06:50] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2026-01-31 08:06:50] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:06:50] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:06:50] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:06:56] Ignore import error when loading sglang.srt.models.glm_ocr: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:06:56] Ignore import error when loading sglang.srt.models.glm_ocr_nextn: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:06:56] Ignore import error when loading sglang.srt.models.glmasr: cannot import name 'GlmAsrConfig' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.62s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.62s/it]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reranker_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path BAAI/bge-reranker-v2-m3 \\\n",
    "    --host 0.0.0.0 --disable-radix-cache --chunked-prefill-size -1 --attention-backend triton --is-embedding --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:07:05.025954Z",
     "iopub.status.busy": "2026-01-31T08:07:05.025822Z",
     "iopub.status.idle": "2026-01-31T08:07:05.096032Z",
     "shell.execute_reply": "2026-01-31T08:07:05.095445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Score: 5.26 - Document: 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Score: -8.19 - Document: 'hi'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute rerank scores for query and documents\n",
    "\n",
    "url = f\"http://localhost:{port}/v1/rerank\"\n",
    "data = {\n",
    "    \"model\": \"BAAI/bge-reranker-v2-m3\",\n",
    "    \"query\": \"what is panda?\",\n",
    "    \"documents\": [\n",
    "        \"hi\",\n",
    "        \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "for item in response_json:\n",
    "    print_highlight(f\"Score: {item['score']:.2f} - Document: '{item['document']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:07:05.097489Z",
     "iopub.status.busy": "2026-01-31T08:07:05.097268Z",
     "iopub.status.idle": "2026-01-31T08:07:05.107028Z",
     "shell.execute_reply": "2026-01-31T08:07:05.106393Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(reranker_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1/score (decoder-only scoring)\n",
    "\n",
    "Compute token probabilities for specified tokens given a query and items. This is useful for classification tasks, scoring responses, or computing log-probabilities.\n",
    "\n",
    "Parameters:\n",
    "- `query`: Query text\n",
    "- `items`: Item text(s) to score\n",
    "- `label_token_ids`: Token IDs to compute probabilities for\n",
    "- `apply_softmax`: Whether to apply softmax to get normalized probabilities (default: False)\n",
    "- `item_first`: Whether items come first in concatenation order (default: False)\n",
    "- `model`: Model name\n",
    "\n",
    "The response contains `scores` - a list of probability lists, one per item, each in the order of `label_token_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:07:05.108520Z",
     "iopub.status.busy": "2026-01-31T08:07:05.108402Z",
     "iopub.status.idle": "2026-01-31T08:07:33.176192Z",
     "shell.execute_reply": "2026-01-31T08:07:33.175506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:07:10] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:07:10] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:07:10] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:07:12] INFO server_args.py:1774: Attention backend not specified. Use fa3 backend by default.\n",
      "[2026-01-31 08:07:12] INFO server_args.py:2700: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:07:19] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:07:19] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:07:19] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2026-01-31 08:07:19] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:07:19] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:07:19] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:07:24] Ignore import error when loading sglang.srt.models.glm_ocr: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:07:24] Ignore import error when loading sglang.srt.models.glm_ocr_nextn: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:07:24] Ignore import error when loading sglang.srt.models.glmasr: cannot import name 'GlmAsrConfig' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.76it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.75it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=77.02 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=77.02 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.18it/s]\r",
      "Capturing batches (bs=2 avail_mem=76.96 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.18it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.96 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.18it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.96 GB): 100%|██████████| 3/3 [00:00<00:00,  5.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct \\\n",
    "    --host 0.0.0.0 --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:07:33.177733Z",
     "iopub.status.busy": "2026-01-31T08:07:33.177607Z",
     "iopub.status.idle": "2026-01-31T08:07:33.247633Z",
     "shell.execute_reply": "2026-01-31T08:07:33.247000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Item 'Paris': probabilities = ['0.0237', '0.9763']</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Item 'London': probabilities = ['0.0284', '0.9716']</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Item 'Berlin': probabilities = ['0.0637', '0.9363']</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Score the probability of different completions given a query\n",
    "query = \"The capital of France is\"\n",
    "items = [\"Paris\", \"London\", \"Berlin\"]\n",
    "\n",
    "url = f\"http://localhost:{port}/v1/score\"\n",
    "data = {\n",
    "    \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n",
    "    \"query\": query,\n",
    "    \"items\": items,\n",
    "    \"label_token_ids\": [9454, 2753],  # e.g. \"Yes\" and \"No\" token ids\n",
    "    \"apply_softmax\": True,  # Normalize probabilities to sum to 1\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "\n",
    "# Display scores for each item\n",
    "for item, scores in zip(items, response_json[\"scores\"]):\n",
    "    print_highlight(f\"Item '{item}': probabilities = {[f'{s:.4f}' for s in scores]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:07:33.249020Z",
     "iopub.status.busy": "2026-01-31T08:07:33.248900Z",
     "iopub.status.idle": "2026-01-31T08:07:33.277313Z",
     "shell.execute_reply": "2026-01-31T08:07:33.276609Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(score_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify (reward model)\n",
    "\n",
    "SGLang Runtime also supports reward models. Here we use a reward model to classify the quality of pairwise generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:07:33.278896Z",
     "iopub.status.busy": "2026-01-31T08:07:33.278770Z",
     "iopub.status.idle": "2026-01-31T08:08:10.372550Z",
     "shell.execute_reply": "2026-01-31T08:08:10.371783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:07:38] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:07:38] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:07:38] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:07:40] INFO server_args.py:1774: Attention backend not specified. Use fa3 backend by default.\n",
      "[2026-01-31 08:07:40] INFO server_args.py:2700: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:07:46] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:07:46] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:07:46] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2026-01-31 08:07:46] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:07:46] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:07:46] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:07:52] Ignore import error when loading sglang.srt.models.glm_ocr: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:07:52] Ignore import error when loading sglang.srt.models.glm_ocr_nextn: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:07:52] Ignore import error when loading sglang.srt.models.glmasr: cannot import name 'GlmAsrConfig' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:10<00:03,  3.56s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.23s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.71s/it]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that SGLang now treats embedding models and reward models as the same type of models.\n",
    "# This will be updated in the future.\n",
    "\n",
    "reward_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path Skywork/Skywork-Reward-Llama-3.1-8B-v0.2 --host 0.0.0.0 --is-embedding --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:08:10.374209Z",
     "iopub.status.busy": "2026-01-31T08:08:10.374081Z",
     "iopub.status.idle": "2026-01-31T08:08:10.955304Z",
     "shell.execute_reply": "2026-01-31T08:08:10.954375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reward: -24.25</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reward: 1.0546875</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "PROMPT = (\n",
    "    \"What is the range of the numeric output of a sigmoid node in a neural network?\"\n",
    ")\n",
    "\n",
    "RESPONSE1 = \"The output of a sigmoid node is bounded between -1 and 1.\"\n",
    "RESPONSE2 = \"The output of a sigmoid node is bounded between 0 and 1.\"\n",
    "\n",
    "CONVS = [\n",
    "    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", \"content\": RESPONSE1}],\n",
    "    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", \"content\": RESPONSE2}],\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\")\n",
    "prompts = tokenizer.apply_chat_template(CONVS, tokenize=False, return_dict=False)\n",
    "\n",
    "url = f\"http://localhost:{port}/classify\"\n",
    "data = {\"model\": \"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\", \"text\": prompts}\n",
    "\n",
    "responses = requests.post(url, json=data).json()\n",
    "for response in responses:\n",
    "    print_highlight(f\"reward: {response['embedding'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:08:10.957087Z",
     "iopub.status.busy": "2026-01-31T08:08:10.956952Z",
     "iopub.status.idle": "2026-01-31T08:08:10.966289Z",
     "shell.execute_reply": "2026-01-31T08:08:10.965442Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(reward_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture expert selection distribution in MoE models\n",
    "\n",
    "SGLang Runtime supports recording the number of times an expert is selected in a MoE model run for each expert in the model. This is useful when analyzing the throughput of the model and plan for optimization.\n",
    "\n",
    "*Note: We only print out the first 10 lines of the csv below for better readability. Please adjust accordingly if you want to analyze the results more deeply.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:08:10.967866Z",
     "iopub.status.busy": "2026-01-31T08:08:10.967729Z",
     "iopub.status.idle": "2026-01-31T08:08:59.076987Z",
     "shell.execute_reply": "2026-01-31T08:08:59.076206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:08:16] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:08:16] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:08:16] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:08:18] INFO server_args.py:1774: Attention backend not specified. Use fa3 backend by default.\n",
      "[2026-01-31 08:08:18] INFO server_args.py:2700: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:08:25] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:08:25] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:08:25] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2026-01-31 08:08:25] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:08:25] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:08:25] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:08:31] Ignore import error when loading sglang.srt.models.glm_ocr: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:08:31] Ignore import error when loading sglang.srt.models.glm_ocr_nextn: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:08:31] Ignore import error when loading sglang.srt.models.glmasr: cannot import name 'GlmAsrConfig' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:02<00:18,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:05<00:15,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:07<00:13,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:10<00:10,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:12<00:07,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:13<00:04,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:15<00:01,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:15<00:00,  1.41s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:15<00:00,  1.99s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=47.76 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:08:48] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /public_sglang_ci/runner-l1-hvkcm-gpu-1/_work/sglang/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=60,N=1408,device_name=NVIDIA_H100_80GB_HBM3.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2026-01-31 08:08:48] Using MoE kernel config with down_moe=False. Performance might be sub-optimal! Config file not found at /public_sglang_ci/runner-l1-hvkcm-gpu-1/_work/sglang/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=60,N=1408,device_name=NVIDIA_H100_80GB_HBM3_down.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=47.76 GB):  33%|███▎      | 1/3 [00:01<00:03,  1.87s/it]\r",
      "Capturing batches (bs=2 avail_mem=46.99 GB):  33%|███▎      | 1/3 [00:01<00:03,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=2 avail_mem=46.99 GB):  67%|██████▋   | 2/3 [00:02<00:01,  1.35s/it]\r",
      "Capturing batches (bs=1 avail_mem=46.98 GB):  67%|██████▋   | 2/3 [00:03<00:01,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=1 avail_mem=46.98 GB): 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\r",
      "Capturing batches (bs=1 avail_mem=46.98 GB): 100%|██████████| 3/3 [00:03<00:00,  1.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expert_record_server_process, port = launch_server_cmd(\n",
    "    \"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0 --expert-distribution-recorder-mode stat --log-level warning\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:08:59.078802Z",
     "iopub.status.busy": "2026-01-31T08:08:59.078545Z",
     "iopub.status.idle": "2026-01-31T08:09:00.573694Z",
     "shell.execute_reply": "2026-01-31T08:09:00.572902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><Response [200]></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': \" - Saigon - Australia - Ottawa - Tokyo - junk gene - Paris. The best answer is\\nA: To determine the answer, let's go step by step through the options:\\n\\n1. Saigon: This is the former name of Ho Chi Minh City, which is in Vietnam, not France.\\n2. Australia: Although Australia has a country capital city called Canberra, the capital of France is not mentioned as being in Australia.\\n3. Ottawa: This is the capital city of Canada, not the capital of France.\\n4. Tokyo: Japan has a capital city called Tokyo. However, the capital of France is not Tokyo.\\n5\", 'output_ids': [481, 15854, 73881, 481, 8330, 481, 32166, 481, 26194, 481, 29674, 14862, 481, 12095, 13, 576, 1850, 4226, 374, 198, 32, 25, 2014, 8253, 279, 4226, 11, 1077, 594, 728, 3019, 553, 3019, 1526, 279, 2606, 1447, 16, 13, 15854, 73881, 25, 1096, 374, 279, 4741, 829, 315, 17275, 33282, 92155, 4311, 11, 892, 374, 304, 22500, 11, 537, 9625, 624, 17, 13, 8330, 25, 10328, 8330, 702, 264, 3146, 6722, 3283, 2598, 68790, 11, 279, 6722, 315, 9625, 374, 537, 9733, 438, 1660, 304, 8330, 624, 18, 13, 32166, 25, 1096, 374, 279, 6722, 3283, 315, 6864, 11, 537, 279, 6722, 315, 9625, 624, 19, 13, 26194, 25, 6323, 702, 264, 6722, 3283, 2598, 26194, 13, 4354, 11, 279, 6722, 315, 9625, 374, 537, 26194, 624, 20], 'meta_info': {'id': '956776b3eac3499fabf2d55e939b7081', 'finish_reason': {'type': 'length', 'length': 128}, 'prompt_tokens': 7, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 128, 'cached_tokens': 0, 'e2e_latency': 0.5703425407409668, 'response_sent_to_client_ts': 1769846939.6563532}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><Response [200]></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><Response [200]></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = requests.post(f\"http://localhost:{port}/start_expert_distribution_record\")\n",
    "print_highlight(response)\n",
    "\n",
    "url = f\"http://localhost:{port}/generate\"\n",
    "data = {\"text\": \"What is the capital of France?\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.json())\n",
    "\n",
    "response = requests.post(f\"http://localhost:{port}/stop_expert_distribution_record\")\n",
    "print_highlight(response)\n",
    "\n",
    "response = requests.post(f\"http://localhost:{port}/dump_expert_distribution_record\")\n",
    "print_highlight(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:09:00.575447Z",
     "iopub.status.busy": "2026-01-31T08:09:00.575320Z",
     "iopub.status.idle": "2026-01-31T08:09:00.591904Z",
     "shell.execute_reply": "2026-01-31T08:09:00.591277Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(expert_record_server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize/Detokenize Example (Round Trip)\n",
    "\n",
    "This example demonstrates how to use the /tokenize and /detokenize endpoints together. We first tokenize a string, then detokenize the resulting IDs to reconstruct the original text. This workflow is useful when you need to handle tokenization externally but still leverage the server for detokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:09:00.593699Z",
     "iopub.status.busy": "2026-01-31T08:09:00.593579Z",
     "iopub.status.idle": "2026-01-31T08:09:27.660017Z",
     "shell.execute_reply": "2026-01-31T08:09:27.659207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:05] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:09:05] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:09:05] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:07] INFO server_args.py:1774: Attention backend not specified. Use fa3 backend by default.\n",
      "[2026-01-31 08:09:07] INFO server_args.py:2700: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "[2026-01-31 08:09:08] server_args=ServerArgs(model_path='qwen/qwen2.5-0.5b-instruct', tokenizer_path='qwen/qwen2.5-0.5b-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=33008, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.841, max_running_requests=128, max_queued_requests=None, max_total_tokens=20480, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', enable_prefill_delayer=False, prefill_delayer_max_delay_passes=30, prefill_delayer_token_usage_low_watermark=None, prefill_delayer_forward_passes_buckets=None, prefill_delayer_wait_seconds_buckets=None, device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=41910404, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=300, dist_timeout=None, download_dir=None, model_checksum=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, log_requests_format='text', log_requests_target=None, uvicorn_access_log_exclude_prefixes=[], crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, admin_api_key=None, served_model_name='qwen/qwen2.5-0.5b-instruct', weight_version='default', chat_template=None, hf_chat_template_name=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, enable_lora_overlap_loading=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='fa3', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', fp4_gemm_runner_backend='auto', nsa_prefill_backend=None, nsa_decode_backend=None, disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, enable_multi_layer_eagle=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', disable_hicache_numa_detect=False, hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, hierarchical_sparse_attention_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=8192, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 576, 640, 704, 768, 832, 896, 960, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816, 3072, 3328, 3584, 3840, 4096, 4608, 5120, 5632, 6144, 6656, 7168, 7680, 8192], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, nsa_prefill_cp_mode='in-seq-split', enable_fused_qk_norm_rope=False, enable_precise_embedding_interpolation=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, disaggregation_decode_enable_fake_auto=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, limit_mm_data_per_request=None, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:08] Watchdog TokenizerManager initialized.\n",
      "[2026-01-31 08:09:08] Using default HuggingFace chat template with detected content format: string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:13] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:09:13] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:09:13] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2026-01-31 08:09:13] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-01-31 08:09:13] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-01-31 08:09:13] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:16] Watchdog DetokenizerManager initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:16] Init torch distributed begin.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[2026-01-31 08:09:17] Init torch distributed ends. elapsed=0.31 s, mem usage=0.09 GB\n",
      "[2026-01-31 08:09:17] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:19] Ignore import error when loading sglang.srt.models.glm_ocr: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:09:19] Ignore import error when loading sglang.srt.models.glm_ocr_nextn: No module named 'transformers.models.glm_ocr'\n",
      "[2026-01-31 08:09:19] Ignore import error when loading sglang.srt.models.glmasr: cannot import name 'GlmAsrConfig' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)\n",
      "[2026-01-31 08:09:19] Load weight begin. avail mem=76.29 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:19] Found local HF snapshot for qwen/qwen2.5-0.5b-instruct at /hf_home/hub/models--qwen--qwen2.5-0.5b-instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775; skipping download.\n",
      "[2026-01-31 08:09:19] No model.safetensors.index.json found in remote.\n",
      "[2026-01-31 08:09:19] Beginning to load weights\n",
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.54it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.53it/s]\n",
      "\n",
      "[2026-01-31 08:09:20] Loading weights took 0.21 seconds\n",
      "[2026-01-31 08:09:20] Load weight end. elapsed=0.38 s, type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=75.31 GB, mem usage=0.98 GB.\n",
      "[2026-01-31 08:09:20] Using KV cache dtype: torch.bfloat16\n",
      "[2026-01-31 08:09:20] KV Cache is allocated. #tokens: 20480, K size: 0.12 GB, V size: 0.12 GB\n",
      "[2026-01-31 08:09:20] Memory pool end. avail mem=74.98 GB\n",
      "[2026-01-31 08:09:20] Init attention backend begin.\n",
      "[2026-01-31 08:09:20] Init attention backend end. elapsed=0.03 s\n",
      "[2026-01-31 08:09:20] Capture cuda graph begin. This can take up to several minutes. avail mem=74.89 GB\n",
      "[2026-01-31 08:09:20] Capture cuda graph bs [1, 2, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=74.89 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=74.89 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.22it/s]\r",
      "Capturing batches (bs=2 avail_mem=74.83 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.22it/s]\r",
      "Capturing batches (bs=1 avail_mem=74.82 GB):  33%|███▎      | 1/3 [00:00<00:00,  2.22it/s]\r",
      "Capturing batches (bs=1 avail_mem=74.82 GB): 100%|██████████| 3/3 [00:00<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:21] Capture cuda graph end. Time elapsed: 0.99 s. mem usage=0.07 GB. avail mem=74.82 GB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:21] max_total_num_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=128, context_len=32768, available_gpu_mem=74.82 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:21] INFO:     Started server process [2145521]\n",
      "[2026-01-31 08:09:21] INFO:     Waiting for application startup.\n",
      "[2026-01-31 08:09:21] Using default chat sampling params from model generation config: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
      "[2026-01-31 08:09:21] Using default chat sampling params from model generation config: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
      "[2026-01-31 08:09:21] INFO:     Application startup complete.\n",
      "[2026-01-31 08:09:21] INFO:     Uvicorn running on http://127.0.0.1:33008 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:22] INFO:     127.0.0.1:36928 - \"GET /v1/models HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:22] INFO:     127.0.0.1:36932 - \"GET /model_info HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:23] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, input throughput (token/s): 0.00, cuda graph: False\n",
      "[2026-01-31 08:09:23] INFO:     127.0.0.1:36940 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2026-01-31 08:09:23] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_free_server_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:09:27.661713Z",
     "iopub.status.busy": "2026-01-31T08:09:27.661580Z",
     "iopub.status.idle": "2026-01-31T08:09:27.677390Z",
     "shell.execute_reply": "2026-01-31T08:09:27.676811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Original Input Text:<br>'SGLang provides efficient tokenization endpoints.'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:27] INFO:     127.0.0.1:36952 - \"POST /tokenize HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br>Tokenized Output (IDs):<br>[50, 3825, 524, 5707, 11050, 3950, 2022, 36342, 13]</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Token Count: 9</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Max Model Length: 131072</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-31 08:09:27] INFO:     127.0.0.1:36966 - \"POST /detokenize HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br>Detokenized Output (Text):<br>'SGLang provides efficient tokenization endpoints.'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br>Round Trip Successful: Original and reconstructed text match.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from sglang.utils import print_highlight\n",
    "\n",
    "base_url = f\"http://localhost:{port}\"\n",
    "tokenize_url = f\"{base_url}/tokenize\"\n",
    "detokenize_url = f\"{base_url}/detokenize\"\n",
    "\n",
    "model_name = \"qwen/qwen2.5-0.5b-instruct\"\n",
    "input_text = \"SGLang provides efficient tokenization endpoints.\"\n",
    "print_highlight(f\"Original Input Text:\\n'{input_text}'\")\n",
    "\n",
    "# --- tokenize the input text ---\n",
    "tokenize_payload = {\n",
    "    \"model\": model_name,\n",
    "    \"prompt\": input_text,\n",
    "    \"add_special_tokens\": False,\n",
    "}\n",
    "try:\n",
    "    tokenize_response = requests.post(tokenize_url, json=tokenize_payload)\n",
    "    tokenize_response.raise_for_status()\n",
    "    tokenization_result = tokenize_response.json()\n",
    "    token_ids = tokenization_result.get(\"tokens\")\n",
    "\n",
    "    if not token_ids:\n",
    "        raise ValueError(\"Tokenization returned empty tokens.\")\n",
    "\n",
    "    print_highlight(f\"\\nTokenized Output (IDs):\\n{token_ids}\")\n",
    "    print_highlight(f\"Token Count: {tokenization_result.get('count')}\")\n",
    "    print_highlight(f\"Max Model Length: {tokenization_result.get('max_model_len')}\")\n",
    "\n",
    "    # --- detokenize the obtained token IDs ---\n",
    "    detokenize_payload = {\n",
    "        \"model\": model_name,\n",
    "        \"tokens\": token_ids,\n",
    "        \"skip_special_tokens\": True,\n",
    "    }\n",
    "\n",
    "    detokenize_response = requests.post(detokenize_url, json=detokenize_payload)\n",
    "    detokenize_response.raise_for_status()\n",
    "    detokenization_result = detokenize_response.json()\n",
    "    reconstructed_text = detokenization_result.get(\"text\")\n",
    "\n",
    "    print_highlight(f\"\\nDetokenized Output (Text):\\n'{reconstructed_text}'\")\n",
    "\n",
    "    if input_text == reconstructed_text:\n",
    "        print_highlight(\n",
    "            \"\\nRound Trip Successful: Original and reconstructed text match.\"\n",
    "        )\n",
    "    else:\n",
    "        print_highlight(\n",
    "            \"\\nRound Trip Mismatch: Original and reconstructed text differ.\"\n",
    "        )\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print_highlight(f\"\\nHTTP Request Error: {e}\")\n",
    "except Exception as e:\n",
    "    print_highlight(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:09:27.678791Z",
     "iopub.status.busy": "2026-01-31T08:09:27.678679Z",
     "iopub.status.idle": "2026-01-31T08:09:27.694050Z",
     "shell.execute_reply": "2026-01-31T08:09:27.691701Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(tokenizer_free_server_process)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
