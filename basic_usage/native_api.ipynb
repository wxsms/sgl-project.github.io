{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGLang Native APIs\n",
    "\n",
    "Apart from the OpenAI compatible APIs, the SGLang Runtime also provides its native server APIs. We introduce the following APIs:\n",
    "\n",
    "- `/generate` (text generation model)\n",
    "- `/get_model_info`\n",
    "- `/get_server_info`\n",
    "- `/health`\n",
    "- `/health_generate`\n",
    "- `/flush_cache`\n",
    "- `/update_weights`\n",
    "- `/encode`(embedding model)\n",
    "- `/v1/rerank`(cross encoder rerank model)\n",
    "- `/classify`(reward model)\n",
    "- `/start_expert_distribution_record`\n",
    "- `/stop_expert_distribution_record`\n",
    "- `/dump_expert_distribution_record`\n",
    "- `/tokenize`\n",
    "- `/detokenize`\n",
    "- A full list of these APIs can be found at [http_server.py](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server.py)\n",
    "\n",
    "We mainly use `requests` to test these APIs in the following examples. You can also use `curl`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch A Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:54:09.324042Z",
     "iopub.status.busy": "2025-12-12T10:54:09.323915Z",
     "iopub.status.idle": "2025-12-12T10:54:45.770849Z",
     "shell.execute_reply": "2025-12-12T10:54:45.770146Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:54:14] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:54:14] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:54:14] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:54:19] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:54:19] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:54:19] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:54:22] WARNING server_args.py:1416: Attention backend not explicitly specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:54:29] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:54:29] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:54:29] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 10:54:29] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:54:29] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:54:29] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 10:54:33.343853 2285392 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 10:54:33.343870 2285392 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.186.137.102 port: 12001\n",
      "I1212 10:54:33.343890 2285392 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.186.137.102:15378\n",
      "I1212 10:54:33.343966 2285392 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 10:54:33.346877 2285392 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 10:54:33.373776 2285392 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 10:54:33.374456 2285392 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:b9:05\n",
      "I1212 10:54:33.401358 2285392 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 10:54:33.402014 2285392 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:39:05\n",
      "I1212 10:54:33.486506 2285392 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 10:54:33.487167 2285392 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:b9:05\n",
      "I1212 10:54:33.513321 2285392 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 10:54:33.513953 2285392 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:39:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 10:54:33.541318 2285392 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 10:54:33.541950 2285392 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:b9:05\n",
      "I1212 10:54:33.569356 2285392 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 10:54:33.569994 2285392 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:39:05\n",
      "I1212 10:54:33.597795 2285392 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 10:54:33.598750 2285392 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:ba:89:66\n",
      "I1212 10:54:33.625371 2285392 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 10:54:33.625999 2285392 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:b9:05\n",
      "I1212 10:54:33.653374 2285392 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 10:54:33.654007 2285392 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:39:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 10:54:34.226207 2285392 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7ef2bbfff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:54:36] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=59.57 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=59.57 GB):  33%|███▎      | 1/3 [00:00<00:00,  3.85it/s]\r",
      "Capturing batches (bs=2 avail_mem=59.51 GB):  33%|███▎      | 1/3 [00:00<00:00,  3.85it/s]\r",
      "Capturing batches (bs=1 avail_mem=59.50 GB):  33%|███▎      | 1/3 [00:00<00:00,  3.85it/s]\r",
      "Capturing batches (bs=1 avail_mem=59.50 GB): 100%|██████████| 3/3 [00:00<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:54:40] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang.test.doc_patch import launch_server_cmd\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --host 0.0.0.0 --log-level warning\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate (text generation model)\n",
    "Generate completions. This is similar to the `/v1/completions` in OpenAI API. Detailed parameters can be found in the [sampling parameters](sampling_params.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:54:45.781303Z",
     "iopub.status.busy": "2025-12-12T10:54:45.780743Z",
     "iopub.status.idle": "2025-12-12T10:54:46.319288Z",
     "shell.execute_reply": "2025-12-12T10:54:46.318675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': ' The capital of France, which is known as Paris, is Paris. Paris is the capital of France, located on the river Marne in the south banks of the French Riviera. It is the largest city in France with a population of about 2 million people. The weather in Paris can change from sunny to rainy and wet, depending on the season and other factors. The city is a major traffic hub and also has a well-known fashion, business, and film scene. The French Parliament and other institutions are also located in Paris. \\n\\nParis was founded in 720AD by the Breton chieftain Eder', 'output_ids': [576, 6722, 315, 9625, 11, 892, 374, 3881, 438, 12095, 11, 374, 12095, 13, 12095, 374, 279, 6722, 315, 9625, 11, 7407, 389, 279, 14796, 2876, 811, 304, 279, 9806, 13959, 315, 279, 8585, 50668, 25834, 13, 1084, 374, 279, 7772, 3283, 304, 9625, 448, 264, 7042, 315, 911, 220, 17, 3526, 1251, 13, 576, 9104, 304, 12095, 646, 2297, 504, 39698, 311, 62757, 323, 14401, 11, 11649, 389, 279, 3200, 323, 1008, 9363, 13, 576, 3283, 374, 264, 3598, 9442, 18719, 323, 1083, 702, 264, 1632, 21309, 11153, 11, 2562, 11, 323, 4531, 6109, 13, 576, 8585, 19723, 323, 1008, 14336, 525, 1083, 7407, 304, 12095, 13, 4710, 59604, 572, 18047, 304, 220, 22, 17, 15, 1808, 553, 279, 11427, 777, 521, 645, 723, 466, 468, 1107], 'meta_info': {'id': '489002bdfc9844ebae93362ce7206646', 'finish_reason': {'type': 'length', 'length': 128}, 'prompt_tokens': 7, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 128, 'cached_tokens': 0, 'e2e_latency': 0.5311481952667236, 'response_sent_to_client_ts': 1765536886.3154228}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = f\"http://localhost:{port}/generate\"\n",
    "data = {\"text\": \"What is the capital of France?\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Info\n",
    "\n",
    "Get the information of the model.\n",
    "\n",
    "- `model_path`: The path/name of the model.\n",
    "- `is_generation`: Whether the model is used as generation model or embedding model.\n",
    "- `tokenizer_path`: The path/name of the tokenizer.\n",
    "- `preferred_sampling_params`: The default sampling params specified via `--preferred-sampling-params`. `None` is returned in this example as we did not explicitly configure it in server args.\n",
    "- `weight_version`: This field contains the version of the model weights. This is often used to track changes or updates to the model’s trained parameters.\n",
    "- `has_image_understanding`: Whether the model has image-understanding capability.\n",
    "- `has_audio_understanding`: Whether the model has audio-understanding capability.\n",
    "- `model_type`: The model type from the HuggingFace config (e.g., \"qwen2\", \"llama\").\n",
    "- `architectures`: The model architectures from the HuggingFace config (e.g., [\"Qwen2ForCausalLM\"])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:54:46.320938Z",
     "iopub.status.busy": "2025-12-12T10:54:46.320787Z",
     "iopub.status.idle": "2025-12-12T10:54:46.327102Z",
     "shell.execute_reply": "2025-12-12T10:54:46.326577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:54:46] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'model_path': 'qwen/qwen2.5-0.5b-instruct', 'tokenizer_path': 'qwen/qwen2.5-0.5b-instruct', 'is_generation': True, 'preferred_sampling_params': None, 'weight_version': 'default', 'has_image_understanding': False, 'has_audio_understanding': False, 'model_type': 'qwen2', 'architectures': ['Qwen2ForCausalLM']}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/get_model_info\"\n",
    "\n",
    "response = requests.get(url)\n",
    "response_json = response.json()\n",
    "print_highlight(response_json)\n",
    "assert response_json[\"model_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\n",
    "assert response_json[\"is_generation\"] is True\n",
    "assert response_json[\"tokenizer_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\n",
    "assert response_json[\"preferred_sampling_params\"] is None\n",
    "assert response_json.keys() == {\n",
    "    \"model_path\",\n",
    "    \"is_generation\",\n",
    "    \"tokenizer_path\",\n",
    "    \"preferred_sampling_params\",\n",
    "    \"weight_version\",\n",
    "    \"has_image_understanding\",\n",
    "    \"has_audio_understanding\",\n",
    "    \"model_type\",\n",
    "    \"architectures\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Server Info\n",
    "Gets the server information including CLI arguments, token limits, and memory pool sizes.\n",
    "- Note: `get_server_info` merges the following deprecated endpoints:\n",
    "  - `get_server_args`\n",
    "  - `get_memory_pool_size` \n",
    "  - `get_max_total_num_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:54:46.328505Z",
     "iopub.status.busy": "2025-12-12T10:54:46.328356Z",
     "iopub.status.idle": "2025-12-12T10:54:46.337815Z",
     "shell.execute_reply": "2025-12-12T10:54:46.337303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:54:46] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"model_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_mode\":\"auto\",\"tokenizer_worker_num\":1,\"skip_tokenizer_init\":false,\"load_format\":\"auto\",\"model_loader_extra_config\":\"{}\",\"rl_quant_profile\":null,\"trust_remote_code\":false,\"context_length\":null,\"is_embedding\":false,\"enable_multimodal\":null,\"revision\":null,\"model_impl\":\"auto\",\"host\":\"0.0.0.0\",\"port\":32051,\"fastapi_root_path\":\"\",\"grpc_mode\":false,\"skip_server_warmup\":false,\"warmups\":null,\"nccl_port\":null,\"checkpoint_engine_wait_weights_before_ready\":false,\"dtype\":\"auto\",\"quantization\":null,\"quantization_param_path\":null,\"kv_cache_dtype\":\"auto\",\"enable_fp32_lm_head\":false,\"modelopt_quant\":null,\"modelopt_checkpoint_restore_path\":null,\"modelopt_checkpoint_save_path\":null,\"modelopt_export_path\":null,\"quantize_and_serve\":false,\"mem_fraction_static\":0.841,\"max_running_requests\":128,\"max_queued_requests\":null,\"max_total_tokens\":20480,\"chunked_prefill_size\":8192,\"enable_dynamic_chunking\":false,\"max_prefill_tokens\":16384,\"schedule_policy\":\"fcfs\",\"enable_priority_scheduling\":false,\"abort_on_priority_when_disabled\":false,\"schedule_low_priority_values_first\":false,\"priority_scheduling_preemption_threshold\":10,\"schedule_conservativeness\":1.0,\"page_size\":1,\"hybrid_kvcache_ratio\":null,\"swa_full_tokens_ratio\":0.8,\"disable_hybrid_swa_memory\":false,\"radix_eviction_policy\":\"lru\",\"device\":\"cuda\",\"tp_size\":1,\"pp_size\":1,\"pp_max_micro_batch_size\":null,\"pp_async_batch_depth\":0,\"stream_interval\":1,\"stream_output\":false,\"random_seed\":1053632942,\"constrained_json_whitespace_pattern\":null,\"constrained_json_disable_any_whitespace\":false,\"watchdog_timeout\":300,\"dist_timeout\":null,\"download_dir\":null,\"base_gpu_id\":0,\"gpu_id_step\":1,\"sleep_on_idle\":false,\"mm_process_config\":{},\"log_level\":\"warning\",\"log_level_http\":null,\"log_requests\":false,\"log_requests_level\":2,\"crash_dump_folder\":null,\"show_time_cost\":false,\"enable_metrics\":false,\"enable_metrics_for_all_schedulers\":false,\"tokenizer_metrics_custom_labels_header\":\"x-custom-labels\",\"tokenizer_metrics_allowed_custom_labels\":null,\"bucket_time_to_first_token\":null,\"bucket_inter_token_latency\":null,\"bucket_e2e_request_latency\":null,\"collect_tokens_histogram\":false,\"prompt_tokens_buckets\":null,\"generation_tokens_buckets\":null,\"gc_warning_threshold_secs\":0.0,\"decode_log_interval\":40,\"enable_request_time_stats_logging\":false,\"kv_events_config\":null,\"enable_trace\":false,\"otlp_traces_endpoint\":\"localhost:4317\",\"export_metrics_to_file\":false,\"export_metrics_to_file_dir\":null,\"api_key\":null,\"served_model_name\":\"qwen/qwen2.5-0.5b-instruct\",\"weight_version\":\"default\",\"chat_template\":null,\"completion_template\":null,\"file_storage_path\":\"sglang_storage\",\"enable_cache_report\":false,\"reasoning_parser\":null,\"tool_call_parser\":null,\"tool_server\":null,\"sampling_defaults\":\"model\",\"dp_size\":1,\"load_balance_method\":\"round_robin\",\"load_watch_interval\":0.1,\"prefill_round_robin_balance\":false,\"dist_init_addr\":null,\"nnodes\":1,\"node_rank\":0,\"json_model_override_args\":\"{}\",\"preferred_sampling_params\":null,\"enable_lora\":null,\"max_lora_rank\":null,\"lora_target_modules\":null,\"lora_paths\":null,\"max_loaded_loras\":null,\"max_loras_per_batch\":8,\"lora_eviction_policy\":\"lru\",\"lora_backend\":\"csgmv\",\"max_lora_chunk_size\":16,\"attention_backend\":\"fa3\",\"decode_attention_backend\":null,\"prefill_attention_backend\":null,\"sampling_backend\":\"flashinfer\",\"grammar_backend\":\"xgrammar\",\"mm_attention_backend\":null,\"fp8_gemm_runner_backend\":\"auto\",\"nsa_prefill_backend\":\"flashmla_sparse\",\"nsa_decode_backend\":\"fa3\",\"enable_flashinfer_autotune\":false,\"speculative_algorithm\":null,\"speculative_draft_model_path\":null,\"speculative_draft_model_revision\":null,\"speculative_draft_load_format\":null,\"speculative_num_steps\":null,\"speculative_eagle_topk\":null,\"speculative_num_draft_tokens\":null,\"speculative_accept_threshold_single\":1.0,\"speculative_accept_threshold_acc\":1.0,\"speculative_token_map\":null,\"speculative_attention_mode\":\"prefill\",\"speculative_moe_runner_backend\":null,\"speculative_moe_a2a_backend\":null,\"speculative_ngram_min_match_window_size\":1,\"speculative_ngram_max_match_window_size\":12,\"speculative_ngram_min_bfs_breadth\":1,\"speculative_ngram_max_bfs_breadth\":10,\"speculative_ngram_match_type\":\"BFS\",\"speculative_ngram_branch_length\":18,\"speculative_ngram_capacity\":10000000,\"ep_size\":1,\"moe_a2a_backend\":\"none\",\"moe_runner_backend\":\"auto\",\"flashinfer_mxfp4_moe_precision\":\"default\",\"enable_flashinfer_allreduce_fusion\":false,\"deepep_mode\":\"auto\",\"ep_num_redundant_experts\":0,\"ep_dispatch_algorithm\":null,\"init_expert_location\":\"trivial\",\"enable_eplb\":false,\"eplb_algorithm\":\"auto\",\"eplb_rebalance_num_iterations\":1000,\"eplb_rebalance_layers_per_chunk\":null,\"eplb_min_rebalancing_utilization_threshold\":1.0,\"expert_distribution_recorder_mode\":null,\"expert_distribution_recorder_buffer_size\":1000,\"enable_expert_distribution_metrics\":false,\"deepep_config\":null,\"moe_dense_tp_size\":null,\"elastic_ep_backend\":null,\"mooncake_ib_device\":null,\"max_mamba_cache_size\":null,\"mamba_ssm_dtype\":\"float32\",\"mamba_full_memory_ratio\":0.9,\"enable_hierarchical_cache\":false,\"hicache_ratio\":2.0,\"hicache_size\":0,\"hicache_write_policy\":\"write_through\",\"hicache_io_backend\":\"kernel\",\"hicache_mem_layout\":\"layer_first\",\"hicache_storage_backend\":null,\"hicache_storage_prefetch_policy\":\"best_effort\",\"hicache_storage_backend_extra_config\":null,\"enable_lmcache\":false,\"kt_weight_path\":null,\"kt_method\":\"AMXINT4\",\"kt_cpuinfer\":null,\"kt_threadpool_count\":2,\"kt_num_gpu_experts\":null,\"kt_max_deferred_experts_per_token\":null,\"dllm_algorithm\":null,\"dllm_algorithm_config\":null,\"enable_double_sparsity\":false,\"ds_channel_config_path\":null,\"ds_heavy_channel_num\":32,\"ds_heavy_token_num\":256,\"ds_heavy_channel_type\":\"qk\",\"ds_sparse_decode_threshold\":4096,\"cpu_offload_gb\":0,\"offload_group_size\":-1,\"offload_num_in_group\":1,\"offload_prefetch_step\":1,\"offload_mode\":\"cpu\",\"multi_item_scoring_delimiter\":null,\"disable_radix_cache\":false,\"cuda_graph_max_bs\":4,\"cuda_graph_bs\":[1,2,4],\"disable_cuda_graph\":false,\"disable_cuda_graph_padding\":false,\"enable_profile_cuda_graph\":false,\"enable_cudagraph_gc\":false,\"enable_layerwise_nvtx_marker\":false,\"enable_nccl_nvls\":false,\"enable_symm_mem\":false,\"disable_flashinfer_cutlass_moe_fp4_allgather\":false,\"enable_tokenizer_batch_encode\":false,\"disable_tokenizer_batch_decode\":false,\"disable_outlines_disk_cache\":false,\"disable_custom_all_reduce\":false,\"enable_mscclpp\":false,\"enable_torch_symm_mem\":false,\"disable_overlap_schedule\":false,\"enable_mixed_chunk\":false,\"enable_dp_attention\":false,\"enable_dp_lm_head\":false,\"enable_two_batch_overlap\":false,\"enable_single_batch_overlap\":false,\"tbo_token_distribution_threshold\":0.48,\"enable_torch_compile\":false,\"enable_piecewise_cuda_graph\":false,\"enable_torch_compile_debug_mode\":false,\"torch_compile_max_bs\":32,\"piecewise_cuda_graph_max_tokens\":4096,\"piecewise_cuda_graph_tokens\":[4,8,12,16,20,24,28,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256,288,320,352,384,416,448,480,512,640,768,896,1024,1152,1280,1408,1536,1664,1792,1920,2048,2176,2304,2432,2560,2688,2816,2944,3072,3200,3328,3456,3584,3712,3840,3968,4096],\"piecewise_cuda_graph_compiler\":\"eager\",\"torchao_config\":\"\",\"enable_nan_detection\":false,\"enable_p2p_check\":false,\"triton_attention_reduce_in_fp32\":false,\"triton_attention_num_kv_splits\":8,\"triton_attention_split_tile_size\":null,\"num_continuous_decode_steps\":1,\"delete_ckpt_after_loading\":false,\"enable_memory_saver\":false,\"enable_weights_cpu_backup\":false,\"enable_draft_weights_cpu_backup\":false,\"allow_auto_truncate\":false,\"enable_custom_logit_processor\":false,\"flashinfer_mla_disable_ragged\":false,\"disable_shared_experts_fusion\":false,\"disable_chunked_prefix_cache\":false,\"disable_fast_image_processor\":false,\"keep_mm_feature_on_device\":false,\"enable_return_hidden_states\":false,\"scheduler_recv_interval\":1,\"numa_node\":null,\"enable_deterministic_inference\":false,\"rl_on_policy_target\":null,\"enable_attn_tp_input_scattered\":false,\"enable_nsa_prefill_context_parallel\":false,\"enable_fused_qk_norm_rope\":false,\"enable_dynamic_batch_tokenizer\":false,\"dynamic_batch_tokenizer_batch_size\":32,\"dynamic_batch_tokenizer_batch_timeout\":0.002,\"debug_tensor_dump_output_folder\":null,\"debug_tensor_dump_layers\":null,\"debug_tensor_dump_input_file\":null,\"debug_tensor_dump_inject\":false,\"disaggregation_mode\":\"null\",\"disaggregation_transfer_backend\":\"mooncake\",\"disaggregation_bootstrap_port\":8998,\"disaggregation_decode_tp\":null,\"disaggregation_decode_dp\":null,\"disaggregation_prefill_pp\":1,\"disaggregation_ib_device\":null,\"disaggregation_decode_enable_offload_kvcache\":false,\"num_reserved_decode_tokens\":512,\"disaggregation_decode_polling_interval\":1,\"custom_weight_loader\":[],\"weight_loader_disable_mmap\":false,\"remote_instance_weight_loader_seed_instance_ip\":null,\"remote_instance_weight_loader_seed_instance_service_port\":null,\"remote_instance_weight_loader_send_weights_group_ports\":null,\"remote_instance_weight_loader_backend\":\"nccl\",\"remote_instance_weight_loader_support_transfer_engine\":true,\"enable_pdmux\":false,\"pdmux_config_path\":null,\"sm_group_num\":8,\"mm_max_concurrent_calls\":32,\"mm_per_request_timeout\":10.0,\"enable_broadcast_mm_inputs_process\":false,\"decrypted_config_file\":null,\"decrypted_draft_config_file\":null,\"mm_enable_dp_encoder\":false,\"forward_hooks\":null,\"status\":\"ready\",\"max_total_num_tokens\":20480,\"max_req_input_len\":20474,\"tp_rank\":0,\"remote_instance_transfer_engine_session_id\":\"10.186.137.102:15378\",\"remote_instance_transfer_engine_weights_info_dict\":{\"model.embed_tokens.weight\":[139579692679168,136134656,2],\"model.layers.0.self_attn.qkv_proj.weight\":[139579965308928,1032192,2],\"model.layers.0.self_attn.qkv_proj.bias\":[139579986280448,1152,2],\"model.layers.0.self_attn.o_proj.weight\":[139579967373312,802816,2],\"model.layers.0.mlp.gate_up_proj.weight\":[139592812462080,8716288,2],\"model.layers.0.mlp.down_proj.weight\":[139592997011456,4358144,2],\"model.layers.0.input_layernorm.weight\":[139579986283008,896,2],\"model.layers.0.post_attention_layernorm.weight\":[139579986285056,896,2],\"model.layers.1.self_attn.qkv_proj.weight\":[139593005727744,1032192,2],\"model.layers.1.self_attn.qkv_proj.bias\":[139579986287104,1152,2],\"model.layers.1.self_attn.o_proj.weight\":[139593007792128,802816,2],\"model.layers.1.mlp.gate_up_proj.weight\":[139592778907648,8716288,2],\"model.layers.1.mlp.down_proj.weight\":[139579968978944,4358144,2],\"model.layers.1.input_layernorm.weight\":[139579986289664,896,2],\"model.layers.1.post_attention_layernorm.weight\":[139579986291712,896,2],\"model.layers.2.self_attn.qkv_proj.weight\":[139579977695232,1032192,2],\"model.layers.2.self_attn.qkv_proj.bias\":[139579986293760,1152,2],\"model.layers.2.self_attn.o_proj.weight\":[139579979759616,802816,2],\"model.layers.2.mlp.gate_up_proj.weight\":[139592745353216,8716288,2],\"model.layers.2.mlp.down_proj.weight\":[139592711798784,4358144,2],\"model.layers.2.input_layernorm.weight\":[139579986296320,896,2],\"model.layers.2.post_attention_layernorm.weight\":[139579986298368,896,2],\"model.layers.3.self_attn.qkv_proj.weight\":[139579981365248,1032192,2],\"model.layers.3.self_attn.qkv_proj.bias\":[139579986300416,1152,2],\"model.layers.3.self_attn.o_proj.weight\":[139579983429632,802816,2],\"model.layers.3.mlp.gate_up_proj.weight\":[139592678244352,8716288,2],\"model.layers.3.mlp.down_proj.weight\":[139592720515072,4358144,2],\"model.layers.3.input_layernorm.weight\":[139579986302976,896,2],\"model.layers.3.post_attention_layernorm.weight\":[139579986305024,896,2],\"model.layers.4.self_attn.qkv_proj.weight\":[139592729231360,1032192,2],\"model.layers.4.self_attn.qkv_proj.bias\":[139579986307072,1152,2],\"model.layers.4.self_attn.o_proj.weight\":[139592644689920,802816,2],\"model.layers.4.mlp.gate_up_proj.weight\":[139592646295552,8716288,2],\"model.layers.4.mlp.down_proj.weight\":[139592611135488,4358144,2],\"model.layers.4.input_layernorm.weight\":[139579986309632,896,2],\"model.layers.4.post_attention_layernorm.weight\":[139579986311680,896,2],\"model.layers.5.self_attn.qkv_proj.weight\":[139592619851776,1032192,2],\"model.layers.5.self_attn.qkv_proj.bias\":[139579986313728,1152,2],\"model.layers.5.self_attn.o_proj.weight\":[139592663728128,802816,2],\"model.layers.5.mlp.gate_up_proj.weight\":[139592577581056,8716288,2],\"model.layers.5.mlp.down_proj.weight\":[139592621916160,4358144,2],\"model.layers.5.input_layernorm.weight\":[139579986316288,896,2],\"model.layers.5.post_attention_layernorm.weight\":[139579986318336,896,2],\"model.layers.6.self_attn.qkv_proj.weight\":[139592544026624,1032192,2],\"model.layers.6.self_attn.qkv_proj.bias\":[139579986320384,1152,2],\"model.layers.6.self_attn.o_proj.weight\":[139592546091008,802816,2],\"model.layers.6.mlp.gate_up_proj.weight\":[139592510472192,8716288,2],\"model.layers.6.mlp.down_proj.weight\":[139592547696640,4358144,2],\"model.layers.6.input_layernorm.weight\":[139579986322944,896,2],\"model.layers.6.post_attention_layernorm.weight\":[139579986324992,896,2],\"model.layers.7.self_attn.qkv_proj.weight\":[139592556412928,1032192,2],\"model.layers.7.self_attn.qkv_proj.bias\":[139579986327040,1152,2],\"model.layers.7.self_attn.o_proj.weight\":[139592558477312,802816,2],\"model.layers.7.mlp.gate_up_proj.weight\":[139591940046848,8716288,2],\"model.layers.7.mlp.down_proj.weight\":[139591738720256,4358144,2],\"model.layers.7.input_layernorm.weight\":[139579986329600,896,2],\"model.layers.7.post_attention_layernorm.weight\":[139579986331648,896,2],\"model.layers.8.self_attn.qkv_proj.weight\":[139592560082944,1032192,2],\"model.layers.8.self_attn.qkv_proj.bias\":[139579986333696,1152,2],\"model.layers.8.self_attn.o_proj.weight\":[139592562147328,802816,2],\"model.layers.8.mlp.gate_up_proj.weight\":[139588249059328,8716288,2],\"model.layers.8.mlp.down_proj.weight\":[139591747436544,4358144,2],\"model.layers.8.input_layernorm.weight\":[139579986336256,896,2],\"model.layers.8.post_attention_layernorm.weight\":[139579986338304,896,2],\"model.layers.9.self_attn.qkv_proj.weight\":[139591756152832,1032192,2],\"model.layers.9.self_attn.qkv_proj.bias\":[139579986340352,1152,2],\"model.layers.9.self_attn.o_proj.weight\":[139587108208640,802816,2],\"model.layers.9.mlp.gate_up_proj.weight\":[139587109814272,8716288,2],\"model.layers.9.mlp.down_proj.weight\":[139583249448960,4358144,2],\"model.layers.9.input_layernorm.weight\":[139579986342912,896,2],\"model.layers.9.post_attention_layernorm.weight\":[139579986344960,896,2],\"model.layers.10.self_attn.qkv_proj.weight\":[139583258165248,1032192,2],\"model.layers.10.self_attn.qkv_proj.bias\":[139579986347008,1152,2],\"model.layers.10.self_attn.o_proj.weight\":[139587127246848,802816,2],\"model.layers.10.mlp.gate_up_proj.weight\":[139579122253824,8716288,2],\"model.layers.10.mlp.down_proj.weight\":[139583260229632,4358144,2],\"model.layers.10.input_layernorm.weight\":[139579986349568,896,2],\"model.layers.10.post_attention_layernorm.weight\":[139579986351616,896,2],\"model.layers.11.self_attn.qkv_proj.weight\":[139579088699392,1032192,2],\"model.layers.11.self_attn.qkv_proj.bias\":[139579986353664,1152,2],\"model.layers.11.self_attn.o_proj.weight\":[139579090763776,802816,2],\"model.layers.11.mlp.gate_up_proj.weight\":[139579055144960,8716288,2],\"model.layers.11.mlp.down_proj.weight\":[139579092369408,4358144,2],\"model.layers.11.input_layernorm.weight\":[139579986356224,896,2],\"model.layers.11.post_attention_layernorm.weight\":[139579986358272,896,2],\"model.layers.12.self_attn.qkv_proj.weight\":[139579101085696,1032192,2],\"model.layers.12.self_attn.qkv_proj.bias\":[139579986360320,1152,2],\"model.layers.12.self_attn.o_proj.weight\":[139579103150080,802816,2],\"model.layers.12.mlp.gate_up_proj.weight\":[139579021590528,8716288,2],\"model.layers.12.mlp.down_proj.weight\":[139578988036096,4358144,2],\"model.layers.12.input_layernorm.weight\":[139579986362880,896,2],\"model.layers.12.post_attention_layernorm.weight\":[139579986364928,896,2],\"model.layers.13.self_attn.qkv_proj.weight\":[139579104755712,1032192,2],\"model.layers.13.self_attn.qkv_proj.bias\":[139579986366976,1152,2],\"model.layers.13.self_attn.o_proj.weight\":[139579106820096,802816,2],\"model.layers.13.mlp.gate_up_proj.weight\":[139578954481664,8716288,2],\"model.layers.13.mlp.down_proj.weight\":[139578996752384,4358144,2],\"model.layers.13.input_layernorm.weight\":[139579986369536,896,2],\"model.layers.13.post_attention_layernorm.weight\":[139579986371584,896,2],\"model.layers.14.self_attn.qkv_proj.weight\":[139579005468672,1032192,2],\"model.layers.14.self_attn.qkv_proj.bias\":[139579986373632,1152,2],\"model.layers.14.self_attn.o_proj.weight\":[139578920927232,802816,2],\"model.layers.14.mlp.gate_up_proj.weight\":[139578922532864,8716288,2],\"model.layers.14.mlp.down_proj.weight\":[139578887372800,4358144,2],\"model.layers.14.input_layernorm.weight\":[139579986376192,896,2],\"model.layers.14.post_attention_layernorm.weight\":[139579986378240,896,2],\"model.layers.15.self_attn.qkv_proj.weight\":[139578896089088,1032192,2],\"model.layers.15.self_attn.qkv_proj.bias\":[139579986380288,1152,2],\"model.layers.15.self_attn.o_proj.weight\":[139578939965440,802816,2],\"model.layers.15.mlp.gate_up_proj.weight\":[139578853818368,8716288,2],\"model.layers.15.mlp.down_proj.weight\":[139578898153472,4358144,2],\"model.layers.15.input_layernorm.weight\":[139579986382848,896,2],\"model.layers.15.post_attention_layernorm.weight\":[139579986384896,896,2],\"model.layers.16.self_attn.qkv_proj.weight\":[139578820263936,1032192,2],\"model.layers.16.self_attn.qkv_proj.bias\":[139579986386944,1152,2],\"model.layers.16.self_attn.o_proj.weight\":[139578822328320,802816,2],\"model.layers.16.mlp.gate_up_proj.weight\":[139578786709504,8716288,2],\"model.layers.16.mlp.down_proj.weight\":[139578823933952,4358144,2],\"model.layers.16.input_layernorm.weight\":[139579986389504,896,2],\"model.layers.16.post_attention_layernorm.weight\":[139579986391552,896,2],\"model.layers.17.self_attn.qkv_proj.weight\":[139578832650240,1032192,2],\"model.layers.17.self_attn.qkv_proj.bias\":[139579986393600,1152,2],\"model.layers.17.self_attn.o_proj.weight\":[139578834714624,802816,2],\"model.layers.17.mlp.gate_up_proj.weight\":[139578753155072,8716288,2],\"model.layers.17.mlp.down_proj.weight\":[139578719600640,4358144,2],\"model.layers.17.input_layernorm.weight\":[139579986396160,896,2],\"model.layers.17.post_attention_layernorm.weight\":[139579986398208,896,2],\"model.layers.18.self_attn.qkv_proj.weight\":[139578836320256,1032192,2],\"model.layers.18.self_attn.qkv_proj.bias\":[139579986400256,1152,2],\"model.layers.18.self_attn.o_proj.weight\":[139578838384640,802816,2],\"model.layers.18.mlp.gate_up_proj.weight\":[139578686046208,8716288,2],\"model.layers.18.mlp.down_proj.weight\":[139578728316928,4358144,2],\"model.layers.18.input_layernorm.weight\":[139579986402816,896,2],\"model.layers.18.post_attention_layernorm.weight\":[139579986404864,896,2],\"model.layers.19.self_attn.qkv_proj.weight\":[139578737033216,1032192,2],\"model.layers.19.self_attn.qkv_proj.bias\":[139579986406912,1152,2],\"model.layers.19.self_attn.o_proj.weight\":[139578652491776,802816,2],\"model.layers.19.mlp.gate_up_proj.weight\":[139578654097408,8716288,2],\"model.layers.19.mlp.down_proj.weight\":[139578618937344,4358144,2],\"model.layers.19.input_layernorm.weight\":[139579986409472,896,2],\"model.layers.19.post_attention_layernorm.weight\":[139579986411520,896,2],\"model.layers.20.self_attn.qkv_proj.weight\":[139578627653632,1032192,2],\"model.layers.20.self_attn.qkv_proj.bias\":[139579986413568,1152,2],\"model.layers.20.self_attn.o_proj.weight\":[139578671529984,802816,2],\"model.layers.20.mlp.gate_up_proj.weight\":[139578585382912,8716288,2],\"model.layers.20.mlp.down_proj.weight\":[139578629718016,4358144,2],\"model.layers.20.input_layernorm.weight\":[139579986416128,896,2],\"model.layers.20.post_attention_layernorm.weight\":[139579986418176,896,2],\"model.layers.21.self_attn.qkv_proj.weight\":[139578551828480,1032192,2],\"model.layers.21.self_attn.qkv_proj.bias\":[139579986420224,1152,2],\"model.layers.21.self_attn.o_proj.weight\":[139578553892864,802816,2],\"model.layers.21.mlp.gate_up_proj.weight\":[139578518274048,8716288,2],\"model.layers.21.mlp.down_proj.weight\":[139578555498496,4358144,2],\"model.layers.21.input_layernorm.weight\":[139579986422784,896,2],\"model.layers.21.post_attention_layernorm.weight\":[139579986424832,896,2],\"model.layers.22.self_attn.qkv_proj.weight\":[139578564214784,1032192,2],\"model.layers.22.self_attn.qkv_proj.bias\":[139579986426880,1152,2],\"model.layers.22.self_attn.o_proj.weight\":[139578566279168,802816,2],\"model.layers.22.mlp.gate_up_proj.weight\":[139578484719616,8716288,2],\"model.layers.22.mlp.down_proj.weight\":[139578451165184,4358144,2],\"model.layers.22.input_layernorm.weight\":[139579986429440,896,2],\"model.layers.22.post_attention_layernorm.weight\":[139579986431488,896,2],\"model.layers.23.self_attn.qkv_proj.weight\":[139578567884800,1032192,2],\"model.layers.23.self_attn.qkv_proj.bias\":[139579986433536,1152,2],\"model.layers.23.self_attn.o_proj.weight\":[139578569949184,802816,2],\"model.layers.23.mlp.gate_up_proj.weight\":[139578417610752,8716288,2],\"model.layers.23.mlp.down_proj.weight\":[139578459881472,4358144,2],\"model.layers.23.input_layernorm.weight\":[139579986436096,896,2],\"model.layers.23.post_attention_layernorm.weight\":[139579986438144,896,2],\"model.norm.weight\":[139579986440192,896,2]},\"internal_states\":[{\"model_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_mode\":\"auto\",\"tokenizer_worker_num\":1,\"skip_tokenizer_init\":false,\"load_format\":\"auto\",\"model_loader_extra_config\":\"{}\",\"rl_quant_profile\":null,\"trust_remote_code\":false,\"context_length\":null,\"is_embedding\":false,\"enable_multimodal\":null,\"revision\":null,\"model_impl\":\"auto\",\"host\":\"0.0.0.0\",\"port\":32051,\"fastapi_root_path\":\"\",\"grpc_mode\":false,\"skip_server_warmup\":false,\"warmups\":null,\"nccl_port\":null,\"checkpoint_engine_wait_weights_before_ready\":false,\"dtype\":\"auto\",\"quantization\":null,\"quantization_param_path\":null,\"kv_cache_dtype\":\"auto\",\"enable_fp32_lm_head\":false,\"modelopt_quant\":null,\"modelopt_checkpoint_restore_path\":null,\"modelopt_checkpoint_save_path\":null,\"modelopt_export_path\":null,\"quantize_and_serve\":false,\"mem_fraction_static\":0.841,\"max_running_requests\":128,\"max_queued_requests\":null,\"max_total_tokens\":20480,\"chunked_prefill_size\":8192,\"enable_dynamic_chunking\":false,\"max_prefill_tokens\":16384,\"schedule_policy\":\"fcfs\",\"enable_priority_scheduling\":false,\"abort_on_priority_when_disabled\":false,\"schedule_low_priority_values_first\":false,\"priority_scheduling_preemption_threshold\":10,\"schedule_conservativeness\":1.0,\"page_size\":1,\"hybrid_kvcache_ratio\":null,\"swa_full_tokens_ratio\":0.8,\"disable_hybrid_swa_memory\":false,\"radix_eviction_policy\":\"lru\",\"device\":\"cuda\",\"tp_size\":1,\"pp_size\":1,\"pp_max_micro_batch_size\":128,\"pp_async_batch_depth\":0,\"stream_interval\":1,\"stream_output\":false,\"random_seed\":1053632942,\"constrained_json_whitespace_pattern\":null,\"constrained_json_disable_any_whitespace\":false,\"watchdog_timeout\":300,\"dist_timeout\":null,\"download_dir\":null,\"base_gpu_id\":0,\"gpu_id_step\":1,\"sleep_on_idle\":false,\"mm_process_config\":{},\"log_level\":\"warning\",\"log_level_http\":null,\"log_requests\":false,\"log_requests_level\":2,\"crash_dump_folder\":null,\"show_time_cost\":false,\"enable_metrics\":false,\"enable_metrics_for_all_schedulers\":false,\"tokenizer_metrics_custom_labels_header\":\"x-custom-labels\",\"tokenizer_metrics_allowed_custom_labels\":null,\"bucket_time_to_first_token\":null,\"bucket_inter_token_latency\":null,\"bucket_e2e_request_latency\":null,\"collect_tokens_histogram\":false,\"prompt_tokens_buckets\":null,\"generation_tokens_buckets\":null,\"gc_warning_threshold_secs\":0.0,\"decode_log_interval\":40,\"enable_request_time_stats_logging\":false,\"kv_events_config\":null,\"enable_trace\":false,\"otlp_traces_endpoint\":\"localhost:4317\",\"export_metrics_to_file\":false,\"export_metrics_to_file_dir\":null,\"api_key\":null,\"served_model_name\":\"qwen/qwen2.5-0.5b-instruct\",\"weight_version\":\"default\",\"chat_template\":null,\"completion_template\":null,\"file_storage_path\":\"sglang_storage\",\"enable_cache_report\":false,\"reasoning_parser\":null,\"tool_call_parser\":null,\"tool_server\":null,\"sampling_defaults\":\"model\",\"dp_size\":1,\"load_balance_method\":\"round_robin\",\"load_watch_interval\":0.1,\"prefill_round_robin_balance\":false,\"dist_init_addr\":null,\"nnodes\":1,\"node_rank\":0,\"json_model_override_args\":\"{}\",\"preferred_sampling_params\":null,\"enable_lora\":null,\"max_lora_rank\":null,\"lora_target_modules\":null,\"lora_paths\":null,\"max_loaded_loras\":null,\"max_loras_per_batch\":8,\"lora_eviction_policy\":\"lru\",\"lora_backend\":\"csgmv\",\"max_lora_chunk_size\":16,\"attention_backend\":\"fa3\",\"decode_attention_backend\":\"fa3\",\"prefill_attention_backend\":\"fa3\",\"sampling_backend\":\"flashinfer\",\"grammar_backend\":\"xgrammar\",\"mm_attention_backend\":null,\"fp8_gemm_runner_backend\":\"auto\",\"nsa_prefill_backend\":\"flashmla_sparse\",\"nsa_decode_backend\":\"fa3\",\"enable_flashinfer_autotune\":false,\"speculative_algorithm\":null,\"speculative_draft_model_path\":null,\"speculative_draft_model_revision\":null,\"speculative_draft_load_format\":null,\"speculative_num_steps\":null,\"speculative_eagle_topk\":null,\"speculative_num_draft_tokens\":null,\"speculative_accept_threshold_single\":1.0,\"speculative_accept_threshold_acc\":1.0,\"speculative_token_map\":null,\"speculative_attention_mode\":\"prefill\",\"speculative_moe_runner_backend\":null,\"speculative_moe_a2a_backend\":null,\"speculative_ngram_min_match_window_size\":1,\"speculative_ngram_max_match_window_size\":12,\"speculative_ngram_min_bfs_breadth\":1,\"speculative_ngram_max_bfs_breadth\":10,\"speculative_ngram_match_type\":\"BFS\",\"speculative_ngram_branch_length\":18,\"speculative_ngram_capacity\":10000000,\"ep_size\":1,\"moe_a2a_backend\":\"none\",\"moe_runner_backend\":\"auto\",\"flashinfer_mxfp4_moe_precision\":\"default\",\"enable_flashinfer_allreduce_fusion\":false,\"deepep_mode\":\"auto\",\"ep_num_redundant_experts\":0,\"ep_dispatch_algorithm\":null,\"init_expert_location\":\"trivial\",\"enable_eplb\":false,\"eplb_algorithm\":\"auto\",\"eplb_rebalance_num_iterations\":1000,\"eplb_rebalance_layers_per_chunk\":null,\"eplb_min_rebalancing_utilization_threshold\":1.0,\"expert_distribution_recorder_mode\":null,\"expert_distribution_recorder_buffer_size\":1000,\"enable_expert_distribution_metrics\":false,\"deepep_config\":null,\"moe_dense_tp_size\":null,\"elastic_ep_backend\":null,\"mooncake_ib_device\":null,\"max_mamba_cache_size\":null,\"mamba_ssm_dtype\":\"float32\",\"mamba_full_memory_ratio\":0.9,\"enable_hierarchical_cache\":false,\"hicache_ratio\":2.0,\"hicache_size\":0,\"hicache_write_policy\":\"write_through\",\"hicache_io_backend\":\"kernel\",\"hicache_mem_layout\":\"layer_first\",\"hicache_storage_backend\":null,\"hicache_storage_prefetch_policy\":\"best_effort\",\"hicache_storage_backend_extra_config\":null,\"enable_lmcache\":false,\"kt_weight_path\":null,\"kt_method\":\"AMXINT4\",\"kt_cpuinfer\":null,\"kt_threadpool_count\":2,\"kt_num_gpu_experts\":null,\"kt_max_deferred_experts_per_token\":null,\"dllm_algorithm\":null,\"dllm_algorithm_config\":null,\"enable_double_sparsity\":false,\"ds_channel_config_path\":null,\"ds_heavy_channel_num\":32,\"ds_heavy_token_num\":256,\"ds_heavy_channel_type\":\"qk\",\"ds_sparse_decode_threshold\":4096,\"cpu_offload_gb\":0,\"offload_group_size\":-1,\"offload_num_in_group\":1,\"offload_prefetch_step\":1,\"offload_mode\":\"cpu\",\"multi_item_scoring_delimiter\":null,\"disable_radix_cache\":false,\"cuda_graph_max_bs\":4,\"cuda_graph_bs\":[1,2,4],\"disable_cuda_graph\":false,\"disable_cuda_graph_padding\":false,\"enable_profile_cuda_graph\":false,\"enable_cudagraph_gc\":false,\"enable_layerwise_nvtx_marker\":false,\"enable_nccl_nvls\":false,\"enable_symm_mem\":false,\"disable_flashinfer_cutlass_moe_fp4_allgather\":false,\"enable_tokenizer_batch_encode\":false,\"disable_tokenizer_batch_decode\":false,\"disable_outlines_disk_cache\":false,\"disable_custom_all_reduce\":false,\"enable_mscclpp\":false,\"enable_torch_symm_mem\":false,\"disable_overlap_schedule\":false,\"enable_mixed_chunk\":false,\"enable_dp_attention\":false,\"enable_dp_lm_head\":false,\"enable_two_batch_overlap\":false,\"enable_single_batch_overlap\":false,\"tbo_token_distribution_threshold\":0.48,\"enable_torch_compile\":false,\"enable_piecewise_cuda_graph\":false,\"enable_torch_compile_debug_mode\":false,\"torch_compile_max_bs\":32,\"piecewise_cuda_graph_max_tokens\":4096,\"piecewise_cuda_graph_tokens\":[4,8,12,16,20,24,28,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256,288,320,352,384,416,448,480,512,640,768,896,1024,1152,1280,1408,1536,1664,1792,1920,2048,2176,2304,2432,2560,2688,2816,2944,3072,3200,3328,3456,3584,3712,3840,3968,4096],\"piecewise_cuda_graph_compiler\":\"eager\",\"torchao_config\":\"\",\"enable_nan_detection\":false,\"enable_p2p_check\":false,\"triton_attention_reduce_in_fp32\":false,\"triton_attention_num_kv_splits\":8,\"triton_attention_split_tile_size\":null,\"num_continuous_decode_steps\":1,\"delete_ckpt_after_loading\":false,\"enable_memory_saver\":false,\"enable_weights_cpu_backup\":false,\"enable_draft_weights_cpu_backup\":false,\"allow_auto_truncate\":false,\"enable_custom_logit_processor\":false,\"flashinfer_mla_disable_ragged\":false,\"disable_shared_experts_fusion\":false,\"disable_chunked_prefix_cache\":true,\"disable_fast_image_processor\":false,\"keep_mm_feature_on_device\":false,\"enable_return_hidden_states\":false,\"scheduler_recv_interval\":1,\"numa_node\":null,\"enable_deterministic_inference\":false,\"rl_on_policy_target\":null,\"enable_attn_tp_input_scattered\":false,\"enable_nsa_prefill_context_parallel\":false,\"enable_fused_qk_norm_rope\":false,\"enable_dynamic_batch_tokenizer\":false,\"dynamic_batch_tokenizer_batch_size\":32,\"dynamic_batch_tokenizer_batch_timeout\":0.002,\"debug_tensor_dump_output_folder\":null,\"debug_tensor_dump_layers\":null,\"debug_tensor_dump_input_file\":null,\"debug_tensor_dump_inject\":false,\"disaggregation_mode\":\"null\",\"disaggregation_transfer_backend\":\"mooncake\",\"disaggregation_bootstrap_port\":8998,\"disaggregation_decode_tp\":null,\"disaggregation_decode_dp\":null,\"disaggregation_prefill_pp\":1,\"disaggregation_ib_device\":null,\"disaggregation_decode_enable_offload_kvcache\":false,\"num_reserved_decode_tokens\":512,\"disaggregation_decode_polling_interval\":1,\"custom_weight_loader\":[],\"weight_loader_disable_mmap\":false,\"remote_instance_weight_loader_seed_instance_ip\":null,\"remote_instance_weight_loader_seed_instance_service_port\":null,\"remote_instance_weight_loader_send_weights_group_ports\":null,\"remote_instance_weight_loader_backend\":\"nccl\",\"remote_instance_weight_loader_support_transfer_engine\":true,\"enable_pdmux\":false,\"pdmux_config_path\":null,\"sm_group_num\":8,\"mm_max_concurrent_calls\":32,\"mm_per_request_timeout\":10.0,\"enable_broadcast_mm_inputs_process\":false,\"decrypted_config_file\":null,\"decrypted_draft_config_file\":null,\"mm_enable_dp_encoder\":false,\"forward_hooks\":null,\"use_mla_backend\":false,\"last_gen_throughput\":305.4609299178426,\"memory_usage\":{\"weight\":1.06,\"kvcache\":0.23,\"token_capacity\":20480,\"graph\":0.07}}],\"version\":\"0.5.6.post2\"}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/get_server_info\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Check\n",
    "- `/health`: Check the health of the server.\n",
    "- `/health_generate`: Check the health of the server by generating one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:54:46.343511Z",
     "iopub.status.busy": "2025-12-12T10:54:46.343371Z",
     "iopub.status.idle": "2025-12-12T10:54:47.350188Z",
     "shell.execute_reply": "2025-12-12T10:54:47.349558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/health_generate\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:54:47.351781Z",
     "iopub.status.busy": "2025-12-12T10:54:47.351623Z",
     "iopub.status.idle": "2025-12-12T10:54:48.358003Z",
     "shell.execute_reply": "2025-12-12T10:54:48.357379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/health\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flush Cache\n",
    "\n",
    "Flush the radix cache. It will be automatically triggered when the model weights are updated by the `/update_weights` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:54:48.359657Z",
     "iopub.status.busy": "2025-12-12T10:54:48.359509Z",
     "iopub.status.idle": "2025-12-12T10:54:48.367016Z",
     "shell.execute_reply": "2025-12-12T10:54:48.366494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Cache flushed.<br>Please check backend logs for more details. (When there are running or waiting requests, the operation will not be performed.)<br></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/flush_cache\"\n",
    "\n",
    "response = requests.post(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Weights From Disk\n",
    "\n",
    "Update model weights from disk without restarting the server. Only applicable for models with the same architecture and parameter size.\n",
    "\n",
    "SGLang support `update_weights_from_disk` API for continuous evaluation during training (save checkpoint to disk and update weights from disk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:54:48.368343Z",
     "iopub.status.busy": "2025-12-12T10:54:48.368197Z",
     "iopub.status.idle": "2025-12-12T10:54:48.908478Z",
     "shell.execute_reply": "2025-12-12T10:54:48.907857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.50it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.50it/s]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"success\":true,\"message\":\"Succeeded to update model weights.\",\"num_paused_requests\":0}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# successful update with same architecture and size\n",
    "\n",
    "url = f\"http://localhost:{port}/update_weights_from_disk\"\n",
    "data = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.text)\n",
    "assert response.json()[\"success\"] is True\n",
    "assert response.json()[\"message\"] == \"Succeeded to update model weights.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:54:48.909988Z",
     "iopub.status.busy": "2025-12-12T10:54:48.909837Z",
     "iopub.status.idle": "2025-12-12T10:54:49.020057Z",
     "shell.execute_reply": "2025-12-12T10:54:49.019482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:54:49] Failed to get weights iterator: qwen/qwen2.5-0.5b-instruct-wrong (repository not found).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'success': False, 'message': 'Failed to get weights iterator: qwen/qwen2.5-0.5b-instruct-wrong (repository not found).', 'num_paused_requests': 0}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# failed update with different parameter size or wrong name\n",
    "\n",
    "url = f\"http://localhost:{port}/update_weights_from_disk\"\n",
    "data = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct-wrong\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "print_highlight(response_json)\n",
    "assert response_json[\"success\"] is False\n",
    "assert response_json[\"message\"] == (\n",
    "    \"Failed to get weights iterator: \"\n",
    "    \"qwen/qwen2.5-0.5b-instruct-wrong\"\n",
    "    \" (repository not found).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:54:49.021507Z",
     "iopub.status.busy": "2025-12-12T10:54:49.021279Z",
     "iopub.status.idle": "2025-12-12T10:54:49.039603Z",
     "shell.execute_reply": "2025-12-12T10:54:49.038925Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode (embedding model)\n",
    "\n",
    "Encode text into embeddings. Note that this API is only available for [embedding models](openai_api_embeddings.ipynb) and will raise an error for generation models.\n",
    "Therefore, we launch a new server to server an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:54:49.041967Z",
     "iopub.status.busy": "2025-12-12T10:54:49.041652Z",
     "iopub.status.idle": "2025-12-12T10:55:19.120505Z",
     "shell.execute_reply": "2025-12-12T10:55:19.119612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:54:54] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:54:54] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:54:54] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:54:56] INFO model_config.py:913: Downcasting torch.float32 to torch.float16.\n",
      "[2025-12-12 10:54:56] WARNING server_args.py:1416: Attention backend not explicitly specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:55:02] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:55:02] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:55:02] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 10:55:02] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:55:02] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:55:02] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 10:55:05.548363 2286359 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 10:55:05.548380 2286359 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.186.137.102 port: 12001\n",
      "I1212 10:55:05.548403 2286359 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.186.137.102:16733\n",
      "I1212 10:55:05.548478 2286359 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 10:55:05.551378 2286359 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 10:55:05.577410 2286359 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 10:55:05.578070 2286359 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:b9:05\n",
      "I1212 10:55:05.605322 2286359 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 10:55:05.605948 2286359 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:39:05\n",
      "I1212 10:55:05.722508 2286359 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 10:55:05.723166 2286359 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:b9:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 10:55:05.749354 2286359 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 10:55:05.749971 2286359 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:39:05\n",
      "I1212 10:55:05.777345 2286359 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 10:55:05.777945 2286359 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:b9:05\n",
      "I1212 10:55:05.805364 2286359 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 10:55:05.805971 2286359 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:39:05\n",
      "I1212 10:55:05.833703 2286359 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 10:55:05.834520 2286359 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:ba:89:66\n",
      "I1212 10:55:05.861362 2286359 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 10:55:05.861979 2286359 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:b9:05\n",
      "I1212 10:55:05.889360 2286359 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 10:55:05.889968 2286359 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:39:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 10:55:06.484836 2286359 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f921ffff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:55:09] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.08s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:55:14] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-1.5B-instruct \\\n",
    "    --host 0.0.0.0 --is-embedding --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:55:19.122297Z",
     "iopub.status.busy": "2025-12-12T10:55:19.122138Z",
     "iopub.status.idle": "2025-12-12T10:55:19.147011Z",
     "shell.execute_reply": "2025-12-12T10:55:19.146304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Text embedding (first 10): [-0.00023102760314941406, -0.04986572265625, -0.0032711029052734375, 0.011077880859375, -0.0140533447265625, 0.0159912109375, -0.01441192626953125, 0.0059051513671875, -0.0228424072265625, 0.0272979736328125]</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# successful encode for embedding model\n",
    "\n",
    "url = f\"http://localhost:{port}/encode\"\n",
    "data = {\"model\": \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", \"text\": \"Once upon a time\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "print_highlight(f\"Text embedding (first 10): {response_json['embedding'][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:55:19.148544Z",
     "iopub.status.busy": "2025-12-12T10:55:19.148388Z",
     "iopub.status.idle": "2025-12-12T10:55:19.158086Z",
     "shell.execute_reply": "2025-12-12T10:55:19.157303Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(embedding_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1/rerank (cross encoder rerank model)\n",
    "Rerank a list of documents given a query using a cross-encoder model. Note that this API is only available for cross encoder model like [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) with `attention-backend` `triton` and `torch_native`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:55:19.160274Z",
     "iopub.status.busy": "2025-12-12T10:55:19.160100Z",
     "iopub.status.idle": "2025-12-12T10:55:49.241675Z",
     "shell.execute_reply": "2025-12-12T10:55:49.240795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:55:24] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:55:24] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:55:24] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:55:27] INFO model_config.py:913: Downcasting torch.float32 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:55:33] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:55:33] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:55:33] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 10:55:33] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:55:33] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:55:33] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 10:55:36.710080 2287713 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 10:55:36.710103 2287713 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.186.137.102 port: 12001\n",
      "I1212 10:55:36.710127 2287713 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.186.137.102:15452\n",
      "I1212 10:55:36.710212 2287713 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 10:55:36.713232 2287713 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 10:55:36.741384 2287713 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 10:55:36.742259 2287713 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:b9:05\n",
      "I1212 10:55:36.769354 2287713 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 10:55:36.770164 2287713 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:39:05\n",
      "I1212 10:55:36.858510 2287713 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 10:55:36.859354 2287713 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:b9:05\n",
      "I1212 10:55:36.885334 2287713 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 10:55:36.886132 2287713 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:39:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 10:55:36.913347 2287713 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 10:55:36.914144 2287713 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:b9:05\n",
      "I1212 10:55:36.941282 2287713 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 10:55:36.982607 2287713 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:39:05\n",
      "I1212 10:55:37.005553 2287713 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 10:55:37.006580 2287713 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:ba:89:66\n",
      "I1212 10:55:37.033262 2287713 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 10:55:37.034035 2287713 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:b9:05\n",
      "I1212 10:55:37.061260 2287713 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 10:55:37.062042 2287713 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:39:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 10:55:37.877880 2287713 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7ef24bfff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:55:40] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.50it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.50it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:55:44] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reranker_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path BAAI/bge-reranker-v2-m3 \\\n",
    "    --host 0.0.0.0 --disable-radix-cache --chunked-prefill-size -1 --attention-backend triton --is-embedding --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:55:49.243464Z",
     "iopub.status.busy": "2025-12-12T10:55:49.243302Z",
     "iopub.status.idle": "2025-12-12T10:55:49.290060Z",
     "shell.execute_reply": "2025-12-12T10:55:49.289379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Score: 5.26 - Document: 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Score: -8.19 - Document: 'hi'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute rerank scores for query and documents\n",
    "\n",
    "url = f\"http://localhost:{port}/v1/rerank\"\n",
    "data = {\n",
    "    \"model\": \"BAAI/bge-reranker-v2-m3\",\n",
    "    \"query\": \"what is panda?\",\n",
    "    \"documents\": [\n",
    "        \"hi\",\n",
    "        \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "for item in response_json:\n",
    "    print_highlight(f\"Score: {item['score']:.2f} - Document: '{item['document']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:55:49.291640Z",
     "iopub.status.busy": "2025-12-12T10:55:49.291484Z",
     "iopub.status.idle": "2025-12-12T10:55:49.300696Z",
     "shell.execute_reply": "2025-12-12T10:55:49.299888Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(reranker_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify (reward model)\n",
    "\n",
    "SGLang Runtime also supports reward models. Here we use a reward model to classify the quality of pairwise generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:55:49.302467Z",
     "iopub.status.busy": "2025-12-12T10:55:49.302171Z",
     "iopub.status.idle": "2025-12-12T10:56:22.389403Z",
     "shell.execute_reply": "2025-12-12T10:56:22.388611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:55:54] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:55:54] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:55:54] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:55:56] WARNING server_args.py:1416: Attention backend not explicitly specified. Use flashinfer backend by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:56:03] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:56:03] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:56:03] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 10:56:03] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:56:03] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:56:03] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 10:56:06.302302 2289030 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 10:56:06.302320 2289030 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.186.137.102 port: 12001\n",
      "I1212 10:56:06.302342 2289030 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.186.137.102:16079\n",
      "I1212 10:56:06.302418 2289030 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 10:56:06.305274 2289030 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 10:56:06.329700 2289030 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 10:56:06.330408 2289030 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:b9:05\n",
      "I1212 10:56:06.353638 2289030 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 10:56:06.354292 2289030 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:39:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 10:56:06.522528 2289030 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 10:56:06.523219 2289030 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:b9:05\n",
      "I1212 10:56:06.545305 2289030 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 10:56:06.545895 2289030 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:39:05\n",
      "I1212 10:56:06.569305 2289030 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 10:56:06.569909 2289030 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:b9:05\n",
      "I1212 10:56:06.593334 2289030 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 10:56:06.593945 2289030 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:39:05\n",
      "I1212 10:56:06.617534 2289030 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 10:56:06.618341 2289030 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:ba:89:66\n",
      "I1212 10:56:06.641345 2289030 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 10:56:06.641954 2289030 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:b9:05\n",
      "I1212 10:56:06.665344 2289030 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 10:56:06.665956 2289030 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:39:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 10:56:07.324401 2289030 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7faf0bfff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:56:10] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:02,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.05s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.41it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.23it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:56:17] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that SGLang now treats embedding models and reward models as the same type of models.\n",
    "# This will be updated in the future.\n",
    "\n",
    "reward_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path Skywork/Skywork-Reward-Llama-3.1-8B-v0.2 --host 0.0.0.0 --is-embedding --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:56:22.391049Z",
     "iopub.status.busy": "2025-12-12T10:56:22.390902Z",
     "iopub.status.idle": "2025-12-12T10:56:22.942354Z",
     "shell.execute_reply": "2025-12-12T10:56:22.941699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reward: -24.125</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reward: 1.0703125</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "PROMPT = (\n",
    "    \"What is the range of the numeric output of a sigmoid node in a neural network?\"\n",
    ")\n",
    "\n",
    "RESPONSE1 = \"The output of a sigmoid node is bounded between -1 and 1.\"\n",
    "RESPONSE2 = \"The output of a sigmoid node is bounded between 0 and 1.\"\n",
    "\n",
    "CONVS = [\n",
    "    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", \"content\": RESPONSE1}],\n",
    "    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", \"content\": RESPONSE2}],\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\")\n",
    "prompts = tokenizer.apply_chat_template(CONVS, tokenize=False, return_dict=False)\n",
    "\n",
    "url = f\"http://localhost:{port}/classify\"\n",
    "data = {\"model\": \"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\", \"text\": prompts}\n",
    "\n",
    "responses = requests.post(url, json=data).json()\n",
    "for response in responses:\n",
    "    print_highlight(f\"reward: {response['embedding'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:56:22.943945Z",
     "iopub.status.busy": "2025-12-12T10:56:22.943794Z",
     "iopub.status.idle": "2025-12-12T10:56:22.953596Z",
     "shell.execute_reply": "2025-12-12T10:56:22.952796Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(reward_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture expert selection distribution in MoE models\n",
    "\n",
    "SGLang Runtime supports recording the number of times an expert is selected in a MoE model run for each expert in the model. This is useful when analyzing the throughput of the model and plan for optimization.\n",
    "\n",
    "*Note: We only print out the first 10 lines of the csv below for better readability. Please adjust accordingly if you want to analyze the results more deeply.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:56:22.955623Z",
     "iopub.status.busy": "2025-12-12T10:56:22.955447Z",
     "iopub.status.idle": "2025-12-12T10:57:02.045233Z",
     "shell.execute_reply": "2025-12-12T10:57:02.044597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:56:28] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:56:28] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:56:28] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:56:30] WARNING server_args.py:1416: Attention backend not explicitly specified. Use flashinfer backend by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:56:37] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:56:37] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:56:37] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:56:37] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 10:56:37] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:56:37] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 10:56:40.332248 2290531 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 10:56:40.332269 2290531 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.186.137.102 port: 12001\n",
      "I1212 10:56:40.332291 2290531 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.186.137.102:16480\n",
      "I1212 10:56:40.332367 2290531 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 10:56:40.335321 2290531 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 10:56:40.361351 2290531 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 10:56:40.362001 2290531 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:b9:05\n",
      "I1212 10:56:40.389338 2290531 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 10:56:40.389977 2290531 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:39:05\n",
      "I1212 10:56:40.474506 2290531 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 10:56:40.475189 2290531 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:b9:05\n",
      "I1212 10:56:40.501348 2290531 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 10:56:40.501974 2290531 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:39:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 10:56:40.529373 2290531 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 10:56:40.530014 2290531 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:b9:05\n",
      "I1212 10:56:40.557281 2290531 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 10:56:40.557894 2290531 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:39:05\n",
      "I1212 10:56:40.581663 2290531 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 10:56:40.582576 2290531 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:ba:89:66\n",
      "I1212 10:56:40.609279 2290531 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 10:56:40.609900 2290531 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:b9:05\n",
      "I1212 10:56:40.637290 2290531 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 10:56:40.637935 2290531 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:39:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 10:56:41.363453 2290531 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f39affff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:56:43] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:04,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:04,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:02<00:04,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:03<00:03,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:04<00:02,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:05<00:01,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:06<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.44it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.27it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=47.28 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:56:52] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /public_sglang_ci/runner-l3d-gpu-67/_work/sglang/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=60,N=1408,device_name=NVIDIA_H100_80GB_HBM3.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-12 10:56:52] Using MoE kernel config with down_moe=False. Performance might be sub-optimal! Config file not found at /public_sglang_ci/runner-l3d-gpu-67/_work/sglang/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=60,N=1408,device_name=NVIDIA_H100_80GB_HBM3_down.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=47.28 GB):  33%|███▎      | 1/3 [00:01<00:02,  1.33s/it]\r",
      "Capturing batches (bs=2 avail_mem=47.16 GB):  33%|███▎      | 1/3 [00:01<00:02,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=2 avail_mem=47.16 GB):  67%|██████▋   | 2/3 [00:02<00:01,  1.12s/it]\r",
      "Capturing batches (bs=1 avail_mem=47.15 GB):  67%|██████▋   | 2/3 [00:02<00:01,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=1 avail_mem=47.15 GB): 100%|██████████| 3/3 [00:02<00:00,  1.30it/s]\r",
      "Capturing batches (bs=1 avail_mem=47.15 GB): 100%|██████████| 3/3 [00:02<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:56:57] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expert_record_server_process, port = launch_server_cmd(\n",
    "    \"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0 --expert-distribution-recorder-mode stat --log-level warning\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:57:02.046947Z",
     "iopub.status.busy": "2025-12-12T10:57:02.046797Z",
     "iopub.status.idle": "2025-12-12T10:57:03.425784Z",
     "shell.execute_reply": "2025-12-12T10:57:03.425107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><Response [200]></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': \" Is it Paris? Berlin? Rome? London? Why is it so difficult to learn the names of the capitals, national capitals themselves? They go by in a hurry, seeming to have no name, forgetting names after only one hearing, not once noticing until the question comes up years later: Berlin?\\nAre these the tricky words: to elf he kneel, secrets to keep, devour unto plague; to feel and fly, — meek, ostensibly, — flay.\\nIt's named after a farm — the bar had a granary in front of it. How strange, towns named after grains: Kingsbury, Wairoa,\", 'output_ids': [2160, 432, 12095, 30, 19846, 30, 21718, 30, 7148, 30, 8429, 374, 432, 773, 5000, 311, 3960, 279, 5036, 315, 279, 92999, 11, 5313, 92999, 5577, 30, 2379, 728, 553, 304, 264, 47235, 11, 75695, 311, 614, 902, 829, 11, 65027, 5036, 1283, 1172, 825, 10778, 11, 537, 3055, 61364, 3080, 279, 3405, 4041, 705, 1635, 2937, 25, 19846, 5267, 11526, 1493, 279, 33453, 4244, 25, 311, 40745, 566, 13853, 301, 11, 23594, 311, 2506, 11, 87189, 29349, 54437, 26, 311, 2666, 323, 11466, 11, 1959, 752, 1225, 11, 80494, 11, 1959, 1320, 352, 624, 2132, 594, 6941, 1283, 264, 8785, 1959, 279, 3619, 1030, 264, 15732, 658, 304, 4065, 315, 432, 13, 2585, 14888, 11, 24824, 6941, 1283, 40836, 25, 24019, 19603, 11, 467, 25549, 64, 11], 'meta_info': {'id': '1022c10509044d2cbc26ad3e6197b4cb', 'finish_reason': {'type': 'length', 'length': 128}, 'prompt_tokens': 7, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 128, 'cached_tokens': 0, 'e2e_latency': 0.5538461208343506, 'response_sent_to_client_ts': 1765537022.6076555}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><Response [200]></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><Response [200]></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = requests.post(f\"http://localhost:{port}/start_expert_distribution_record\")\n",
    "print_highlight(response)\n",
    "\n",
    "url = f\"http://localhost:{port}/generate\"\n",
    "data = {\"text\": \"What is the capital of France?\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.json())\n",
    "\n",
    "response = requests.post(f\"http://localhost:{port}/stop_expert_distribution_record\")\n",
    "print_highlight(response)\n",
    "\n",
    "response = requests.post(f\"http://localhost:{port}/dump_expert_distribution_record\")\n",
    "print_highlight(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:57:03.427513Z",
     "iopub.status.busy": "2025-12-12T10:57:03.427365Z",
     "iopub.status.idle": "2025-12-12T10:57:03.445175Z",
     "shell.execute_reply": "2025-12-12T10:57:03.444329Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(expert_record_server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize/Detokenize Example (Round Trip)\n",
    "\n",
    "This example demonstrates how to use the /tokenize and /detokenize endpoints together. We first tokenize a string, then detokenize the resulting IDs to reconstruct the original text. This workflow is useful when you need to handle tokenization externally but still leverage the server for detokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:57:03.447531Z",
     "iopub.status.busy": "2025-12-12T10:57:03.447275Z",
     "iopub.status.idle": "2025-12-12T10:57:32.521630Z",
     "shell.execute_reply": "2025-12-12T10:57:32.520859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:08] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:57:08] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:57:08] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:11] WARNING server_args.py:1416: Attention backend not explicitly specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:11] server_args=ServerArgs(model_path='qwen/qwen2.5-0.5b-instruct', tokenizer_path='qwen/qwen2.5-0.5b-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', rl_quant_profile=None, trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=34367, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.841, max_running_requests=128, max_queued_requests=None, max_total_tokens=20480, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=522434987, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, mm_process_config={}, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='qwen/qwen2.5-0.5b-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='fa3', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_moe_a2a_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_support_transfer_engine=True, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None, mm_enable_dp_encoder=False, forward_hooks=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:11] Using default HuggingFace chat template with detected content format: string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:17] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:57:17] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 10:57:17] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:57:17] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 10:57:17] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 10:57:17] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:20] Init torch distributed begin.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[2025-12-12 10:57:20] Init torch distributed ends. mem usage=0.00 GB\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 10:57:20.518213 2292191 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 10:57:20.518235 2292191 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.186.137.102 port: 12001\n",
      "I1212 10:57:20.518262 2292191 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.186.137.102:15525\n",
      "I1212 10:57:20.518342 2292191 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 10:57:20.521332 2292191 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 10:57:20.549372 2292191 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 10:57:20.550057 2292191 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:b9:05\n",
      "I1212 10:57:20.577339 2292191 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 10:57:20.577986 2292191 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:39:05\n",
      "I1212 10:57:20.662506 2292191 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 10:57:20.663156 2292191 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:b9:05\n",
      "I1212 10:57:20.689342 2292191 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 10:57:20.689970 2292191 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:39:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 10:57:20.717331 2292191 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 10:57:20.717952 2292191 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:b9:05\n",
      "I1212 10:57:20.745280 2292191 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 10:57:20.745898 2292191 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:39:05\n",
      "I1212 10:57:20.774063 2292191 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 10:57:20.774995 2292191 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:ba:89:66\n",
      "I1212 10:57:20.801280 2292191 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 10:57:20.801898 2292191 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:b9:05\n",
      "I1212 10:57:20.829282 2292191 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 10:57:20.829897 2292191 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:39:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 10:57:21.479316 2292191 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7fa627fff010, len: 2147483648: Operation not permitted [1]\n",
      "[2025-12-12 10:57:21] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:24] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-12-12 10:57:24] Load weight begin. avail mem=60.33 GB\n",
      "[2025-12-12 10:57:24] Found local HF snapshot for qwen/qwen2.5-0.5b-instruct at /hf_home/hub/models--qwen--qwen2.5-0.5b-instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775; skipping download.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:24] No model.safetensors.index.json found in remote.\n",
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.43it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.42it/s]\n",
      "\n",
      "[2025-12-12 10:57:24] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=59.23 GB, mem usage=1.09 GB.\n",
      "[2025-12-12 10:57:24] Using KV cache dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:24] KV Cache is allocated. #tokens: 20480, K size: 0.12 GB, V size: 0.12 GB\n",
      "[2025-12-12 10:57:24] Memory pool end. avail mem=58.83 GB\n",
      "[2025-12-12 10:57:24] Capture cuda graph begin. This can take up to several minutes. avail mem=58.74 GB\n",
      "[2025-12-12 10:57:24] Capture cuda graph bs [1, 2, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=58.74 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=58.74 GB):  33%|███▎      | 1/3 [00:00<00:00,  4.33it/s]\r",
      "Capturing batches (bs=2 avail_mem=58.68 GB):  33%|███▎      | 1/3 [00:00<00:00,  4.33it/s]\r",
      "Capturing batches (bs=1 avail_mem=58.67 GB):  33%|███▎      | 1/3 [00:00<00:00,  4.33it/s]\r",
      "Capturing batches (bs=1 avail_mem=58.67 GB): 100%|██████████| 3/3 [00:00<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:25] Capture cuda graph end. Time elapsed: 0.82 s. mem usage=0.07 GB. avail mem=58.67 GB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:26] max_total_num_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=128, context_len=32768, available_gpu_mem=58.67 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:26] INFO:     Started server process [2291744]\n",
      "[2025-12-12 10:57:26] INFO:     Waiting for application startup.\n",
      "[2025-12-12 10:57:26] Using default chat sampling params from model generation config: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
      "[2025-12-12 10:57:26] Using default chat sampling params from model generation config: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
      "[2025-12-12 10:57:26] INFO:     Application startup complete.\n",
      "[2025-12-12 10:57:26] INFO:     Uvicorn running on http://127.0.0.1:34367 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:27] INFO:     127.0.0.1:48096 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-12-12 10:57:27] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n",
      "[2025-12-12 10:57:27] INFO:     127.0.0.1:48104 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-12-12 10:57:27] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:28] INFO:     127.0.0.1:48118 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-12-12 10:57:28] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_free_server_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:57:32.523543Z",
     "iopub.status.busy": "2025-12-12T10:57:32.523388Z",
     "iopub.status.idle": "2025-12-12T10:57:32.540203Z",
     "shell.execute_reply": "2025-12-12T10:57:32.539535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Original Input Text:<br>'SGLang provides efficient tokenization endpoints.'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:32] INFO:     127.0.0.1:48122 - \"POST /tokenize HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br>Tokenized Output (IDs):<br>[50, 3825, 524, 5707, 11050, 3950, 2022, 36342, 13]</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Token Count: 9</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Max Model Length: 131072</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 10:57:32] INFO:     127.0.0.1:48128 - \"POST /detokenize HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br>Detokenized Output (Text):<br>'SGLang provides efficient tokenization endpoints.'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br>Round Trip Successful: Original and reconstructed text match.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from sglang.utils import print_highlight\n",
    "\n",
    "base_url = f\"http://localhost:{port}\"\n",
    "tokenize_url = f\"{base_url}/tokenize\"\n",
    "detokenize_url = f\"{base_url}/detokenize\"\n",
    "\n",
    "model_name = \"qwen/qwen2.5-0.5b-instruct\"\n",
    "input_text = \"SGLang provides efficient tokenization endpoints.\"\n",
    "print_highlight(f\"Original Input Text:\\n'{input_text}'\")\n",
    "\n",
    "# --- tokenize the input text ---\n",
    "tokenize_payload = {\n",
    "    \"model\": model_name,\n",
    "    \"prompt\": input_text,\n",
    "    \"add_special_tokens\": False,\n",
    "}\n",
    "try:\n",
    "    tokenize_response = requests.post(tokenize_url, json=tokenize_payload)\n",
    "    tokenize_response.raise_for_status()\n",
    "    tokenization_result = tokenize_response.json()\n",
    "    token_ids = tokenization_result.get(\"tokens\")\n",
    "\n",
    "    if not token_ids:\n",
    "        raise ValueError(\"Tokenization returned empty tokens.\")\n",
    "\n",
    "    print_highlight(f\"\\nTokenized Output (IDs):\\n{token_ids}\")\n",
    "    print_highlight(f\"Token Count: {tokenization_result.get('count')}\")\n",
    "    print_highlight(f\"Max Model Length: {tokenization_result.get('max_model_len')}\")\n",
    "\n",
    "    # --- detokenize the obtained token IDs ---\n",
    "    detokenize_payload = {\n",
    "        \"model\": model_name,\n",
    "        \"tokens\": token_ids,\n",
    "        \"skip_special_tokens\": True,\n",
    "    }\n",
    "\n",
    "    detokenize_response = requests.post(detokenize_url, json=detokenize_payload)\n",
    "    detokenize_response.raise_for_status()\n",
    "    detokenization_result = detokenize_response.json()\n",
    "    reconstructed_text = detokenization_result.get(\"text\")\n",
    "\n",
    "    print_highlight(f\"\\nDetokenized Output (Text):\\n'{reconstructed_text}'\")\n",
    "\n",
    "    if input_text == reconstructed_text:\n",
    "        print_highlight(\n",
    "            \"\\nRound Trip Successful: Original and reconstructed text match.\"\n",
    "        )\n",
    "    else:\n",
    "        print_highlight(\n",
    "            \"\\nRound Trip Mismatch: Original and reconstructed text differ.\"\n",
    "        )\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print_highlight(f\"\\nHTTP Request Error: {e}\")\n",
    "except Exception as e:\n",
    "    print_highlight(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T10:57:32.541794Z",
     "iopub.status.busy": "2025-12-12T10:57:32.541647Z",
     "iopub.status.idle": "2025-12-12T10:57:32.557910Z",
     "shell.execute_reply": "2025-12-12T10:57:32.557318Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(tokenizer_free_server_process)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
