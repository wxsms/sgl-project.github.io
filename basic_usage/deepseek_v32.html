
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>DeepSeek V3.2 Usage &#8212; SGLang</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=8fcf004d"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=ccdb6887"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'basic_usage/deepseek_v32';</script>
    <link rel="icon" href="../_static/logo.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Launch GLM-4.5 / GLM-4.6 / GLM-4.7 with SGLang" href="glm45.html" />
    <link rel="prev" title="DeepSeek V3/V3.1/R1 Usage" href="deepseek_v3.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Feb 12, 2026"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="SGLang - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="SGLang - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../get_started/install.html">Install SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Basic Usage</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="send_request.html">Sending Requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="openai_api.html">OpenAI-Compatible APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="ollama_api.html">Ollama-Compatible API</a></li>
<li class="toctree-l1"><a class="reference internal" href="offline_engine_api.html">Offline Engine API</a></li>
<li class="toctree-l1"><a class="reference internal" href="native_api.html">SGLang Native APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="sampling_params.html">Sampling Parameters</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="popular_model_usage.html">Popular Model Usage (DeepSeek, GPT-OSS, GLM, Llama, MiniMax, Qwen, and more)</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="deepseek_v3.html">DeepSeek V3/V3.1/R1 Usage</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">DeepSeek V3.2 Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="glm45.html">Launch GLM-4.5 / GLM-4.6 / GLM-4.7 with SGLang</a></li>
<li class="toctree-l2"><a class="reference internal" href="glmv.html">GLM-4.6V / GLM-4.5V Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpt_oss.html">GPT OSS Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="minimax_m2.html">MiniMax M2.1/M2 Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="qwen3.html">Qwen3-Next Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="qwen3_vl.html">Qwen3-VL Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepseek_ocr.html">DeepSeek OCR (OCR-1 / OCR-2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="llama4.html">Llama4 Usage</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion_llms.html">Diffusion Language Models (dLLMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion.html">Diffusion</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/server_arguments.html">Server Arguments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/hyperparameter_tuning.html">Hyperparameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/attention_backend.html">Attention Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/speculative_decoding.html">Speculative Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/structured_outputs.html">Structured Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/structured_outputs_for_reasoning_models.html">Structured Outputs For Reasoning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/tool_parser.html">Tool Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/separate_reasoning.html">Reasoning Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/quantized_kv_cache.html">Quantized KV Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/expert_parallelism.html">Expert Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/dp_dpa_smg_guide.html">DP, DPA and SGLang DP Router</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/lora.html">LoRA Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/pd_disaggregation.html">PD Disaggregation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/epd_disaggregation.html">EPD Disaggregation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/pipeline_parallelism.html">Pipeline Parallelism for Long Context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/hicache.html">Hierarchical KV Caching (HiCache)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/vlm_query.html">Query VLM with Offline Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/dp_for_multi_modal_encoder.html">DP for Multi-Modal Encoder in SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/cuda_graph_for_multi_modal_encoder.html">Cuda Graph for Multi-Modal Encoder in SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/sgl_model_gateway.html">SGLang Model Gateway</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/deterministic_inference.html">Deterministic Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/observability.html">Observability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/checkpoint_engine.html">Checkpoint Engine Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/sglang_for_rl.html">SGLang for RL Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supported Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../supported_models/text_generation/index.html">Text Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/image_generation/index.html">Image Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/retrieval_ranking/index.html">Retrieval &amp; Ranking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/specialized/index.html">Specialized Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/extending/index.html">Extending SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Hardware Platforms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../platforms/amd_gpu.html">AMD GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/cpu_server.html">CPU Servers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/tpu.html">TPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/nvidia_jetson.html">NVIDIA Jetson Orin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/ascend_npu_support.html">Ascend NPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/xpu.html">XPU</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/contribution_guide.html">Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/development_guide_using_docker.html">Development Guide Using Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/development_jit_kernel_guide.html">Development Guide for JIT Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/benchmark_and_profiling.html">Benchmark and Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/bench_serving.html">Bench Serving Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/evaluating_new_models.html">Evaluating New Models with SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../references/faq.html">Troubleshooting and Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/environment_variables.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/production_metrics.html">Production Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/production_request_trace.html">Production Request Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/multi_node_deployment/multi_node_index.html">Multi-Node Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/custom_chat_template.html">Custom Chat Template</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/frontend/frontend_index.html">Frontend Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/post_training_integration.html">Post-Training Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/learn_more.html">Learn More and Join the Community</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/blob/main/basic_usage/deepseek_v32.md?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/edit/main/basic_usage/deepseek_v32.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/issues/new?title=Issue%20on%20page%20%2Fbasic_usage/deepseek_v32.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/basic_usage/deepseek_v32.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>DeepSeek V3.2 Usage</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installation">Installation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#docker">Docker</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-from-source">Build From Source</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#launch-deepseek-v3-2-with-sglang">Launch DeepSeek V3.2 with SGLang</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration-tips">Configuration Tips</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-token-prediction">Multi-token Prediction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#function-calling-and-reasoning-parser">Function Calling and Reasoning Parser</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nvfp4-checkpoint">NVFP4 Checkpoint</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pd-disaggregation">PD Disaggregation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking-results">Benchmarking Results</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-test-with-gsm8k">Accuracy Test with <code class="docutils literal notranslate"><span class="pre">gsm8k</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-test-with-gpqa-diamond">Accuracy Test with <code class="docutils literal notranslate"><span class="pre">gpqa-diamond</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-test-with-aime-2025">Accuracy Test with <code class="docutils literal notranslate"><span class="pre">aime</span> <span class="pre">2025</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dsa-long-sequence-context-parallel-optimization-experimental">DSA long sequence context parallel optimization(experimental)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#in-sequence-splitting">In sequence splitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#round-robin-splitting-default-setting">Round robin splitting (default setting)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pipeline-parallel-context-parallel-pp-cp">Pipeline Parallel + Context Parallel (PP + CP)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-usage">Standard Usage</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pd-disaggregation-with-pp-cp">PD Disaggregation with PP + CP</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="deepseek-v3-2-usage">
<h1>DeepSeek V3.2 Usage<a class="headerlink" href="#deepseek-v3-2-usage" title="Link to this heading">#</a></h1>
<p>DeepSeek-V3.2 model family equips DeepSeek-V3.1-Terminus with DeepSeek Sparse Attention (DSA) through continued training. With DSA, a fine-grained sparse attention mechanism powered by a lightning indexer, DeepSeek-V3.2 achieves efficiency improvements in long-context scenarios.</p>
<p>For reporting issues or tracking upcoming features, please refer to this <a class="reference external" href="https://github.com/sgl-project/sglang/issues/11060">Roadmap</a>.</p>
<p>Note: This document is originally written for the usage of <a class="reference external" href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp">DeepSeek-V3.2-Exp</a> model. The usage of <a class="reference external" href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2">DeepSeek-V3.2</a> or <a class="reference external" href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale">DeepSeek-V3.2-Speciale</a> is the same as DeepSeek-V3.2-Exp except for the tool call parser.</p>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Link to this heading">#</a></h2>
<section id="docker">
<h3>Docker<a class="headerlink" href="#docker" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># H200/B200</span>
docker<span class="w"> </span>pull<span class="w"> </span>lmsysorg/sglang:latest

<span class="c1"># MI350/MI355</span>
docker<span class="w"> </span>pull<span class="w"> </span>lmsysorg/sglang:v0.5.8-rocm700-mi35x

<span class="c1"># MI300</span>
<span class="c1"># v0.5.8-rocm700-mi30x does not include PR #17504. Prefer the newest MI30x ROCm</span>
<span class="c1"># image tag from Docker Hub when available, or build from source (below).</span>
docker<span class="w"> </span>pull<span class="w"> </span>lmsysorg/sglang:v0.5.8-rocm700-mi30x


<span class="c1"># NPUs</span>
docker<span class="w"> </span>pull<span class="w"> </span>lmsysorg/sglang:dsv32-a2
docker<span class="w"> </span>pull<span class="w"> </span>lmsysorg/sglang:dsv32-a3
</pre></div>
</div>
</section>
<section id="build-from-source">
<h3>Build From Source<a class="headerlink" href="#build-from-source" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install SGLang</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/sgl-project/sglang
<span class="nb">cd</span><span class="w"> </span>sglang
pip3<span class="w"> </span>install<span class="w"> </span>pip<span class="w"> </span>--upgrade
pip3<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;python&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="launch-deepseek-v3-2-with-sglang">
<h2>Launch DeepSeek V3.2 with SGLang<a class="headerlink" href="#launch-deepseek-v3-2-with-sglang" title="Link to this heading">#</a></h2>
<p>To serve <a class="reference external" href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp">DeepSeek-V3.2-Exp</a> on 8xH200/B200 GPUs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Launch with TP + DP (Recommended)</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w"> </span>--tp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--dp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--enable-dp-attention

<span class="c1"># Launch with EP + DP</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w"> </span>--tp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--ep<span class="w"> </span><span class="m">8</span><span class="w"> </span>--dp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--enable-dp-attention

<span class="c1"># Launch with Pure TP</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w"> </span>--tp<span class="w"> </span><span class="m">8</span>

<span class="c1"># Launch with TP on MI30x/MI35x</span>
python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w"> </span>--tp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--nsa-prefill-backend<span class="w"> </span>tilelang<span class="w"> </span>--nsa-decode-backend<span class="w"> </span>tilelang
</pre></div>
</div>
<section id="configuration-tips">
<h3>Configuration Tips<a class="headerlink" href="#configuration-tips" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>DP Attention (Recommended)</strong>: For DeepSeek V3.2 model, the kernels are customized for the use case of <code class="docutils literal notranslate"><span class="pre">dp_size=8</span></code>, so DP attention (<code class="docutils literal notranslate"><span class="pre">--dp</span> <span class="pre">8</span> <span class="pre">--enable-dp-attention</span></code>) is the recommended configuration for better stability and performance. All test cases use this configuration by default.</p></li>
<li><p><strong>Pure TP Mode</strong>: Launching with pure TP (without <code class="docutils literal notranslate"><span class="pre">--dp</span></code> and <code class="docutils literal notranslate"><span class="pre">--enable-dp-attention</span></code>) is also supported. Note that this mode has not been fully validated in PD disaggregation scenarios.</p></li>
<li><p><strong>Short-sequence MHA prefill (adaptive)</strong>: For short prefill sequences (default threshold: <strong>2048 tokens</strong>), the NSA backend uses standard MHA automatically (no extra flags). On H200 (SM90) this path uses the FlashAttention variable-length kernel; on B200 (SM100) it uses TRT-LLM ragged MHA. MHA uses <code class="docutils literal notranslate"><span class="pre">MHA_ONE_SHOT</span></code> for best performance. <code class="docutils literal notranslate"><span class="pre">MHA_ONE_SHOT</span></code> computes multi-head attention over all tokens (both cached prefix and newly extended tokens) in a single kernel invocation, avoiding the overhead of chunked KV cache processing. This achieves optimal throughput for short sequences where total sequence length fits within the chunk capacity limit.</p></li>
<li><p><strong>Choices of Attention Kernels</strong>: The attention backend is automatically set to <code class="docutils literal notranslate"><span class="pre">nsa</span></code> attention backend for DeepSeek V3.2 model. In this backend, different kernels for sparse prefilling/decoding are implemented, which can be specified by <code class="docutils literal notranslate"><span class="pre">--nsa-prefill-backend</span></code> and <code class="docutils literal notranslate"><span class="pre">--nsa-decode-backend</span></code> server arguments. The choices of nsa prefill/decode attention kernels include:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">flashmla_sparse</span></code>: <code class="docutils literal notranslate"><span class="pre">flash_mla_sparse_fwd</span></code> kernel from <code class="docutils literal notranslate"><span class="pre">flash_mla</span></code> library. Can run on both Hopper and Blackwell GPUs. It requires bf16 q, kv inputs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">flashmla_kv</span></code>: <code class="docutils literal notranslate"><span class="pre">flash_mla_with_kvcache</span></code> kernel from <code class="docutils literal notranslate"><span class="pre">flash_mla</span></code> library. Can run on both Hopper and Blackwell GPUs. It requires bf16 q, fp8 k_cache inputs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fa3</span></code>: <code class="docutils literal notranslate"><span class="pre">flash_attn_with_kvcache</span></code> kernel from <code class="docutils literal notranslate"><span class="pre">flash_attn</span></code> library. Can only run on Hopper GPUs. It requires bf16 q, kv inputs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tilelang</span></code>: <code class="docutils literal notranslate"><span class="pre">tilelang</span></code> implementation that can run on GPU, HPU and NPU.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">aiter</span></code>: Aiter kernel on AMD HPUs. Can only be used as decode kernel.</p></li>
</ul>
</li>
<li><p>On the basis of performance benchmarks, the default configuration on H200 and B200 are set as follows :</p>
<ul>
<li><p>H200: <code class="docutils literal notranslate"><span class="pre">flashmla_sparse</span></code> prefill attention (short-seq prefill uses MHA via FlashAttention varlen), <code class="docutils literal notranslate"><span class="pre">fa3</span></code> decode attention, <code class="docutils literal notranslate"><span class="pre">bf16</span></code> kv cache dtype.</p></li>
<li><p>B200: <code class="docutils literal notranslate"><span class="pre">flashmla_auto</span></code> prefill attention (short-seq prefill uses MHA via TRT-LLM ragged), <code class="docutils literal notranslate"><span class="pre">flashmla_kv</span></code> decode attention, <code class="docutils literal notranslate"><span class="pre">fp8_e4m3</span></code> kv cache dtype. <code class="docutils literal notranslate"><span class="pre">flashmla_auto</span></code> enables automatic selection of either <code class="docutils literal notranslate"><span class="pre">flashmla_sparse</span></code> or <code class="docutils literal notranslate"><span class="pre">flashmla_kv</span></code> kernel for prefill based on KV cache dtype, hardware, and heuristics. When FP8 KV cache is enabled and <code class="docutils literal notranslate"><span class="pre">total_kv_tokens</span> <span class="pre">&lt;</span> <span class="pre">total_q_tokens</span> <span class="pre">*</span> <span class="pre">512</span></code>, it uses the <code class="docutils literal notranslate"><span class="pre">flashmla_sparse</span></code> kernel; otherwise, it falls back to the <code class="docutils literal notranslate"><span class="pre">flashmla_kv</span></code> kernel. The heuristics may need to be tuned if the performance of either the <code class="docutils literal notranslate"><span class="pre">flashmla_sparse</span></code> or <code class="docutils literal notranslate"><span class="pre">flashmla_kv</span></code> kernel changes significantly.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="multi-token-prediction">
<h2>Multi-token Prediction<a class="headerlink" href="#multi-token-prediction" title="Link to this heading">#</a></h2>
<p>SGLang implements Multi-Token Prediction (MTP) for DeepSeek V3.2 based on <a class="reference external" href="https://docs.sglang.io/advanced_features/speculative_decoding.html#EAGLE-Decoding">EAGLE speculative decoding</a>. With this optimization, the decoding speed can be improved significantly on small batch sizes. Please look at <a class="reference external" href="https://github.com/sgl-project/sglang/pull/11652">this PR</a> for more information.</p>
<p>Example usage with DP Attention:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w"> </span>--tp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--dp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--enable-dp-attention<span class="w"> </span>--speculative-algorithm<span class="w"> </span>EAGLE<span class="w"> </span>--speculative-num-steps<span class="w"> </span><span class="m">3</span><span class="w"> </span>--speculative-eagle-topk<span class="w"> </span><span class="m">1</span><span class="w"> </span>--speculative-num-draft-tokens<span class="w"> </span><span class="m">4</span>
</pre></div>
</div>
<p>Example usage with Pure TP:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w"> </span>--tp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--speculative-algorithm<span class="w"> </span>EAGLE<span class="w"> </span>--speculative-num-steps<span class="w"> </span><span class="m">3</span><span class="w"> </span>--speculative-eagle-topk<span class="w"> </span><span class="m">1</span><span class="w"> </span>--speculative-num-draft-tokens<span class="w"> </span><span class="m">4</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The best configuration for <code class="docutils literal notranslate"><span class="pre">--speculative-num-steps</span></code>, <code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span></code> and <code class="docutils literal notranslate"><span class="pre">--speculative-num-draft-tokens</span></code> can be searched with <a class="reference external" href="https://github.com/sgl-project/sglang/blob/main/scripts/playground/bench_speculative.py">bench_speculative.py</a> script for given batch size. The minimum configuration is <code class="docutils literal notranslate"><span class="pre">--speculative-num-steps</span> <span class="pre">1</span> <span class="pre">--speculative-eagle-topk</span> <span class="pre">1</span> <span class="pre">--speculative-num-draft-tokens</span> <span class="pre">2</span></code>, which can achieve speedup for larger batch sizes.</p></li>
<li><p>The default value of  <code class="docutils literal notranslate"><span class="pre">--max-running-requests</span></code> is set to <code class="docutils literal notranslate"><span class="pre">48</span></code> for MTP. For larger batch sizes, this value should be increased beyond the default value.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To enable the experimental overlap scheduler for EAGLE speculative decoding, set the environment variable <code class="docutils literal notranslate"><span class="pre">SGLANG_ENABLE_SPEC_V2=1</span></code>. This can improve performance by enabling overlap scheduling between draft and verification stages.</p>
</div>
</section>
<section id="function-calling-and-reasoning-parser">
<h2>Function Calling and Reasoning Parser<a class="headerlink" href="#function-calling-and-reasoning-parser" title="Link to this heading">#</a></h2>
<p>The usage of function calling and reasoning parser is the same as DeepSeek V3.1. Please refer to <a class="reference external" href="https://docs.sglang.io/advanced_features/separate_reasoning.html">Reasoning Parser</a> and <a class="reference external" href="https://docs.sglang.io/advanced_features/tool_parser.html">Tool Parser</a> documents.</p>
<p>To launch <code class="docutils literal notranslate"><span class="pre">DeepSeek-V3.2-Exp</span></code> with function calling and reasoning parser:</p>
<blockquote>
<div><p>Note: It is recommended to specify the chat-template, ensuring that you are within the sglang’s root directory.</p>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model-path<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tp-size<span class="w"> </span><span class="m">8</span><span class="w"> </span>--dp-size<span class="w"> </span><span class="m">8</span><span class="w"> </span>--enable-dp-attention<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tool-call-parser<span class="w"> </span>deepseekv31<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--reasoning-parser<span class="w"> </span>deepseek-v3<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--chat-template<span class="w"> </span>./examples/chat_template/tool_chat_template_deepseekv32.jinja
</pre></div>
</div>
<p>To launch <code class="docutils literal notranslate"><span class="pre">DeepSeek-V3.2</span></code> with function calling and reasoning parser:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model-path<span class="w"> </span>deepseek-ai/DeepSeek-V3.2<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tp-size<span class="w"> </span><span class="m">8</span><span class="w"> </span>--dp-size<span class="w"> </span><span class="m">8</span><span class="w"> </span>--enable-dp-attention<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tool-call-parser<span class="w"> </span>deepseekv32<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--reasoning-parser<span class="w"> </span>deepseek-v3
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">DeepSeek-V3.2-Speciale</span></code> doesn’t support tool calling, so can only be launched with reasoning parser:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model-path<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Speciale<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tp-size<span class="w"> </span><span class="m">8</span><span class="w"> </span>--dp-size<span class="w"> </span><span class="m">8</span><span class="w"> </span>--enable-dp-attention<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--reasoning-parser<span class="w"> </span>deepseek-v3
</pre></div>
</div>
</section>
<section id="nvfp4-checkpoint">
<h2>NVFP4 Checkpoint<a class="headerlink" href="#nvfp4-checkpoint" title="Link to this heading">#</a></h2>
<p>To launch deepseek v3.2 <a class="reference external" href="https://huggingface.co/nvidia/DeepSeek-V3.2-NVFP4">NVFP4 checkpoint</a> on Blackwell devices, the user needs to specify the quantization method as <code class="docutils literal notranslate"><span class="pre">modelopt_fp4</span></code>, and moe runner backend as one of <code class="docutils literal notranslate"><span class="pre">flashinfer_trtllm</span></code>(recommended), <code class="docutils literal notranslate"><span class="pre">flashinfer_cutlass</span></code> and <code class="docutils literal notranslate"><span class="pre">flashinfer_cutedsl</span></code>. Any other usage (parallelism, reasoning parser, …) is the same as FP8 checkpoint.</p>
<p>An example launching command can be:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model<span class="w"> </span>nvidia/DeepSeek-V3.2-NVFP4<span class="w"> </span>--tp<span class="w"> </span><span class="m">4</span><span class="w"> </span>--quantization<span class="w"> </span>modelopt_fp4<span class="w"> </span>--moe-runner-backend<span class="w"> </span>flashinfer_trtllm<span class="w"> </span>--tool-call-parser<span class="w"> </span>deepseekv32<span class="w">  </span>--reasoning-parser<span class="w"> </span>deepseek-v3
</pre></div>
</div>
</section>
<section id="pd-disaggregation">
<h2>PD Disaggregation<a class="headerlink" href="#pd-disaggregation" title="Link to this heading">#</a></h2>
<p>Prefill Command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model-path<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--disaggregation-mode<span class="w"> </span>prefill<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--host<span class="w"> </span><span class="nv">$LOCAL_IP</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--port<span class="w"> </span><span class="nv">$PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--tp<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dp<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--enable-dp-attention<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dist-init-addr<span class="w"> </span><span class="si">${</span><span class="nv">HOST</span><span class="si">}</span>:<span class="si">${</span><span class="nv">DIST_PORT</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--disaggregation-bootstrap-port<span class="w"> </span><span class="m">8998</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.9<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>Decode command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model-path<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--disaggregation-mode<span class="w"> </span>decode<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--host<span class="w"> </span><span class="nv">$LOCAL_IP</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--port<span class="w"> </span><span class="nv">$PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--tp<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dp<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--enable-dp-attention<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dist-init-addr<span class="w"> </span><span class="si">${</span><span class="nv">HOST</span><span class="si">}</span>:<span class="si">${</span><span class="nv">DIST_PORT</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.9<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>Router command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang_router.launch_router<span class="w"> </span>--pd-disaggregation<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--prefill<span class="w"> </span><span class="nv">$PREFILL_ADDR</span><span class="w"> </span><span class="m">8998</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--decode<span class="w"> </span><span class="nv">$DECODE_ADDR</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--host<span class="w"> </span><span class="m">127</span>.0.0.1<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>If you need more advanced deployment methods or production-ready deployment methods, such as RBG or LWS-based deployment, please refer to <a class="reference internal" href="../references/multi_node_deployment/rbg_pd/deepseekv32_pd.html"><span class="std std-doc">references/multi_node_deployment/rbg_pd/deepseekv32_pd.md</span></a>. Additionally, you can also find startup commands for DeepEP-based EP parallelism in the aforementioned documentation.</p>
</section>
<section id="benchmarking-results">
<h2>Benchmarking Results<a class="headerlink" href="#benchmarking-results" title="Link to this heading">#</a></h2>
<section id="accuracy-test-with-gsm8k">
<h3>Accuracy Test with <code class="docutils literal notranslate"><span class="pre">gsm8k</span></code><a class="headerlink" href="#accuracy-test-with-gsm8k" title="Link to this heading">#</a></h3>
<p>A simple accuracy benchmark can be tested with <code class="docutils literal notranslate"><span class="pre">gsm8k</span></code> dataset:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>benchmark/gsm8k/bench_sglang.py<span class="w"> </span>--num-shots<span class="w"> </span><span class="m">8</span><span class="w"> </span>--num-questions<span class="w"> </span><span class="m">1319</span><span class="w"> </span>--parallel<span class="w"> </span><span class="m">1319</span>
</pre></div>
</div>
<p>The result is 0.956, which matches our expectation:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Accuracy:<span class="w"> </span><span class="m">0</span>.956
Invalid:<span class="w"> </span><span class="m">0</span>.000
Latency:<span class="w"> </span><span class="m">25</span>.109<span class="w"> </span>s
Output<span class="w"> </span>throughput:<span class="w"> </span><span class="m">5226</span>.235<span class="w"> </span>token/s
</pre></div>
</div>
<p>To test long-context accuracy, run gsm8k with <code class="docutils literal notranslate"><span class="pre">--num-shots</span> <span class="pre">20</span></code>. The results are very close to the 8 shots results:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Accuracy</span><span class="p">:</span> <span class="mf">0.956</span>
<span class="n">Invalid</span><span class="p">:</span> <span class="mf">0.000</span>
<span class="n">Latency</span><span class="p">:</span> <span class="mf">29.545</span> <span class="n">s</span>
<span class="n">Output</span> <span class="n">throughput</span><span class="p">:</span> <span class="mf">4418.617</span> <span class="n">token</span><span class="o">/</span><span class="n">s</span>
</pre></div>
</div>
</section>
<section id="accuracy-test-with-gpqa-diamond">
<h3>Accuracy Test with <code class="docutils literal notranslate"><span class="pre">gpqa-diamond</span></code><a class="headerlink" href="#accuracy-test-with-gpqa-diamond" title="Link to this heading">#</a></h3>
<p>Accuracy benchmark on long context can be tested on GPQA-diamond dataset with long output tokens and thinking enabled:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.test.run_eval<span class="w"> </span>--port<span class="w"> </span><span class="m">30000</span><span class="w"> </span>--eval-name<span class="w"> </span>gpqa<span class="w"> </span>--num-examples<span class="w"> </span><span class="m">198</span><span class="w"> </span>--max-tokens<span class="w"> </span><span class="m">128000</span><span class="w"> </span>--repeat<span class="w"> </span><span class="m">8</span><span class="w"> </span>--thinking-mode<span class="w"> </span>deepseek-v3
</pre></div>
</div>
<p>The mean accuracy over 8 runs shows 0.797, which matches the number 0.799 in official tech report.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Repeat:<span class="w"> </span><span class="m">8</span>,<span class="w"> </span>mean:<span class="w"> </span><span class="m">0</span>.797
Scores:<span class="w"> </span><span class="o">[</span><span class="s1">&#39;0.808&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.798&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.808&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.798&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.783&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.788&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.803&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.793&#39;</span><span class="o">]</span>
</pre></div>
</div>
<p>For Deepseek V3.2, Deepseek recommends setting the sampling parameters to temperature = 1.0, top_p = 0.95:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.test.run_eval<span class="w"> </span>--port<span class="w"> </span><span class="m">30000</span><span class="w"> </span>--eval-name<span class="w"> </span>gpqa<span class="w"> </span>--num-examples<span class="w"> </span><span class="m">198</span><span class="w"> </span>--max-tokens<span class="w"> </span><span class="m">128000</span><span class="w"> </span>--repeat<span class="w"> </span><span class="m">8</span><span class="w"> </span>--top-p<span class="w"> </span><span class="m">0</span>.95<span class="w"> </span>--temperature<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span>--thinking-mode<span class="w"> </span>deepseek-v3

Repeat:<span class="w"> </span><span class="m">8</span>,<span class="w"> </span>mean:<span class="w"> </span><span class="m">0</span>.840
Scores:<span class="w"> </span><span class="o">[</span><span class="s1">&#39;0.848&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.808&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.848&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.838&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.879&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.813&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.838&#39;</span>,<span class="w"> </span><span class="s1">&#39;0.848&#39;</span><span class="o">]</span>
</pre></div>
</div>
<p>which matches the official score, 0.824, as reported in the <a class="reference external" href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf">Deepseek-V3.2 technical report</a>.</p>
</section>
<section id="accuracy-test-with-aime-2025">
<h3>Accuracy Test with <code class="docutils literal notranslate"><span class="pre">aime</span> <span class="pre">2025</span></code><a class="headerlink" href="#accuracy-test-with-aime-2025" title="Link to this heading">#</a></h3>
<p>Prepare the environment by installing NeMo-Skills in the docker or your own virtual environment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">NVIDIA</span><span class="o">/</span><span class="n">NeMo</span><span class="o">-</span><span class="n">Skills</span><span class="o">.</span><span class="n">git</span> <span class="o">--</span><span class="n">ignore</span><span class="o">-</span><span class="n">installed</span> <span class="n">blinker</span>
</pre></div>
</div>
<p>Then launch the SGLang server:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">sglang</span><span class="o">.</span><span class="n">launch_server</span> <span class="o">--</span><span class="n">model</span> <span class="n">deepseek</span><span class="o">-</span><span class="n">ai</span><span class="o">/</span><span class="n">DeepSeek</span><span class="o">-</span><span class="n">V3</span><span class="mf">.2</span><span class="o">-</span><span class="n">Exp</span> <span class="o">--</span><span class="n">tp</span> <span class="mi">8</span> <span class="o">--</span><span class="n">dp</span> <span class="mi">8</span> <span class="o">--</span><span class="n">enable</span><span class="o">-</span><span class="n">dp</span><span class="o">-</span><span class="n">attention</span>
</pre></div>
</div>
<p><strong>For <code class="docutils literal notranslate"><span class="pre">DeepSeek-V3.2</span></code> and <code class="docutils literal notranslate"><span class="pre">DeepSeek-V3.2-Speciale</span></code></strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">sglang</span><span class="o">.</span><span class="n">launch_server</span>   <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="n">path</span> <span class="n">deepseek</span><span class="o">-</span><span class="n">ai</span><span class="o">/</span><span class="n">DeepSeek</span><span class="o">-</span><span class="n">V3</span><span class="mf">.2</span>   <span class="o">--</span><span class="n">trust</span><span class="o">-</span><span class="n">remote</span><span class="o">-</span><span class="n">code</span>   <span class="o">--</span><span class="n">tp</span><span class="o">-</span><span class="n">size</span> <span class="mi">8</span> <span class="o">--</span><span class="n">dp</span><span class="o">-</span><span class="n">size</span> <span class="mi">8</span> <span class="o">--</span><span class="n">enable</span><span class="o">-</span><span class="n">dp</span><span class="o">-</span><span class="n">attention</span>   <span class="o">--</span><span class="n">tool</span><span class="o">-</span><span class="n">call</span><span class="o">-</span><span class="n">parser</span> <span class="n">deepseekv32</span>   <span class="o">--</span><span class="n">reasoning</span><span class="o">-</span><span class="n">parser</span> <span class="n">deepseek</span><span class="o">-</span><span class="n">v3</span>
</pre></div>
</div>
<p>Run the following script to evaluate AIME 2025:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#! /bin/bash
export NEMO_SKILLS_DISABLE_UNCOMMITTED_CHANGES_CHECK=1

ns prepare_data aime25

PORT=30000
BACKEND=sglang
MODEL=&quot;deepseek-ai/DeepSeek-V3.2-Exp&quot; # Should be changed to the model name
MODEL_NAME=&quot;dsv32-fp8&quot;

echo &quot;Starting AIME25 evaluation with model $MODEL on port $PORT using backend $BACKEND...&quot;
ns eval \
  --benchmarks=aime25:4 \
  --server_type=$BACKEND \
  --model=$MODEL \
  --server_address=http://localhost:${PORT}/v1 \
  --output_dir=nemo_skills_aime25_${MODEL_NAME}_output_${BACKEND}_$(date +%Y%m%d_%H%M%S) \
  ++chat_template_kwargs.thinking=true \
  ++inference.temperature=1.0 \
  ++inference.top_p=0.95 \
  ++inference.tokens_to_generate=64000
  # ++inference.tokens_to_generate=120000 for Speciale model
</pre></div>
</div>
<p>Test results (8*B200):</p>
<p>DeepSeek-V3.2-Exp：</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>evaluation_mode</p></th>
<th class="head"><p>num_entries</p></th>
<th class="head"><p>avg_tokens</p></th>
<th class="head"><p>gen_seconds</p></th>
<th class="head"><p>symbolic_correct</p></th>
<th class="head"><p>no_answer</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>pass&#64;1[avg-of-4]</p></td>
<td><p>30</p></td>
<td><p>15040</p></td>
<td><p>1673</p></td>
<td><p>87.50% ± 1.67%</p></td>
<td><p>0.00%</p></td>
</tr>
<tr class="row-odd"><td><p>majority&#64;4</p></td>
<td><p>30</p></td>
<td><p>15040</p></td>
<td><p>1673</p></td>
<td><p>90.00%</p></td>
<td><p>0.00%</p></td>
</tr>
<tr class="row-even"><td><p>pass&#64;4</p></td>
<td><p>30</p></td>
<td><p>15040</p></td>
<td><p>1673</p></td>
<td><p>90.00%</p></td>
<td><p>0.00%</p></td>
</tr>
</tbody>
</table>
</div>
<p>DeepSeek-V3.2:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>evaluation_mode</p></th>
<th class="head"><p>num_entries</p></th>
<th class="head"><p>avg_tokens</p></th>
<th class="head"><p>gen_seconds</p></th>
<th class="head"><p>symbolic_correct</p></th>
<th class="head"><p>no_answer</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>pass&#64;1[avg-of-4]</p></td>
<td><p>30</p></td>
<td><p>13550</p></td>
<td><p>1632</p></td>
<td><p>92.50% ± 1.67%</p></td>
<td><p>0.00%</p></td>
</tr>
<tr class="row-odd"><td><p>majority&#64;4</p></td>
<td><p>30</p></td>
<td><p>13550</p></td>
<td><p>1632</p></td>
<td><p>94.71%</p></td>
<td><p>0.00%</p></td>
</tr>
<tr class="row-even"><td><p>pass&#64;4</p></td>
<td><p>30</p></td>
<td><p>13550</p></td>
<td><p>1632</p></td>
<td><p>96.67%</p></td>
<td><p>0.00%</p></td>
</tr>
</tbody>
</table>
</div>
<p>DeepSeek-V3.2-Speciale:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>evaluation_mode</p></th>
<th class="head"><p>num_entries</p></th>
<th class="head"><p>avg_tokens</p></th>
<th class="head"><p>gen_seconds</p></th>
<th class="head"><p>symbolic_correct</p></th>
<th class="head"><p>no_answer</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>pass&#64;1[avg-of-4]</p></td>
<td><p>30</p></td>
<td><p>24155</p></td>
<td><p>3583</p></td>
<td><p>95.00% ± 1.92%</p></td>
<td><p>0.00%</p></td>
</tr>
<tr class="row-odd"><td><p>majority&#64;4</p></td>
<td><p>30</p></td>
<td><p>24155</p></td>
<td><p>3583</p></td>
<td><p>95.83%</p></td>
<td><p>0.00%</p></td>
</tr>
<tr class="row-even"><td><p>pass&#64;4</p></td>
<td><p>30</p></td>
<td><p>24155</p></td>
<td><p>3583</p></td>
<td><p>100.00%</p></td>
<td><p>0.00%</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="dsa-long-sequence-context-parallel-optimization-experimental">
<h2>DSA long sequence context parallel optimization(experimental)<a class="headerlink" href="#dsa-long-sequence-context-parallel-optimization-experimental" title="Link to this heading">#</a></h2>
<p><strong>Note: This feature is only verified on Hopper machines</strong></p>
<p>For context parallel in DeepSeek V3.2 model, we provide two different modes of splitting tokens, which can be controlled with argument <code class="docutils literal notranslate"><span class="pre">--nsa-prefill-cp-mode</span></code>.</p>
<section id="in-sequence-splitting">
<h3>In sequence splitting<a class="headerlink" href="#in-sequence-splitting" title="Link to this heading">#</a></h3>
<p>The first mode can be enabled by <code class="docutils literal notranslate"><span class="pre">--nsa-prefill-cp-mode</span> <span class="pre">in-seq-split</span></code>. This mode implements context parallel for DSA by splitting the sequence uniformly between context parallel ranks. At attention stage, each cp rank computes the indexer results of sharded sequence, and collects the whole kv cache through all gather operator.</p>
<p>The communication group for context parallel reuses the one for attention tp, thus <code class="docutils literal notranslate"><span class="pre">cp_size</span></code> equals <code class="docutils literal notranslate"><span class="pre">atten_tp_size</span> <span class="pre">=</span> <span class="pre">tp_size</span> <span class="pre">/</span> <span class="pre">dp_size</span></code>.</p>
<p>Note that in sequence splitting mode has the following restrictions:</p>
<ul class="simple">
<li><p>The batch size is restricted to 1 for prefill batches</p></li>
<li><p>Multi-node/PD disaggregation is still not supported</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">moe_dense_tp_size=1</span></code>, <code class="docutils literal notranslate"><span class="pre">kv_cache_dtype</span> <span class="pre">=</span> <span class="pre">&quot;bf16&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">moe_a2a_backend</span> <span class="pre">=</span> <span class="pre">&quot;deepep&quot;</span></code></p></li>
<li><p>To ensure <code class="docutils literal notranslate"><span class="pre">cp_size</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, the passed in <code class="docutils literal notranslate"><span class="pre">tp_size</span></code> must be larger than <code class="docutils literal notranslate"><span class="pre">dp_size</span></code></p></li>
</ul>
<p>For more details, please refer to PR https://github.com/sgl-project/sglang/pull/12065.</p>
<p>Example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># In-seq splitting mode launched with EP + DP</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w">  </span>--tp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--ep<span class="w"> </span><span class="m">8</span><span class="w"> </span>--dp<span class="w"> </span><span class="m">2</span><span class="w"> </span>--enable-dp-attention<span class="w"> </span>--enable-nsa-prefill-context-parallel<span class="w"> </span>--nsa-prefill-cp-mode<span class="w"> </span><span class="k">in</span>-seq-split<span class="w"> </span>--max-running-requests<span class="w"> </span><span class="m">32</span>
</pre></div>
</div>
</section>
<section id="round-robin-splitting-default-setting">
<h3>Round robin splitting (default setting)<a class="headerlink" href="#round-robin-splitting-default-setting" title="Link to this heading">#</a></h3>
<p>This mode can be enabled by specifying the parameter <code class="docutils literal notranslate"><span class="pre">--nsa-prefill-cp-mode</span> <span class="pre">round-robin-split</span></code>, which distributes tokens across ranks based on <code class="docutils literal notranslate"><span class="pre">token_idx</span> <span class="pre">%</span> <span class="pre">cp_size</span></code>.</p>
<p>In this scenario, compared with the aforementioned method, it additionally supports the fused MoE backend (the fused MoE backend may deliver better performance than DeepEP in single-machine scenarios), FP8 KV-cache, and multi-batch prefill inference. But it cannot be enabled with dp attention together.</p>
<p>For more details, please refer to PR https://github.com/sgl-project/sglang/pull/13959.</p>
<p>Example usage:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Launch with FusedMoe + CP8</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w">  </span>--tp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--enable-nsa-prefill-context-parallel<span class="w"> </span>--nsa-prefill-cp-mode<span class="w"> </span>round-robin-split<span class="w"> </span>--max-running-requests<span class="w"> </span><span class="m">32</span>
</pre></div>
</div>
</section>
<section id="pipeline-parallel-context-parallel-pp-cp">
<h3>Pipeline Parallel + Context Parallel (PP + CP)<a class="headerlink" href="#pipeline-parallel-context-parallel-pp-cp" title="Link to this heading">#</a></h3>
<p>This mode combines Pipeline Parallelism (PP) and Context Parallelism (CP) to scale across multiple nodes, which can achieve better throughput and Time To First Token (TTFT). Note that this method has only been tested on H20 96G.</p>
<section id="standard-usage">
<h4>Standard Usage<a class="headerlink" href="#standard-usage" title="Link to this heading">#</a></h4>
<p>To launch with PP=2 and CP (via <code class="docutils literal notranslate"><span class="pre">round-robin-split</span></code> mode) on 2 nodes. This configuration uses the fused MoE kernel by default, which generally provides better performance.</p>
<p>For related development details, please refer to:</p>
<ul class="simple">
<li><p>Fused MoE + CP support: <a class="reference external" href="https://github.com/sgl-project/sglang/pull/13959">PR #13959</a></p></li>
<li><p>PP + CP support: <a class="reference external" href="https://github.com/sgl-project/sglang/issues/15358">Issue #15358</a> and <a class="reference external" href="https://github.com/sgl-project/sglang/pull/16380">PR #16380</a></p></li>
</ul>
<p>Node 0:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SGLANG_PP_LAYER_PARTITION</span><span class="o">=</span><span class="m">30</span>,31
python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model-path<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--nnodes<span class="w"> </span><span class="m">2</span><span class="w"> </span>--node-rank<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dist-init-addr<span class="w"> </span>&lt;HEAD_NODE_IP&gt;:62001<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--pp-size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dp-size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--moe-dense-tp-size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--enable-nsa-prefill-context-parallel<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--nsa-prefill-cp-mode<span class="w"> </span>round-robin-split<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--disable-radix-cache<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.8<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--max-running-requests<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--chunked-prefill-size<span class="w"> </span><span class="m">16384</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--cuda-graph-max-bs<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--page-size<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--watchdog-timeout<span class="w"> </span><span class="m">3600</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tool-call-parser<span class="w"> </span>deepseekv32
</pre></div>
</div>
<p>Node 1:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SGLANG_PP_LAYER_PARTITION</span><span class="o">=</span><span class="m">30</span>,31
python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model-path<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--nnodes<span class="w"> </span><span class="m">2</span><span class="w"> </span>--node-rank<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dist-init-addr<span class="w"> </span>&lt;HEAD_NODE_IP&gt;:62001<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--pp-size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dp-size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--moe-dense-tp-size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--enable-nsa-prefill-context-parallel<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--nsa-prefill-cp-mode<span class="w"> </span>round-robin-split<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--disable-radix-cache<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.8<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--max-running-requests<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--chunked-prefill-size<span class="w"> </span><span class="m">16384</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--cuda-graph-max-bs<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--page-size<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--watchdog-timeout<span class="w"> </span><span class="m">3600</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tool-call-parser<span class="w"> </span>deepseekv32
</pre></div>
</div>
</section>
<section id="pd-disaggregation-with-pp-cp">
<h4>PD Disaggregation with PP + CP<a class="headerlink" href="#pd-disaggregation-with-pp-cp" title="Link to this heading">#</a></h4>
<p>If using PD (Prefill-Decode) Disaggregation, the Prefill nodes can be configured with PP + CP as follows.</p>
<p>Prefill Node 0:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model-path<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--served-model-name<span class="w"> </span>deepseek-v32<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--nnodes<span class="w"> </span><span class="m">2</span><span class="w"> </span>--node-rank<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dist-init-addr<span class="w"> </span>&lt;PREFILL_HEAD_IP&gt;:20102<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--pp-size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dp-size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--moe-dense-tp-size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--enable-nsa-prefill-context-parallel<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--nsa-prefill-cp-mode<span class="w"> </span>round-robin-split<span class="w">  </span><span class="se">\</span>
<span class="w">  </span>--disaggregation-ib-device<span class="w"> </span>mlx5_bond_0,mlx5_bond_1,mlx5_bond_2,mlx5_bond_3<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--disable-radix-cache<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--max-running-requests<span class="w"> </span><span class="m">512</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--chunked-prefill-size<span class="w"> </span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--context-length<span class="w"> </span><span class="m">131072</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.9<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--page-size<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--enable-metrics<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--collect-tokens-histogram<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tokenizer-worker-num<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">30000</span>
</pre></div>
</div>
<p>Prefill Node 1:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model-path<span class="w"> </span>deepseek-ai/DeepSeek-V3.2-Exp<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--served-model-name<span class="w"> </span>deepseek-v32-prefill<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--nnodes<span class="w"> </span><span class="m">2</span><span class="w"> </span>--node-rank<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dist-init-addr<span class="w"> </span>&lt;PREFILL_HEAD_IP&gt;:20102<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tp<span class="w"> </span><span class="m">8</span><span class="w"> </span>--pp-size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dp-size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--moe-dense-tp-size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--enable-nsa-prefill-context-parallel<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--nsa-prefill-cp-mode<span class="w"> </span>round-robin-split<span class="w">  </span><span class="se">\</span>
<span class="w">  </span>--disaggregation-ib-device<span class="w"> </span>mlx5_bond_0,mlx5_bond_1,mlx5_bond_2,mlx5_bond_3<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--disable-radix-cache<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--max-running-requests<span class="w"> </span><span class="m">512</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--chunked-prefill-size<span class="w"> </span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--context-length<span class="w"> </span><span class="m">131072</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.9<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--page-size<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--enable-metrics<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--collect-tokens-histogram<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tokenizer-worker-num<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">30000</span>
</pre></div>
</div>
<p>For the Decode nodes, it is recommended to use the <strong>EP mode</strong>.</p>
</section>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="deepseek_v3.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">DeepSeek V3/V3.1/R1 Usage</p>
      </div>
    </a>
    <a class="right-next"
       href="glm45.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Launch GLM-4.5 / GLM-4.6 / GLM-4.7 with SGLang</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installation">Installation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#docker">Docker</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-from-source">Build From Source</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#launch-deepseek-v3-2-with-sglang">Launch DeepSeek V3.2 with SGLang</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration-tips">Configuration Tips</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-token-prediction">Multi-token Prediction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#function-calling-and-reasoning-parser">Function Calling and Reasoning Parser</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nvfp4-checkpoint">NVFP4 Checkpoint</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pd-disaggregation">PD Disaggregation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking-results">Benchmarking Results</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-test-with-gsm8k">Accuracy Test with <code class="docutils literal notranslate"><span class="pre">gsm8k</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-test-with-gpqa-diamond">Accuracy Test with <code class="docutils literal notranslate"><span class="pre">gpqa-diamond</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-test-with-aime-2025">Accuracy Test with <code class="docutils literal notranslate"><span class="pre">aime</span> <span class="pre">2025</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dsa-long-sequence-context-parallel-optimization-experimental">DSA long sequence context parallel optimization(experimental)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#in-sequence-splitting">In sequence splitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#round-robin-splitting-default-setting">Round robin splitting (default setting)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pipeline-parallel-context-parallel-pp-cp">Pipeline Parallel + Context Parallel (PP + CP)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-usage">Standard Usage</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pd-disaggregation-with-pp-cp">PD Disaggregation with PP + CP</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By SGLang Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023-2026, SGLang.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Feb 12, 2026.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  
    <!-- RunLLM Widget Script -->
    <script type="module" id="runllm-widget-script" src="https://widget.runllm.com" crossorigin="true" version="stable" runllm-keyboard-shortcut="Mod+j" runllm-name="SGLang Chatbot" runllm-position="BOTTOM_RIGHT" runllm-assistant-id="629" async></script>
    
</body>
</html>