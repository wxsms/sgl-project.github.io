
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Benchmark and Profiling &#8212; SGLang</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=a58fb4e2"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=ccdb6887"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'developer_guide/benchmark_and_profiling';</script>
    <link rel="icon" href="../_static/logo.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bench Serving Guide" href="bench_serving.html" />
    <link rel="prev" title="Development Guide Using Docker" href="development_guide_using_docker.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Jan 27, 2026"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="SGLang - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="SGLang - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../get_started/install.html">Install SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Basic Usage</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/send_request.html">Sending Requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/openai_api.html">OpenAI-Compatible APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/ollama_api.html">Ollama-Compatible API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/offline_engine_api.html">Offline Engine API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/native_api.html">SGLang Native APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/sampling_params.html">Sampling Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/popular_model_usage.html">Popular Model Usage (DeepSeek, GPT-OSS, GLM, Llama, MiniMax, Qwen, and more)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/server_arguments.html">Server Arguments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/hyperparameter_tuning.html">Hyperparameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/attention_backend.html">Attention Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/speculative_decoding.html">Speculative Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/structured_outputs.html">Structured Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/structured_outputs_for_reasoning_models.html">Structured Outputs For Reasoning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/tool_parser.html">Tool Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/separate_reasoning.html">Reasoning Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/quantized_kv_cache.html">Quantized KV Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/expert_parallelism.html">Expert Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/lora.html">LoRA Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/pd_disaggregation.html">PD Disaggregation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/epd_disaggregation.html">EPD Disaggregation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/pipeline_parallelism.html">Pipeline Parallelism for Long Context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/hicache.html">Hierarchical KV Caching (HiCache)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/vlm_query.html">Query VLM with Offline Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/dp_for_multi_modal_encoder.html">DP for Multi-Modal Encoder in SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/cuda_graph_for_multi_modal_encoder.html">Cuda Graph for Multi-Modal Encoder in SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/sgl_model_gateway.html">SGLang Model Gateway</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/deterministic_inference.html">Deterministic Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/observability.html">Observability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/checkpoint_engine.html">Checkpoint Engine Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_features/sglang_for_rl.html">SGLang for RL Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supported Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../supported_models/generative_models.html">Large Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/multimodal_language_models.html">Multimodal Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/diffusion_language_models.html">Diffusion Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/diffusion_models.html">Diffusion Models</a></li>










<li class="toctree-l1"><a class="reference internal" href="../supported_models/embedding_models.html">Embedding Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/reward_models.html">Reward Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/rerank_models.html">Rerank Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/classify_models.html">Classification API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/support_new_models.html">How to Support New Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/transformers_fallback.html">Transformers fallback in SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/modelscope.html">Use Models From ModelScope</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/mindspore_models.html">MindSpore Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Hardware Platforms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../platforms/amd_gpu.html">AMD GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/cpu_server.html">CPU Servers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/tpu.html">TPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/nvidia_jetson.html">NVIDIA Jetson Orin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/ascend_npu_support.html">Ascend NPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/xpu.html">XPU</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="contribution_guide.html">Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="development_guide_using_docker.html">Development Guide Using Docker</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Benchmark and Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="bench_serving.html">Bench Serving Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluating_new_models.html">Evaluating New Models with SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../references/faq.html">Troubleshooting and Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/environment_variables.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/production_metrics.html">Production Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/production_request_trace.html">Production Request Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/multi_node_deployment/multi_node_index.html">Multi-Node Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/custom_chat_template.html">Custom Chat Template</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/frontend/frontend_index.html">Frontend Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/post_training_integration.html">Post-Training Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/learn_more.html">Learn More and Join the Community</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/blob/main/developer_guide/benchmark_and_profiling.md?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/edit/main/developer_guide/benchmark_and_profiling.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/issues/new?title=Issue%20on%20page%20%2Fdeveloper_guide/benchmark_and_profiling.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/developer_guide/benchmark_and_profiling.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Benchmark and Profiling</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark">Benchmark</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-with-pytorch-profiler">Profile with PyTorch Profiler</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-a-server-with-sglang-bench-serving">Profile a server with <code class="docutils literal notranslate"><span class="pre">sglang.bench_serving</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-in-pd-disaggregation-mode">Profile In PD Disaggregation Mode</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-prefill-workers">Profile Prefill Workers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-decode-workers">Profile Decode Workers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#important-notes">Important Notes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-a-server-with-sglang-bench-offline-throughput">Profile a server with <code class="docutils literal notranslate"><span class="pre">sglang.bench_offline_throughput</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-a-server-with-sglang-profiler">Profile a server with <code class="docutils literal notranslate"><span class="pre">sglang.profiler</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-a-server-with-http-api-endpoints">Profile a server with HTTP API endpoints</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-start-profile-endpoint">Using <code class="docutils literal notranslate"><span class="pre">/start_profile</span></code> endpoint</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-end-profile-endpoint">Using <code class="docutils literal notranslate"><span class="pre">/end_profile</span></code> endpoint</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-workflow">Example workflow</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profiler-trace-merger-for-distributed-traces">Profiler Trace Merger for Distributed Traces</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-profiling-and-shared-storage-considerations">Multi-Node Profiling and Shared Storage Considerations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#http-api-usage">HTTP API Usage</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#command-line-usage">Command Line Usage</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output-files">Output Files</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#possible-pytorch-bugs">Possible PyTorch bugs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#view-traces">View traces</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-with-nsight">Profile with Nsight</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-wise-nvtx-profiling-with-nsight-systems">Layer-wise NVTX Profiling with Nsight Systems</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-enable-layerwise-nvtx-marker-with-nsight-systems-and-start-profile">Using <code class="docutils literal notranslate"><span class="pre">--enable-layerwise-nvtx-marker</span></code> with Nsight Systems and <code class="docutils literal notranslate"><span class="pre">/start_profile</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-tips">Other tips</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="benchmark-and-profiling">
<h1>Benchmark and Profiling<a class="headerlink" href="#benchmark-and-profiling" title="Link to this heading">#</a></h1>
<section id="benchmark">
<h2>Benchmark<a class="headerlink" href="#benchmark" title="Link to this heading">#</a></h2>
<ul>
<li><p>Benchmark the latency of running a single static batch without a server. The arguments are the same as for <code class="docutils literal notranslate"><span class="pre">launch_server.py</span></code>.
Note that this is a simplified test script without a dynamic batching server, so it may run out of memory for a batch size that a real server can handle. A real server truncates the prefill into several batches, while this simplified script does not.</p>
<ul>
<li><p>Without a server (do not need to launch a server)</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_one_batch<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Meta-Llama-3.1-8B-Instruct<span class="w"> </span>--batch<span class="w"> </span><span class="m">32</span><span class="w"> </span>--input-len<span class="w"> </span><span class="m">256</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">32</span>
</pre></div>
</div>
</li>
<li><p>With a server (please use <code class="docutils literal notranslate"><span class="pre">sglang.launch_server</span></code> to launch a server first and run the following command.)</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_one_batch_server<span class="w"> </span>--base-url<span class="w"> </span>http://127.0.0.1:30000<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Meta-Llama-3.1-8B-Instruct<span class="w"> </span>--batch-size<span class="w"> </span><span class="m">32</span><span class="w"> </span>--input-len<span class="w"> </span><span class="m">256</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">32</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Benchmark offline processing. This script will start an offline engine and run the benchmark.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.bench_offline_throughput<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Meta-Llama-3.1-8B-Instruct<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">10</span>
</pre></div>
</div>
</li>
<li><p>Benchmark online serving. Please use <code class="docutils literal notranslate"><span class="pre">sglang.launch_server</span></code> to launch a server first and run the following command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.bench_serving<span class="w"> </span>--backend<span class="w"> </span>sglang<span class="w"> </span>--num-prompt<span class="w"> </span><span class="m">10</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="profile-with-pytorch-profiler">
<h2>Profile with PyTorch Profiler<a class="headerlink" href="#profile-with-pytorch-profiler" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html">Pytorch Profiler</a> is a convenient basic tool to inspect kernel execution time, call stack, and kernel overlap and occupancy.</p>
<section id="profile-a-server-with-sglang-bench-serving">
<h3>Profile a server with <code class="docutils literal notranslate"><span class="pre">sglang.bench_serving</span></code><a class="headerlink" href="#profile-a-server-with-sglang-bench-serving" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># set trace path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SGLANG_TORCH_PROFILER_DIR</span><span class="o">=</span>/root/sglang/profile_log

<span class="c1"># start server</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct

<span class="c1"># send profiling request from client</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_serving<span class="w"> </span>--backend<span class="w"> </span>sglang<span class="w"> </span>--model<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">10</span><span class="w"> </span>--sharegpt-output-len<span class="w"> </span><span class="m">100</span><span class="w"> </span>--profile
</pre></div>
</div>
<p>Please make sure that the <code class="docutils literal notranslate"><span class="pre">SGLANG_TORCH_PROFILER_DIR</span></code> should be set at both server and client side, otherwise the trace file cannot be generated correctly . A secure way will be setting <code class="docutils literal notranslate"><span class="pre">SGLANG_TORCH_PROFILER_DIR</span></code> in the <code class="docutils literal notranslate"><span class="pre">.*rc</span></code> file of shell (e.g. <code class="docutils literal notranslate"><span class="pre">~/.bashrc</span></code> for bash shells).</p>
<p>For more details, please refer to <a class="reference internal" href="bench_serving.html"><span class="std std-doc">Bench Serving Guide</span></a>.</p>
</section>
<section id="profile-in-pd-disaggregation-mode">
<h3>Profile In PD Disaggregation Mode<a class="headerlink" href="#profile-in-pd-disaggregation-mode" title="Link to this heading">#</a></h3>
<p>When profiling in PD disaggregation mode, prefill and decode workers <strong>must be profiled separately</strong> due to torch profiler limitations. The <code class="docutils literal notranslate"><span class="pre">bench_serving</span></code> command provides dedicated options for this:</p>
<section id="profile-prefill-workers">
<h4>Profile Prefill Workers<a class="headerlink" href="#profile-prefill-workers" title="Link to this heading">#</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># set trace path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SGLANG_TORCH_PROFILER_DIR</span><span class="o">=</span>/root/sglang/profile_log

<span class="c1"># start prefill and decode servers (see PD disaggregation docs for setup)</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span>--disaggregation-mode<span class="w"> </span>prefill
python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span>--disaggregation-mode<span class="w"> </span>decode<span class="w"> </span>--port<span class="w"> </span><span class="m">30001</span><span class="w"> </span>--base-gpu-id<span class="w"> </span><span class="m">1</span>

<span class="c1"># start router</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang_router.launch_router<span class="w"> </span>--pd-disaggregation<span class="w"> </span>--prefill<span class="w"> </span>http://127.0.0.1:30000<span class="w"> </span>--decode<span class="w"> </span>http://127.0.0.1:30001<span class="w"> </span>--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span>

<span class="c1"># send profiling request targeting prefill workers</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_serving<span class="w"> </span>--backend<span class="w"> </span>sglang<span class="w"> </span>--model<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">10</span><span class="w"> </span>--sharegpt-output-len<span class="w"> </span><span class="m">100</span><span class="w"> </span>--profile<span class="w"> </span>--pd-separated<span class="w"> </span>--profile-prefill-url<span class="w"> </span>http://127.0.0.1:30000
</pre></div>
</div>
</section>
<section id="profile-decode-workers">
<h4>Profile Decode Workers<a class="headerlink" href="#profile-decode-workers" title="Link to this heading">#</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># send profiling request targeting decode workers</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_serving<span class="w"> </span>--backend<span class="w"> </span>sglang<span class="w"> </span>--model<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">10</span><span class="w"> </span>--sharegpt-output-len<span class="w"> </span><span class="m">100</span><span class="w"> </span>--profile<span class="w"> </span>--pd-separated<span class="w"> </span>--profile-decode-url<span class="w"> </span>http://127.0.0.1:30001
</pre></div>
</div>
</section>
<section id="important-notes">
<h4>Important Notes<a class="headerlink" href="#important-notes" title="Link to this heading">#</a></h4>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">--profile-prefill-url</span></code> and <code class="docutils literal notranslate"><span class="pre">--profile-decode-url</span></code> are <strong>mutually exclusive</strong> - you cannot profile both at the same time</p></li>
<li><p>Both options support multiple worker URLs for multi-instance setups:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Profile multiple prefill workers</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_serving<span class="w"> </span>--backend<span class="w"> </span>sglang<span class="w"> </span>--model<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">10</span><span class="w"> </span>--profile<span class="w"> </span>--pd-separated<span class="w"> </span>--profile-prefill-url<span class="w"> </span>http://127.0.0.1:30000<span class="w"> </span>http://127.0.0.1:30002

<span class="c1"># Profile multiple decode workers</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_serving<span class="w"> </span>--backend<span class="w"> </span>sglang<span class="w"> </span>--model<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">10</span><span class="w"> </span>--profile<span class="w"> </span>--pd-separated<span class="w"> </span>--profile-decode-url<span class="w"> </span>http://127.0.0.1:30001<span class="w"> </span>http://127.0.0.1:30003
</pre></div>
</div>
</li>
<li><p>Make sure <code class="docutils literal notranslate"><span class="pre">SGLANG_TORCH_PROFILER_DIR</span></code> is set on all worker nodes before starting the servers</p></li>
<li><p>For more details on setting up PD disaggregation, see <a class="reference internal" href="../advanced_features/pd_disaggregation.html"><span class="std std-doc">PD Disaggregation Guide</span></a></p></li>
</ul>
</section>
</section>
<section id="profile-a-server-with-sglang-bench-offline-throughput">
<h3>Profile a server with <code class="docutils literal notranslate"><span class="pre">sglang.bench_offline_throughput</span></code><a class="headerlink" href="#profile-a-server-with-sglang-bench-offline-throughput" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SGLANG_TORCH_PROFILER_DIR</span><span class="o">=</span>/root/sglang/profile_log

<span class="c1"># profile one batch with bench_one_batch.py</span>
<span class="c1"># batch size can be controlled with --batch argument</span>
python3<span class="w"> </span>-m<span class="w"> </span>sglang.bench_one_batch<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span>--batch<span class="w"> </span><span class="m">32</span><span class="w"> </span>--input-len<span class="w"> </span><span class="m">1024</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">10</span><span class="w"> </span>--profile

<span class="c1"># profile multiple batches with bench_offline_throughput.py</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_offline_throughput<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span>--dataset-name<span class="w"> </span>random<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">10</span><span class="w"> </span>--profile<span class="w"> </span>--mem-frac<span class="o">=</span><span class="m">0</span>.8
</pre></div>
</div>
</section>
<section id="profile-a-server-with-sglang-profiler">
<h3>Profile a server with <code class="docutils literal notranslate"><span class="pre">sglang.profiler</span></code><a class="headerlink" href="#profile-a-server-with-sglang-profiler" title="Link to this heading">#</a></h3>
<p>When the server is running (e.g., processing a decoding request), you can start live profiling immediately by sending a profile request to the server.</p>
<p>You can do this by running <code class="docutils literal notranslate"><span class="pre">python3</span> <span class="pre">-m</span> <span class="pre">sglang.profiler</span></code>. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Terminal 1: Send a generation request</span>
<span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">sglang</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">send_one</span>

<span class="c1"># Terminal 2: Before the above request finishes, quickly launch the following command in a separate terminal.</span>
<span class="c1"># It will generate a profile of the above request for several decoding batches.</span>
<span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">sglang</span><span class="o">.</span><span class="n">profiler</span>
</pre></div>
</div>
<p>You can also combine the above operations into a single command</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">sglang</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">send_one</span> <span class="o">--</span><span class="n">profile</span>
</pre></div>
</div>
</section>
<section id="profile-a-server-with-http-api-endpoints">
<h3>Profile a server with HTTP API endpoints<a class="headerlink" href="#profile-a-server-with-http-api-endpoints" title="Link to this heading">#</a></h3>
<p>SGLang provides HTTP API endpoints to control profiling on a running server. This allows you to start and stop profiling programmatically, which is useful for capturing specific workload patterns.</p>
<section id="using-start-profile-endpoint">
<h4>Using <code class="docutils literal notranslate"><span class="pre">/start_profile</span></code> endpoint<a class="headerlink" href="#using-start-profile-endpoint" title="Link to this heading">#</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">/start_profile</span></code> endpoint starts profiling on the server. You can control when profiling begins and how long it runs using the following parameters:</p>
<p><strong>Basic usage:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start profiling immediately for 10 steps</span>
curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>http://127.0.0.1:30000/start_profile<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;num_steps&quot;: 10</span>
<span class="s1">  }&#39;</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">output_dir</span></code> (optional): Directory where profile traces will be saved. If not specified, uses <code class="docutils literal notranslate"><span class="pre">SGLANG_TORCH_PROFILER_DIR</span></code> environment variable, or <code class="docutils literal notranslate"><span class="pre">/tmp</span></code> as the default</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_steps</span></code> (optional): Number of steps to profile. If not specified, profiling continues until manually stopped with <code class="docutils literal notranslate"><span class="pre">/end_profile</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">start_step</span></code> (optional): Step number at which to start profiling (inclusive). Useful for skipping warmup iterations</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">activities</span></code> (optional): List of activities to profile, e.g., <code class="docutils literal notranslate"><span class="pre">[&quot;CPU&quot;,</span> <span class="pre">&quot;GPU&quot;]</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">[&quot;CPU&quot;,</span> <span class="pre">&quot;GPU&quot;]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">merge_profiles</span></code> (optional): Whether to merge distributed traces. Default is <code class="docutils literal notranslate"><span class="pre">false</span></code></p></li>
</ul>
<p><strong>Note on step ranges:</strong> Profiling starts at <code class="docutils literal notranslate"><span class="pre">start_step</span></code> (inclusive) and continues for <code class="docutils literal notranslate"><span class="pre">num_steps</span></code> iterations. For example, with <code class="docutils literal notranslate"><span class="pre">start_step=3</span></code> and <code class="docutils literal notranslate"><span class="pre">num_steps=10</span></code>, profiling captures steps 3, 4, 5, 6, 7, 8, 9, 10, 11, and 12 (10 steps total, starting from step 3).</p>
<p><strong>Advanced usage with <code class="docutils literal notranslate"><span class="pre">start_step</span></code>:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Wait 5 steps (warmup), then profile for 10 steps</span>
curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>http://127.0.0.1:30000/start_profile<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;output_dir&quot;: &quot;/tmp/profiles&quot;,</span>
<span class="s1">    &quot;start_step&quot;: 5,</span>
<span class="s1">    &quot;num_steps&quot;: 10,</span>
<span class="s1">    &quot;activities&quot;: [&quot;CPU&quot;, &quot;GPU&quot;]</span>
<span class="s1">  }&#39;</span>
</pre></div>
</div>
<p><strong>Continuous profiling (manual stop):</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start profiling without num_steps - must manually stop with /end_profile</span>
curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>http://127.0.0.1:30000/start_profile
</pre></div>
</div>
</section>
<section id="using-end-profile-endpoint">
<h4>Using <code class="docutils literal notranslate"><span class="pre">/end_profile</span></code> endpoint<a class="headerlink" href="#using-end-profile-endpoint" title="Link to this heading">#</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">/end_profile</span></code> endpoint stops an ongoing profiling session and saves the trace file.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Stop profiling and save traces</span>
curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>http://127.0.0.1:30000/end_profile
</pre></div>
</div>
<p>This is only needed when you start profiling without specifying <code class="docutils literal notranslate"><span class="pre">num_steps</span></code>. If <code class="docutils literal notranslate"><span class="pre">num_steps</span></code> is specified, profiling will automatically stop after that many steps.</p>
</section>
<section id="example-workflow">
<h4>Example workflow<a class="headerlink" href="#example-workflow" title="Link to this heading">#</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Terminal 1: Start the server</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SGLANG_TORCH_PROFILER_DIR</span><span class="o">=</span>/tmp/profiles
python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct

<span class="c1"># Terminal 2: Start continuous profiling</span>
curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>http://127.0.0.1:30000/start_profile<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;start_step&quot;: 3</span>
<span class="s1">  }&#39;</span>

<span class="c1"># Terminal 3: Send requests to generate load</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_serving<span class="w"> </span>--backend<span class="w"> </span>sglang<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">100</span>

<span class="c1"># Terminal 2: Stop profiling when done</span>
curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>http://127.0.0.1:30000/end_profile
</pre></div>
</div>
</section>
</section>
<section id="profiler-trace-merger-for-distributed-traces">
<h3>Profiler Trace Merger for Distributed Traces<a class="headerlink" href="#profiler-trace-merger-for-distributed-traces" title="Link to this heading">#</a></h3>
<p>SGLang now supports automatic merging of profiling traces from distributed setups with multiple parallelism types (TP, DP, PP, EP). This feature is particularly useful for analyzing performance across distributed runs.</p>
<section id="multi-node-profiling-and-shared-storage-considerations">
<h4>Multi-Node Profiling and Shared Storage Considerations<a class="headerlink" href="#multi-node-profiling-and-shared-storage-considerations" title="Link to this heading">#</a></h4>
<p>Single-node profiler output merging is completely supported. When profiling in distributed environments spanning multiple nodes, shared storage (e.g., NFS, Lustre) should be accessible by all nodes for the output directory to enable merging of trace files.</p>
<p>If there is no shared storage accessible across nodes, automatic merging of trace files during profiling is not supported directly as of now.</p>
</section>
<section id="http-api-usage">
<h4>HTTP API Usage<a class="headerlink" href="#http-api-usage" title="Link to this heading">#</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start profiling with automatic trace merging enabled</span>
curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>&lt;BASE_URL&gt;/start_profile<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;output_dir&quot;: &quot;/tmp/profiles&quot;, # where to store profile traces</span>
<span class="s1">    &quot;num_steps&quot;: 10,</span>
<span class="s1">    &quot;activities&quot;: [&quot;CPU&quot;, &quot;GPU&quot;],</span>
<span class="s1">    &quot;merge_profiles&quot;: true # optional argument to merge profile traces (default=False)</span>
<span class="s1">  }&#39;</span>
</pre></div>
</div>
</section>
<section id="command-line-usage">
<h4>Command Line Usage<a class="headerlink" href="#command-line-usage" title="Link to this heading">#</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start profiling with merge enabled</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.profiler<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--num-steps<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--cpu<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--gpu<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--output-dir<span class="w"> </span>/tmp/profiles<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--merge-profiles<span class="w"> </span><span class="c1"># optional argument to merge profile traces (default=False)</span>
</pre></div>
</div>
</section>
<section id="output-files">
<h4>Output Files<a class="headerlink" href="#output-files" title="Link to this heading">#</a></h4>
<p>The profile merger generates:</p>
<ul class="simple">
<li><p>Individual rank trace files: <code class="docutils literal notranslate"><span class="pre">{profile_id}-TP-{tp}-DP-{dp}-PP-{pp}-EP-{ep}.trace.json.gz</span></code></p></li>
<li><p>Merged trace file: <code class="docutils literal notranslate"><span class="pre">merged-{profile_id}.trace.json.gz</span></code></p></li>
</ul>
</section>
</section>
<section id="possible-pytorch-bugs">
<h3>Possible PyTorch bugs<a class="headerlink" href="#possible-pytorch-bugs" title="Link to this heading">#</a></h3>
<p>If in any cases you encounter the following error (for example, using qwen 2.5 VL):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>RuntimeError:<span class="w"> </span>!stack.empty<span class="o">()</span><span class="w"> </span>INTERNAL<span class="w"> </span>ASSERT<span class="w"> </span>FAILED<span class="w"> </span>at<span class="w"> </span><span class="s2">&quot;/pytorch/torch/csrc/autograd/profiler_python.cpp&quot;</span>:983,<span class="w"> </span>please<span class="w"> </span>report<span class="w"> </span>a<span class="w"> </span>bug<span class="w"> </span>to<span class="w"> </span>PyTorch.<span class="w"> </span>Python<span class="w"> </span>replay<span class="w"> </span>stack<span class="w"> </span>is<span class="w"> </span>empty.
</pre></div>
</div>
<p>This is likely a PyTorch Bug reported in <a class="reference external" href="https://github.com/vllm-project/vllm/issues/18240">Bug: vLLM Profiler</a> and <a class="reference external" href="https://github.com/pytorch/pytorch/issues/101632">Bug: torch.profiler.profile</a>. As a workaround, you may disable <code class="docutils literal notranslate"><span class="pre">with_stack</span></code> with an environment variable such as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SGLANG_PROFILE_WITH_STACK</span><span class="o">=</span>False
python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_offline_throughput<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span>--dataset-name<span class="w"> </span>random<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">10</span><span class="w"> </span>--profile<span class="w"> </span>--mem-frac<span class="o">=</span><span class="m">0</span>.8
</pre></div>
</div>
</section>
<section id="view-traces">
<h3>View traces<a class="headerlink" href="#view-traces" title="Link to this heading">#</a></h3>
<p>Trace files can be loaded and visualized from:</p>
<ol class="arabic simple">
<li><p>https://ui.perfetto.dev/ (any browser)</p></li>
<li><p>chrome://tracing (Chrome browser only)</p></li>
</ol>
<p>If browser cannot open trace file due to its large size,
client can generate a small trace file (&lt;100MB) by controlling number of prompts and lengths of prompt outputs.
For example, when profiling a server,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_serving<span class="w"> </span>--backend<span class="w"> </span>sglang<span class="w"> </span>--model<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">2</span><span class="w"> </span>--sharegpt-output-len<span class="w"> </span><span class="m">100</span><span class="w"> </span>--profile
</pre></div>
</div>
<p>This command sets the number of prompts to 2 with <code class="docutils literal notranslate"><span class="pre">--num-prompts</span></code> argument and limits the length of output sequences to 100 with <code class="docutils literal notranslate"><span class="pre">--sharegpt-output-len</span></code> argument, which can generate a small trace file for browser to open smoothly.</p>
<p>Additionally, if you want to locate the SGLang Python source code through the cuda kernel in Trace, you need to disable CUDA Graph when starting the service. This can be done by using the <code class="docutils literal notranslate"><span class="pre">--disable-cuda-graph</span></code> parameter in the command to start the service.</p>
</section>
</section>
<section id="profile-with-nsight">
<h2>Profile with Nsight<a class="headerlink" href="#profile-with-nsight" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://docs.nvidia.com/nsight-systems/">Nsight systems</a> is an advanced tool that exposes more profiling details, such as register and shared memory usage, annotated code regions and low-level CUDA APIs and events.</p>
<ol class="arabic">
<li><p>Prerequisite:</p>
<p>Install using apt, or run inside a <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch/tags">NVIDIA Docker container</a> or <a class="reference external" href="https://github.com/sgl-project/sglang/tree/main/docker">SGLang Docker container</a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># install nsys</span>
<span class="c1"># https://docs.nvidia.com/nsight-systems/InstallationGuide/index.html</span>
apt<span class="w"> </span>update
apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>--no-install-recommends<span class="w"> </span>gnupg
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;deb http://developer.download.nvidia.com/devtools/repos/ubuntu</span><span class="k">$(</span><span class="nb">source</span><span class="w"> </span>/etc/lsb-release<span class="p">;</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$DISTRIB_RELEASE</span><span class="s2">&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>tr<span class="w"> </span>-d<span class="w"> </span>.<span class="k">)</span><span class="s2">/</span><span class="k">$(</span>dpkg<span class="w"> </span>--print-architecture<span class="k">)</span><span class="s2"> /&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>tee<span class="w"> </span>/etc/apt/sources.list.d/nvidia-devtools.list
apt-key<span class="w"> </span>adv<span class="w"> </span>--fetch-keys<span class="w"> </span>http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
apt<span class="w"> </span>update
apt<span class="w"> </span>install<span class="w"> </span>nsight-systems-cli
</pre></div>
</div>
</li>
<li><p>To profile a single batch, use</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nsys<span class="w"> </span>profile<span class="w"> </span>--trace-fork-before-exec<span class="o">=</span><span class="nb">true</span><span class="w"> </span>--cuda-graph-trace<span class="o">=</span>node<span class="w"> </span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.bench_one_batch<span class="w"> </span>--model<span class="w"> </span>meta-llama/Meta-Llama-3-8B<span class="w"> </span>--batch-size<span class="w"> </span><span class="m">64</span><span class="w"> </span>--input-len<span class="w"> </span><span class="m">512</span>
</pre></div>
</div>
</li>
<li><p>To profile a server, e.g.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># launch the server, set the delay and duration times according to needs</span>
<span class="c1"># after the duration time has been used up, server will be killed by nsys</span>

nsys<span class="w"> </span>profile<span class="w"> </span>--trace-fork-before-exec<span class="o">=</span><span class="nb">true</span><span class="w"> </span>--cuda-graph-trace<span class="o">=</span>node<span class="w"> </span>-o<span class="w"> </span>sglang.out<span class="w"> </span>--delay<span class="w"> </span><span class="m">60</span><span class="w"> </span>--duration<span class="w"> </span><span class="m">70</span><span class="w"> </span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span>--disable-radix-cache

<span class="c1"># client</span>
python3<span class="w"> </span>-m<span class="w"> </span>sglang.bench_serving<span class="w"> </span>--backend<span class="w"> </span>sglang<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">1000</span><span class="w"> </span>--dataset-name<span class="w"> </span>random<span class="w"> </span>--random-input<span class="w"> </span><span class="m">1024</span><span class="w"> </span>--random-output<span class="w"> </span><span class="m">512</span>
</pre></div>
</div>
<p>In practice, we recommend users to set <code class="docutils literal notranslate"><span class="pre">--duration</span></code> argument to a large value. Whenever user wants the server to stop profiling. Firstly run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nsys<span class="w"> </span>sessions<span class="w"> </span>list
</pre></div>
</div>
<p>to get the session id in the form of <code class="docutils literal notranslate"><span class="pre">profile-XXXXX</span></code>, then run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nsys<span class="w"> </span>stop<span class="w"> </span>--session<span class="o">=</span>profile-XXXXX
</pre></div>
</div>
<p>to manually kill the profiler and generate <code class="docutils literal notranslate"><span class="pre">nsys-rep</span></code> files instantly.</p>
</li>
<li><p>Use NVTX to annotate code regions, e.g. to see their execution time.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># install nvtx</span>
pip<span class="w"> </span>install<span class="w"> </span>nvtx
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># code snippets</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">nvtx</span>
<span class="k">with</span> <span class="n">nvtx</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;description&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;color&quot;</span><span class="p">):</span>
    <span class="c1"># some critical code</span>
</pre></div>
</div>
</li>
</ol>
<section id="layer-wise-nvtx-profiling-with-nsight-systems">
<h3>Layer-wise NVTX Profiling with Nsight Systems<a class="headerlink" href="#layer-wise-nvtx-profiling-with-nsight-systems" title="Link to this heading">#</a></h3>
<p>SGLang provides built-in layerwise NVTX annotations that can be combined with the CUDA Profiler for detailed per-layer profiling in Nsight Systems. This is particularly useful for identifying performance bottlenecks at the layer level.</p>
<section id="using-enable-layerwise-nvtx-marker-with-nsight-systems-and-start-profile">
<h4>Using <code class="docutils literal notranslate"><span class="pre">--enable-layerwise-nvtx-marker</span></code> with Nsight Systems and <code class="docutils literal notranslate"><span class="pre">/start_profile</span></code><a class="headerlink" href="#using-enable-layerwise-nvtx-marker-with-nsight-systems-and-start-profile" title="Link to this heading">#</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">--enable-layerwise-nvtx-marker</span></code> flag automatically adds NVTX markers to every layer in your model. This is particularly powerful when combined with Nsight Systems profiling to see detailed per-layer performance.</p>
<p><strong>Method 1: Using <code class="docutils literal notranslate"><span class="pre">/start_profile</span></code> with CUDA_PROFILER (for programmatic control)</strong></p>
<p>This method allows you to control exactly when profiling starts/stops via HTTP API while Nsight Systems is running.</p>
<ol class="arabic">
<li><p>Launch the server with layerwise NVTX enabled under Nsight Systems:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Terminal 1: Start server with nsys and capture-range option</span>
nsys<span class="w"> </span>profile<span class="w"> </span>--trace-fork-before-exec<span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--cuda-graph-trace<span class="o">=</span>node<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--capture-range<span class="o">=</span>cudaProfilerApi<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--capture-range-end<span class="o">=</span>stop<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-o<span class="w"> </span>layerwise_profile<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model-path<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--enable-layerwise-nvtx-marker<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--disable-cuda-graph
</pre></div>
</div>
<p>Note: NVTX markers are not emitted for kernel launches captured by CUDA graphs. Use <code class="docutils literal notranslate"><span class="pre">--disable-cuda-graph</span></code> to ensure all layerwise NVTX markers are emitted in the trace.</p>
</li>
<li><p>In another terminal, control profiling via <code class="docutils literal notranslate"><span class="pre">/start_profile</span></code> with <code class="docutils literal notranslate"><span class="pre">CUDA_PROFILER</span></code> activity:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Terminal 2: Wait for server to be ready, then start CUDA profiling</span>
<span class="c1"># Wait 3 steps for warmup, then profile for 10 steps</span>
curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>http://127.0.0.1:30000/start_profile<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;start_step&quot;: 3,</span>
<span class="s1">    &quot;num_steps&quot;: 10,</span>
<span class="s1">    &quot;activities&quot;: [&quot;CUDA_PROFILER&quot;]</span>
<span class="s1">  }&#39;</span>
</pre></div>
</div>
</li>
<li><p>Send requests to generate load:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Terminal 3: Generate workload</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_serving<span class="w"> </span>--backend<span class="w"> </span>sglang<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">100</span>
</pre></div>
</div>
</li>
<li><p>Profiling will automatically stop after 10 steps (due to <code class="docutils literal notranslate"><span class="pre">num_steps:</span> <span class="pre">10</span></code>). If you hadnt specified <code class="docutils literal notranslate"><span class="pre">num_steps</span></code>, you would need to manually stop it:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Terminal 2: Only needed if num_steps was not specified</span>
curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>http://127.0.0.1:30000/end_profile
</pre></div>
</div>
</li>
</ol>
<p>The <code class="docutils literal notranslate"><span class="pre">--capture-range=cudaProfilerApi</span></code> option tells Nsight Systems to only capture data between <code class="docutils literal notranslate"><span class="pre">cudaProfilerStart()</span></code> and <code class="docutils literal notranslate"><span class="pre">cudaProfilerStop()</span></code> calls (triggered by <code class="docutils literal notranslate"><span class="pre">/start_profile</span></code> and <code class="docutils literal notranslate"><span class="pre">/end_profile</span></code>), reducing overhead and file size. The <code class="docutils literal notranslate"><span class="pre">start_step</span></code> parameter skips the first 3 steps to avoid capturing warmup overhead.</p>
<p><strong>Method 2: Simpler approach without <code class="docutils literal notranslate"><span class="pre">/start_profile</span></code> API</strong></p>
<p>For simpler use cases where you dont need fine-grained control over profiling start/stop, you can profile with Nsight Systems capturing the entire workload:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Terminal 1: Start server with layerwise NVTX</span>
<span class="c1"># Note: --disable-cuda-graph ensures all NVTX markers are emitted</span>
python<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model-path<span class="w"> </span>meta-llama/Llama-3.1-8B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--enable-layerwise-nvtx-marker<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--disable-cuda-graph

<span class="c1"># Terminal 2: Profile the benchmarking client</span>
nsys<span class="w"> </span>profile<span class="w"> </span>--trace-fork-before-exec<span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--cuda-graph-trace<span class="o">=</span>node<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-o<span class="w"> </span>layerwise_profile<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_serving<span class="w"> </span>--backend<span class="w"> </span>sglang<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">10</span>
</pre></div>
</div>
<p>This approach profiles the entire client execution, including all server interactions. The layerwise NVTX markers will be visible in the Nsight Systems timeline.</p>
<p><strong>Viewing the profiling results:</strong></p>
<p>Open the generated <code class="docutils literal notranslate"><span class="pre">.qdrep</span></code> file with Nsight Systems:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nsys-ui<span class="w"> </span>layerwise_profile.qdrep
</pre></div>
</div>
<p>In the Nsight Systems GUI, youll see:</p>
<ul class="simple">
<li><p><strong>NVTX ranges</strong>: Each layer appears as a labeled range in the timeline with detailed information in the marker metadata</p></li>
<li><p><strong>CUDA kernels</strong>: All GPU kernels are shown alongside the layer annotations</p></li>
<li><p><strong>Layer hierarchy</strong>: The full module path (e.g., <code class="docutils literal notranslate"><span class="pre">meta-llama/Meta-Llama-3.1-8B-Instruct.model.layers.0.self_attn.qkv_proj</span></code>) helps identify specific layers. The prefix uses the full model path from <code class="docutils literal notranslate"><span class="pre">--model-path</span></code>.</p></li>
<li><p><strong>Tensor shapes</strong>: Input/output dimensions and parameter shapes are included in the NVTX marker data</p></li>
</ul>
<p><strong>Benefits of layerwise NVTX profiling:</strong></p>
<ul class="simple">
<li><p><strong>Granular visibility</strong>: See exactly which layers are taking the most time</p></li>
<li><p><strong>Memory tracking</strong>: Identify layers with large memory allocations</p></li>
<li><p><strong>Bottleneck identification</strong>: Quickly locate inefficient operations</p></li>
<li><p><strong>Communication overhead</strong>: In multi-GPU setups, see per-layer communication costs</p></li>
<li><p><strong>Development debugging</strong>: Validate that model architecture changes have the expected performance impact</p></li>
</ul>
</section>
</section>
</section>
<section id="other-tips">
<h2>Other tips<a class="headerlink" href="#other-tips" title="Link to this heading">#</a></h2>
<ol class="arabic">
<li><p>You can benchmark a model using dummy weights by only providing the config.json file. This allows for quick testing of model variants without training. To do so, add <code class="docutils literal notranslate"><span class="pre">--load-format</span> <span class="pre">dummy</span></code> to the above commands and then you only need a correct <code class="docutils literal notranslate"><span class="pre">config.json</span></code> under the checkpoint folder.</p></li>
<li><p>You can benchmark a model with modified configs (e.g., less layers) by using <code class="docutils literal notranslate"><span class="pre">--json-model-override-args</span></code>. For example, you can benchmark a model with only 2 layers and 2 kv heads using:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>sglang.bench_one_batch<span class="w"> </span>--model-path<span class="w"> </span>meta-llama/Meta-Llama-3.1-8B-Instruct<span class="w"> </span>--batch<span class="w"> </span><span class="m">32</span><span class="w"> </span>--input-len<span class="w"> </span><span class="m">256</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">32</span><span class="w"> </span>--load-format<span class="w"> </span>dummy<span class="w"> </span>--json-model-override-args<span class="w"> </span><span class="s1">&#39;{&quot;num_hidden_layers&quot;: 1, &quot;num_key_value_heads&quot;: 1}&#39;</span>
</pre></div>
</div>
</li>
<li><p>You can use <code class="docutils literal notranslate"><span class="pre">--python-backtrace=cuda</span></code> to see python call stack for all CUDA kernels, as in PyTorch Profiler. (Caveat: this can cause inaccurately long kernel runtimes for CUDA event based timing)</p></li>
<li><p>For more arguments see <a class="reference external" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html">Nsight Systems User Guide</a>.</p></li>
</ol>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="development_guide_using_docker.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Development Guide Using Docker</p>
      </div>
    </a>
    <a class="right-next"
       href="bench_serving.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bench Serving Guide</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark">Benchmark</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-with-pytorch-profiler">Profile with PyTorch Profiler</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-a-server-with-sglang-bench-serving">Profile a server with <code class="docutils literal notranslate"><span class="pre">sglang.bench_serving</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-in-pd-disaggregation-mode">Profile In PD Disaggregation Mode</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-prefill-workers">Profile Prefill Workers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-decode-workers">Profile Decode Workers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#important-notes">Important Notes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-a-server-with-sglang-bench-offline-throughput">Profile a server with <code class="docutils literal notranslate"><span class="pre">sglang.bench_offline_throughput</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-a-server-with-sglang-profiler">Profile a server with <code class="docutils literal notranslate"><span class="pre">sglang.profiler</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-a-server-with-http-api-endpoints">Profile a server with HTTP API endpoints</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-start-profile-endpoint">Using <code class="docutils literal notranslate"><span class="pre">/start_profile</span></code> endpoint</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-end-profile-endpoint">Using <code class="docutils literal notranslate"><span class="pre">/end_profile</span></code> endpoint</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-workflow">Example workflow</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profiler-trace-merger-for-distributed-traces">Profiler Trace Merger for Distributed Traces</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-profiling-and-shared-storage-considerations">Multi-Node Profiling and Shared Storage Considerations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#http-api-usage">HTTP API Usage</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#command-line-usage">Command Line Usage</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output-files">Output Files</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#possible-pytorch-bugs">Possible PyTorch bugs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#view-traces">View traces</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-with-nsight">Profile with Nsight</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-wise-nvtx-profiling-with-nsight-systems">Layer-wise NVTX Profiling with Nsight Systems</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-enable-layerwise-nvtx-marker-with-nsight-systems-and-start-profile">Using <code class="docutils literal notranslate"><span class="pre">--enable-layerwise-nvtx-marker</span></code> with Nsight Systems and <code class="docutils literal notranslate"><span class="pre">/start_profile</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-tips">Other tips</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By SGLang Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
       Copyright 2023-2026, SGLang.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Jan 27, 2026.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  
    <!-- RunLLM Widget Script -->
    <script type="module" id="runllm-widget-script" src="https://widget.runllm.com" crossorigin="true" version="stable" runllm-keyboard-shortcut="Mod+j" runllm-name="SGLang Chatbot" runllm-position="BOTTOM_RIGHT" runllm-assistant-id="629" async></script>
    
</body>
</html>