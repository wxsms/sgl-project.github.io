{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Engine API\n",
    "\n",
    "SGLang provides a direct inference engine without the need for an HTTP server, especially for use cases where additional HTTP server adds unnecessary complexity or overhead. Here are two general use cases:\n",
    "\n",
    "- Offline Batch Inference\n",
    "- Custom Server on Top of the Engine\n",
    "\n",
    "This document focuses on the offline batch inference, demonstrating four different inference modes:\n",
    "\n",
    "- Non-streaming synchronous generation\n",
    "- Streaming synchronous generation\n",
    "- Non-streaming asynchronous generation\n",
    "- Streaming asynchronous generation\n",
    "\n",
    "Additionally, you can easily build a custom server on top of the SGLang offline engine. A detailed example working in a python script can be found in [custom_server](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/custom_server.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nest Asyncio\n",
    "Note that if you want to use **Offline Engine** in ipython or some other nested loop code, you need to add the following code:\n",
    "```python\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "The engine supports [vlm inference](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/offline_batch_inference_vlm.py) as well as [extracting hidden states](https://github.com/sgl-project/sglang/blob/main/examples/runtime/hidden_states). \n",
    "\n",
    "Please see [the examples](https://github.com/sgl-project/sglang/tree/main/examples/runtime/engine) for further use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Batch Inference\n",
    "\n",
    "SGLang offline engine supports batch inference with efficient scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T01:59:04.642902Z",
     "iopub.status.busy": "2026-02-01T01:59:04.642811Z",
     "iopub.status.idle": "2026-02-01T01:59:26.814598Z",
     "shell.execute_reply": "2026-02-01T01:59:26.813819Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-01 01:59:09] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-01 01:59:09] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-01 01:59:09] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-01 01:59:12] INFO server_args.py:1775: Attention backend not specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-01 01:59:12] INFO server_args.py:2762: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-01 01:59:12] INFO engine.py:154: server_args=ServerArgs(model_path='qwen/qwen2.5-0.5b-instruct', tokenizer_path='qwen/qwen2.5-0.5b-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.835, max_running_requests=128, max_queued_requests=None, max_total_tokens=20480, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', enable_prefill_delayer=False, prefill_delayer_max_delay_passes=30, prefill_delayer_token_usage_low_watermark=None, prefill_delayer_forward_passes_buckets=None, prefill_delayer_wait_seconds_buckets=None, device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=662352838, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=300, dist_timeout=None, download_dir=None, model_checksum=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, log_requests_format='text', log_requests_target=None, uvicorn_access_log_exclude_prefixes=[], crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, admin_api_key=None, served_model_name='qwen/qwen2.5-0.5b-instruct', weight_version='default', chat_template=None, hf_chat_template_name=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, enable_lora_overlap_loading=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='fa3', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', fp4_gemm_runner_backend='auto', nsa_prefill_backend=None, nsa_decode_backend=None, disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, enable_multi_layer_eagle=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', disable_hicache_numa_detect=False, hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, hierarchical_sparse_attention_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=8192, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 576, 640, 704, 768, 832, 896, 960, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816, 3072, 3328, 3584, 3840, 4096, 4608, 5120, 5632, 6144, 6656, 7168, 7680, 8192], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, nsa_prefill_cp_mode='in-seq-split', enable_fused_qk_norm_rope=False, enable_precise_embedding_interpolation=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, disaggregation_decode_enable_fake_auto=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, limit_mm_data_per_request=None, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.72it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.72it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=128 avail_mem=76.27 GB):   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=128 avail_mem=76.27 GB):   5%|▌         | 1/20 [00:00<00:07,  2.41it/s]\r",
      "Capturing batches (bs=120 avail_mem=76.16 GB):   5%|▌         | 1/20 [00:00<00:07,  2.41it/s]\r",
      "Capturing batches (bs=112 avail_mem=76.16 GB):   5%|▌         | 1/20 [00:00<00:07,  2.41it/s]\r",
      "Capturing batches (bs=104 avail_mem=76.16 GB):   5%|▌         | 1/20 [00:00<00:07,  2.41it/s]\r",
      "Capturing batches (bs=104 avail_mem=76.16 GB):  20%|██        | 4/20 [00:00<00:01,  9.32it/s]\r",
      "Capturing batches (bs=96 avail_mem=76.15 GB):  20%|██        | 4/20 [00:00<00:01,  9.32it/s] \r",
      "Capturing batches (bs=88 avail_mem=76.15 GB):  20%|██        | 4/20 [00:00<00:01,  9.32it/s]\r",
      "Capturing batches (bs=80 avail_mem=76.14 GB):  20%|██        | 4/20 [00:00<00:01,  9.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=80 avail_mem=76.14 GB):  35%|███▌      | 7/20 [00:00<00:00, 14.51it/s]\r",
      "Capturing batches (bs=72 avail_mem=76.14 GB):  35%|███▌      | 7/20 [00:00<00:00, 14.51it/s]\r",
      "Capturing batches (bs=64 avail_mem=76.13 GB):  35%|███▌      | 7/20 [00:00<00:00, 14.51it/s]\r",
      "Capturing batches (bs=56 avail_mem=76.12 GB):  35%|███▌      | 7/20 [00:00<00:00, 14.51it/s]\r",
      "Capturing batches (bs=56 avail_mem=76.12 GB):  50%|█████     | 10/20 [00:00<00:00, 18.11it/s]\r",
      "Capturing batches (bs=48 avail_mem=76.12 GB):  50%|█████     | 10/20 [00:00<00:00, 18.11it/s]\r",
      "Capturing batches (bs=40 avail_mem=76.11 GB):  50%|█████     | 10/20 [00:00<00:00, 18.11it/s]\r",
      "Capturing batches (bs=32 avail_mem=76.11 GB):  50%|█████     | 10/20 [00:00<00:00, 18.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=32 avail_mem=76.11 GB):  65%|██████▌   | 13/20 [00:00<00:00, 21.08it/s]\r",
      "Capturing batches (bs=24 avail_mem=76.11 GB):  65%|██████▌   | 13/20 [00:00<00:00, 21.08it/s]\r",
      "Capturing batches (bs=16 avail_mem=76.10 GB):  65%|██████▌   | 13/20 [00:00<00:00, 21.08it/s]\r",
      "Capturing batches (bs=12 avail_mem=76.10 GB):  65%|██████▌   | 13/20 [00:00<00:00, 21.08it/s]\r",
      "Capturing batches (bs=12 avail_mem=76.10 GB):  80%|████████  | 16/20 [00:01<00:00, 19.68it/s]\r",
      "Capturing batches (bs=8 avail_mem=76.09 GB):  80%|████████  | 16/20 [00:01<00:00, 19.68it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=76.09 GB):  80%|████████  | 16/20 [00:01<00:00, 19.68it/s]\r",
      "Capturing batches (bs=2 avail_mem=76.08 GB):  80%|████████  | 16/20 [00:01<00:00, 19.68it/s]\r",
      "Capturing batches (bs=2 avail_mem=76.08 GB):  95%|█████████▌| 19/20 [00:01<00:00, 18.04it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.08 GB):  95%|█████████▌| 19/20 [00:01<00:00, 18.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=1 avail_mem=76.08 GB): 100%|██████████| 20/20 [00:01<00:00, 15.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# launch the offline engine\n",
    "import asyncio\n",
    "\n",
    "import sglang as sgl\n",
    "import sglang.test.doc_patch\n",
    "from sglang.utils import async_stream_and_merge, stream_and_merge\n",
    "\n",
    "llm = sgl.Engine(model_path=\"qwen/qwen2.5-0.5b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T01:59:26.817462Z",
     "iopub.status.busy": "2026-02-01T01:59:26.816800Z",
     "iopub.status.idle": "2026-02-01T01:59:28.498879Z",
     "shell.execute_reply": "2026-02-01T01:59:28.497752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Hello, my name is\n",
      "Generated text:  Huaqing. I am a 13-year-old high school student. I have been studying and practicing Chinese since my childhood. I also have a pet dog named Xingguo. He is my pet, but I don't like to play with him because he is often my \"friend\".\n",
      "\n",
      "Q1: Why do you like dogs?\n",
      "\n",
      "Q2: Why do you dislike playing with Xingguo?\n",
      "\n",
      "A: (1) The Chinese people love dogs very much. In many people's hearts, dogs have a special place in their lives, and it's very rare to see a person without dogs in their life. Dogs\n",
      "===============================\n",
      "Prompt: The president of the United States is\n",
      "Generated text:  running for a second term. To ensure that the second term will not be interrupted, the president will ask for and receive an additional term from Congress. By law, Congress is required to approve the president's request for a second term, but there is a risk that Congress will approve the request and the president will be considered to have won a second term. Furthermore, there is a 50% chance that Congress will not approve the request and the president will not be considered to have won a second term. In this election cycle, there are 400 Representatives and 40 Senators in the United States, and the president has\n",
      "===============================\n",
      "Prompt: The capital of France is\n",
      "Generated text:  Paris.\n",
      "A. 正确\n",
      "B. 错误\n",
      "Answer:\n",
      "A\n",
      "\n",
      "A person who has been sentenced to public surveillance shall be under surveillance by the public security organ until the expiration of the term. A. Correct B. Incorrect\n",
      "Answer:\n",
      "A\n",
      "\n",
      "Which of the following substances can hydrolyze to produce 2 molecules of acetic acid?\n",
      "A. :\n",
      "A. Lactic acid\n",
      "B. :\n",
      "B. Acetone\n",
      "C. :\n",
      "C. Acetaldehyde\n",
      "D. :\n",
      "D. Phenol\n",
      "Answer:\n",
      "C\n",
      "\n",
      "Which of the following is not a storage medium? \n",
      "A. CD-ROM\n",
      "\n",
      "===============================\n",
      "Prompt: The future of AI is\n",
      "Generated text:  in our hands. The world of technology is going through a major shift in terms of how it is being used and developed, and for the better. We are seeing a lot of innovation in the field of AI and the tools that are being developed for it are constantly improving and evolving.\n",
      "One of the most exciting developments in the field of AI has been the introduction of Generative AI. This is a type of AI that uses algorithms to generate new content, such as images, videos, text, and more. Generative AI has the potential to revolutionize the way we create and generate content, and it has the potential to make a significant\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T01:59:28.501216Z",
     "iopub.status.busy": "2026-02-01T01:59:28.501052Z",
     "iopub.status.idle": "2026-02-01T01:59:29.309927Z",
     "shell.execute_reply": "2026-02-01T01:59:29.304912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing synchronous streaming generation with overlap removal ===\n",
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  [Name], and I'm a [job title] at [company name]. I'm excited to meet you and learn more about your interests and experiences. Let's chat! [Name] [Job Title] [Company Name] [Company Address] [City, State, Zip Code] [Phone Number] [Email Address] [LinkedIn Profile] [Twitter Profile] [Facebook Profile] [Instagram Profile] [GitHub Profile] [LinkedIn Profile] [Twitter Profile] [Facebook Profile] [Instagram Profile] [GitHub Profile] [LinkedIn Profile] [Twitter Profile] [Facebook Profile] [Instagram Profile] [GitHub Profile] [LinkedIn\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  Paris, known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and Louvre Museum. It is also a major cultural and economic center, hosting numerous museums, theaters, and other attractions. Paris is a popular tourist destination and a major hub for international business and diplomacy. It is also known for its rich history and diverse cultural scene. The city is home to many famous French artists, writers, and musicians, and is a major center for the arts and entertainment industry. Paris is a vibrant and dynamic city with a rich cultural heritage that continues to attract visitors from around the world. The city is also known\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  likely to be characterized by several key trends:\n",
      "\n",
      "1. Increased integration with human intelligence: AI is likely to become more integrated with human intelligence, allowing for more complex and nuanced decision-making. This could lead to more sophisticated and adaptive AI systems that can learn from human behavior and adapt to new situations.\n",
      "\n",
      "2. Greater emphasis on ethical considerations: As AI becomes more integrated with human intelligence, there will be a greater emphasis on ethical considerations. This could lead to more rigorous testing and evaluation of AI systems, as well as greater transparency and accountability in their development and deployment.\n",
      "\n",
      "3. Increased use of AI in healthcare: AI is already being used in healthcare\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "print(\"\\n=== Testing synchronous streaming generation with overlap removal ===\\n\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    merged_output = stream_and_merge(llm, prompt, sampling_params)\n",
    "    print(\"Generated text:\", merged_output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T01:59:29.313302Z",
     "iopub.status.busy": "2026-02-01T01:59:29.313122Z",
     "iopub.status.idle": "2026-02-01T01:59:29.566172Z",
     "shell.execute_reply": "2026-02-01T01:59:29.565729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing asynchronous batch generation ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n",
      "Generated text:  [Name]. I'm a [Age] year old [Gender] [Occupation]. I enjoy [list all about your hobbies, interests, and passions]. [You are passionate about] [describe a specific hobby or activity you enjoy]. I'm a team player who [describe a specific personality trait or quality]. I value [mention the qualities that matter most to you]. I have a good knowledge of [specific subject or area of interest]. I'm a [describe any other characteristics or qualities]. [You are someone who] [state a positive attribute that you believe in].\n",
      "Hello, my name is [Name]. I'm a [\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n",
      "Generated text:  Paris. It is known for its charming canals, art museums, and iconic Notre-Dame Cathedral, among other attractions. The city is renowned for its rich history, particularly in terms of its role in the French Revolution and its status as the world's oldest capital city. Paris also has a strong cultural output, with many world-famous museums and attractions that draw visitors from around the globe. Finally, France’s capital is often a hub for business and politics, hosting important government meetings and state events throughout the year. \n",
      "\n",
      "In summary, the capital of France, Paris, is a vibrant and picturesque city known for its art, architecture,\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n",
      "Generated text:  promising and diverse. Here are some possible trends that could shape the AI landscape in the next decade:\n",
      "\n",
      "1. Increased automation: AI is expected to become more and more advanced, with a focus on increasing automation. This could lead to the development of more efficient, scalable, and reliable AI systems that can perform complex tasks with greater speed and precision.\n",
      "\n",
      "2. AI ethics: As AI becomes more integrated into our daily lives, there will be a growing need for ethical guidelines and regulations to govern its use. This could lead to the development of new AI systems that prioritize ethical considerations over purely technical performance.\n",
      "\n",
      "3. AI for healthcare: AI has the\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous batch generation ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    outputs = await llm.async_generate(prompts, sampling_params)\n",
    "\n",
    "    for prompt, output in zip(prompts, outputs):\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated text: {output['text']}\")\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T01:59:29.581926Z",
     "iopub.status.busy": "2026-02-01T01:59:29.581693Z",
     "iopub.status.idle": "2026-02-01T01:59:30.247224Z",
     "shell.execute_reply": "2026-02-01T01:59:30.246364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing asynchronous streaming generation (no repeats) ===\n",
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Name"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]. I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [age"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "], ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "] ("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " female)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [skill"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or quality"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]. I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occupation]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " area of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " study"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]. What"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " can you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tell me"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " about yourself"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " am a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " personality trait"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " characteristic,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " if any"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]. And"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " what brings"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you here"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " today?"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I believe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it's"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " important to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be honest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " share who"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you are"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with someone"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". What"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " are your"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " goals for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the future"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and how"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " will you"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pursue them"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'m excited"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to meet"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Remember"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to keep"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " your intro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " short and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " point,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " focusing on"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " your unique"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " qualities and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " achievements."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Use a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " neutral and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " unbiased"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". It"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " largest city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in France"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " UNESCO"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " World Heritage"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Site"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " France"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " national"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " capital"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " seat"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " government"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " heart"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " economy"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " center"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " culture"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " entertainment"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " boasting"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " world"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " most"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " famous"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " museums"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " restaurants"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " architecture"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " also"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hub"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cultural"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " industry"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hosting"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " many"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " France"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " major"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cultural"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " festivals"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " such"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Festival"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " de"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cannes"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mus"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ée"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " de"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " l"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ie"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Op"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "éra"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cultural"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " economic"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " center"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " has"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " largest"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " metropolitan"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " area"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " world"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " population"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " approximately"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " million"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " people"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n",
      "Generated text: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " full"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " exciting"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " possibilities"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " here"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " are"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " some"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " potential"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trends"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " look"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " out"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Adv"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ancements"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " machine"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " learning"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " neural"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " networks"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " help"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " powerful"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " computing"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " power"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " massive"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " amounts"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ","
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " models"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " will"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " become"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " increasingly"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sophisticated"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " will"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lead"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more accurate"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predictions and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " better"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " solutions"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " complex problems"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Increased"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " integration"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " human"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " intelligence"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " will"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continue"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " merge"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " human"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " intelligence"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", making"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " them more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " effective and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " efficient."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This will"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lead to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more personalized"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and adaptive"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " solutions that"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " are tailored"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to individual"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " needs.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Rise"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " new"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " forms"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " development"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of quantum"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " computers,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " will"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " become"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " powerful"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " capable"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " solving"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous streaming generation (no repeats) ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(\"Generated text: \", end=\"\", flush=True)\n",
    "\n",
    "        # Replace direct calls to async_generate with our custom overlap-aware version\n",
    "        async for cleaned_chunk in async_stream_and_merge(llm, prompt, sampling_params):\n",
    "            print(cleaned_chunk, end=\"\", flush=True)\n",
    "\n",
    "        print()  # New line after each prompt\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T01:59:30.249259Z",
     "iopub.status.busy": "2026-02-01T01:59:30.249111Z",
     "iopub.status.idle": "2026-02-01T01:59:30.271140Z",
     "shell.execute_reply": "2026-02-01T01:59:30.270687Z"
    }
   },
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
