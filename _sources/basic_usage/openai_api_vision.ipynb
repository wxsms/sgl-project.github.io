{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI APIs - Vision\n",
    "\n",
    "SGLang provides OpenAI-compatible APIs to enable a smooth transition from OpenAI services to self-hosted local models.\n",
    "A complete reference for the API is available in the [OpenAI API Reference](https://platform.openai.com/docs/guides/vision).\n",
    "This tutorial covers the vision APIs for vision language models.\n",
    "\n",
    "SGLang supports various vision language models such as Llama 3.2, LLaVA-OneVision, Qwen2.5-VL, Gemma3 and [more](../supported_models/text_generation/multimodal_language_models.md).\n",
    "\n",
    "As an alternative to the OpenAI API, you can also use the [SGLang offline engine](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/offline_batch_inference_vlm.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch A Server\n",
    "\n",
    "Launch the server in your terminal and wait for it to initialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T09:50:59.879807Z",
     "iopub.status.busy": "2026-02-27T09:50:59.879709Z",
     "iopub.status.idle": "2026-02-27T09:51:43.657741Z",
     "shell.execute_reply": "2026-02-27T09:51:43.657082Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 09:51:07] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 09:51:07] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 09:51:07] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-27 09:51:13] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-02-27 09:51:13] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-02-27 09:51:13] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-27 09:51:16] INFO server_args.py:1864: Attention backend not specified. Use fa3 backend by default.\n",
      "[2026-02-27 09:51:16] INFO server_args.py:2934: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-27 09:51:19] Ignore import error when loading sglang.srt.multimodal.processors.glmasr: cannot import name 'GlmAsrConfig' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-27 09:51:22] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-02-27 09:51:22] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-02-27 09:51:22] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2026-02-27 09:51:22] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-02-27 09:51:22] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-02-27 09:51:22] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-27 09:51:30] Ignore import error when loading sglang.srt.models.glm_ocr: No module named 'transformers.models.glm_ocr'\n",
      "[2026-02-27 09:51:30] Ignore import error when loading sglang.srt.models.glm_ocr_nextn: No module named 'transformers.models.glm_ocr'\n",
      "[2026-02-27 09:51:30] Ignore import error when loading sglang.srt.models.glmasr: cannot import name 'GlmAsrConfig' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:03,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:03<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.53it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.35it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=61.37 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=61.37 GB):  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s]\r",
      "Capturing batches (bs=2 avail_mem=61.34 GB):  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s]\r",
      "Capturing batches (bs=1 avail_mem=61.33 GB):  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s]\r",
      "Capturing batches (bs=1 avail_mem=61.33 GB): 100%|██████████| 3/3 [00:00<00:00,  4.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>        NOTE: Typically, the server runs in a separate terminal.<br>        In this notebook, we run the server and notebook code together, so their outputs are combined.<br>        To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>        To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>        We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>        </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang.test.doc_patch import launch_server_cmd\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "vision_process, port = launch_server_cmd(\"\"\"\n",
    "python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-VL-7B-Instruct --log-level warning\n",
    "\"\"\")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\", process=vision_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cURL\n",
    "\n",
    "Once the server is up, you can send test requests using curl or requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T09:51:43.659756Z",
     "iopub.status.busy": "2026-02-27T09:51:43.659458Z",
     "iopub.status.idle": "2026-02-27T09:51:45.741093Z",
     "shell.execute_reply": "2026-02-27T09:51:45.740503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"id\":\"f50c5bcb379d4b74acc853b8dbe5b7c9\",\"object\":\"chat.completion\",\"created\":1772185905,\"model\":\"Qwen/Qwen2.5-VL-7B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The image shows a man standing on the back of a yellow taxi, using an iron to iron a pair of blue jeans. The taxi is parked on a city street, and there are other taxis and buildings in the background. The man appears to be balancing on the taxi's rear bumper while ironing the jeans.\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":151645}],\"usage\":{\"prompt_tokens\":307,\"total_tokens\":371,\"completion_tokens\":64,\"prompt_tokens_details\":null,\"reasoning_tokens\":0},\"metadata\":{\"weight_version\":\"default\"}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-27 09:51:45] [load_mm_data(simple)] error loading IMAGE data at index=0\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 716, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 468, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 463, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 323, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 292, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 802, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 552, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 716, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 468, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 463, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 323, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 292, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 433, in _load_single_item\n",
      "    img, _ = load_image(data)\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/utils/common.py\", line 909, in load_image\n",
      "    response = requests.get(image_file, stream=True, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 659, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 747, in fast_load_mm_data\n",
      "    result = future.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 443, in _load_single_item\n",
      "    raise RuntimeError(f\"Error while loading data {data}: {e}\")\n",
      "RuntimeError: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto', max_dynamic_patch=None): ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[2026-02-27 09:51:45] Error in request: An exception occurred while loading IMAGE data at index 0: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto', max_dynamic_patch=None): ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 716, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 468, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 463, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 323, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 292, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 802, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 552, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/packages/six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 716, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 468, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 463, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 323, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 292, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 433, in _load_single_item\n",
      "    img, _ = load_image(data)\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/utils/common.py\", line 909, in load_image\n",
      "    response = requests.get(image_file, stream=True, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 659, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 747, in fast_load_mm_data\n",
      "    result = future.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 443, in _load_single_item\n",
      "    raise RuntimeError(f\"Error while loading data {data}: {e}\")\n",
      "RuntimeError: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto', max_dynamic_patch=None): ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/entrypoints/openai/serving_base.py\", line 102, in handle_request\n",
      "    return await self._handle_non_streaming_request(\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/entrypoints/openai/serving_chat.py\", line 874, in _handle_non_streaming_request\n",
      "    ret = await self.tokenizer_manager.generate_request(\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 503, in generate_request\n",
      "    tokenized_obj = await self._tokenize_one_request(obj)\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 714, in _tokenize_one_request\n",
      "    mm_inputs: Dict = await self.mm_data_processor.process(\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/managers/async_mm_data_processor.py\", line 99, in process\n",
      "    return await asyncio.wait_for(_invoke(), timeout=self.timeout_s)\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/managers/async_mm_data_processor.py\", line 70, in _invoke\n",
      "    return await self._proc_async(\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/multimodal/processors/qwen_vl.py\", line 320, in process_mm_data_async\n",
      "    base_output = self.load_mm_data(\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 684, in load_mm_data\n",
      "    return self.fast_load_mm_data(\n",
      "  File \"/public_sglang_ci/runner-l3-9gzh5-gpu-23/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 754, in fast_load_mm_data\n",
      "    raise RuntimeError(\n",
      "RuntimeError: An exception occurred while loading IMAGE data at index 0: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto', max_dynamic_patch=None): ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"object\":\"error\",\"message\":\"Internal server error: An exception occurred while loading IMAGE data at index 0: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto', max_dynamic_patch=None): ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\",\"type\":\"InternalServerError\",\"param\":null,\"code\":500}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "curl_command = f\"\"\"\n",
    "curl -s http://localhost:{port}/v1/chat/completions \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{{\n",
    "    \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    \"messages\": [\n",
    "      {{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"What’s in this image?\"\n",
    "          }},\n",
    "          {{\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {{\n",
    "              \"url\": \"https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\"\n",
    "            }}\n",
    "          }}\n",
    "        ]\n",
    "      }}\n",
    "    ],\n",
    "    \"max_tokens\": 300\n",
    "  }}'\n",
    "\"\"\"\n",
    "\n",
    "response = subprocess.check_output(curl_command, shell=True).decode()\n",
    "print_highlight(response)\n",
    "\n",
    "\n",
    "response = subprocess.check_output(curl_command, shell=True).decode()\n",
    "print_highlight(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T09:51:45.742558Z",
     "iopub.status.busy": "2026-02-27T09:51:45.742437Z",
     "iopub.status.idle": "2026-02-27T09:51:46.952543Z",
     "shell.execute_reply": "2026-02-27T09:51:46.951971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"id\":\"59743cfb077d4eeb82002c84602b97e1\",\"object\":\"chat.completion\",\"created\":1772185906,\"model\":\"Qwen/Qwen2.5-VL-7B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The image shows a man standing on the back of a yellow taxi, using an iron to iron a piece of clothing. The taxi is parked on a city street, and there are other taxis and buildings in the background. The man appears to be balancing on the taxi's rear bumper while ironing, which is an unusual and somewhat humorous scene.\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":151645}],\"usage\":{\"prompt_tokens\":307,\"total_tokens\":377,\"completion_tokens\":70,\"prompt_tokens_details\":null,\"reasoning_tokens\":0},\"metadata\":{\"weight_version\":\"default\"}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = f\"http://localhost:{port}/v1/chat/completions\"\n",
    "\n",
    "data = {\n",
    "    \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\"\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 300,\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T09:51:46.954016Z",
     "iopub.status.busy": "2026-02-27T09:51:46.953896Z",
     "iopub.status.idle": "2026-02-27T09:51:47.876054Z",
     "shell.execute_reply": "2026-02-27T09:51:47.875463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The image shows a man standing on the back of a yellow taxi, using an iron to iron a piece of clothing. The taxi is parked on a city street, and there are other taxis and buildings in the background. The man appears to be balancing on the taxi's rear bumper while ironing, which is an unusual and somewhat humorous scene.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=f\"http://localhost:{port}/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is in this image?\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\"\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "print_highlight(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple-Image Inputs\n",
    "\n",
    "The server also supports multiple images and interleaved text and images if the model supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T09:51:47.877467Z",
     "iopub.status.busy": "2026-02-27T09:51:47.877351Z",
     "iopub.status.idle": "2026-02-27T09:51:49.170087Z",
     "shell.execute_reply": "2026-02-27T09:51:49.169534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The first image shows a man ironing clothes on the back of a yellow taxi in an urban setting. The second image is a stylized logo featuring the letters \"SGL\" in orange with a book and a computer icon as part of the design.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=f\"http://localhost:{port}/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\",\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://raw.githubusercontent.com/sgl-project/sglang/main/assets/logo.png\",\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"I have two very different images. They are not related at all. \"\n",
    "                    \"Please describe the first image in one sentence, and then describe the second image in another sentence.\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print_highlight(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T09:51:49.171512Z",
     "iopub.status.busy": "2026-02-27T09:51:49.171395Z",
     "iopub.status.idle": "2026-02-27T09:51:49.187399Z",
     "shell.execute_reply": "2026-02-27T09:51:49.186842Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(vision_process)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
