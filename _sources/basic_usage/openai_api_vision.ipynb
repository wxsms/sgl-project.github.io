{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI APIs - Vision\n",
    "\n",
    "SGLang provides OpenAI-compatible APIs to enable a smooth transition from OpenAI services to self-hosted local models.\n",
    "A complete reference for the API is available in the [OpenAI API Reference](https://platform.openai.com/docs/guides/vision).\n",
    "This tutorial covers the vision APIs for vision language models.\n",
    "\n",
    "SGLang supports various vision language models such as Llama 3.2, LLaVA-OneVision, Qwen2.5-VL, Gemma3 and [more](../supported_models/multimodal_language_models.md).\n",
    "\n",
    "As an alternative to the OpenAI API, you can also use the [SGLang offline engine](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/offline_batch_inference_vlm.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch A Server\n",
    "\n",
    "Launch the server in your terminal and wait for it to initialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T12:39:08.518996Z",
     "iopub.status.busy": "2025-12-30T12:39:08.518868Z",
     "iopub.status.idle": "2025-12-30T12:39:51.543607Z",
     "shell.execute_reply": "2025-12-30T12:39:51.542841Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-30 12:39:14] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-30 12:39:14] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-30 12:39:14] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 12:39:20] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-30 12:39:20] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-30 12:39:20] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 12:39:22] INFO server_args.py:1565: Attention backend not specified. Use flashinfer backend by default.\n",
      "[2025-12-30 12:39:22] INFO server_args.py:2443: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 12:39:25] Ignore import error when loading sglang.srt.multimodal.processors.glmasr: cannot import name 'GlmAsrConfig' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 12:39:30] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-30 12:39:30] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-30 12:39:30] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-30 12:39:30] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-30 12:39:30] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-30 12:39:30] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 12:39:37] Ignore import error when loading sglang.srt.models.glmasr: cannot import name 'GlmAsrConfig' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:01<00:04,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:02<00:03,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:03<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:04<00:00,  1.43it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:04<00:00,  1.23it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=60.89 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=60.89 GB):  33%|███▎      | 1/3 [00:00<00:01,  1.54it/s]\r",
      "Capturing batches (bs=2 avail_mem=60.84 GB):  33%|███▎      | 1/3 [00:00<00:01,  1.54it/s]\r",
      "Capturing batches (bs=1 avail_mem=60.84 GB):  33%|███▎      | 1/3 [00:00<00:01,  1.54it/s]\r",
      "Capturing batches (bs=1 avail_mem=60.84 GB): 100%|██████████| 3/3 [00:00<00:00,  4.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang.test.doc_patch import launch_server_cmd\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "vision_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-VL-7B-Instruct --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cURL\n",
    "\n",
    "Once the server is up, you can send test requests using curl or requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T12:39:51.546136Z",
     "iopub.status.busy": "2025-12-30T12:39:51.545357Z",
     "iopub.status.idle": "2025-12-30T12:39:51.924918Z",
     "shell.execute_reply": "2025-12-30T12:39:51.924255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 12:39:51] Error in request: An exception occurred while loading multimodal data: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto'): 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 406, in _load_single_item\n",
      "    img, _ = load_image(data)\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/utils/common.py\", line 859, in load_image\n",
      "    response.raise_for_status()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 623, in load_mm_data\n",
      "    result = next(futures_iter).result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 416, in _load_single_item\n",
      "    raise RuntimeError(f\"Error while loading data {data}: {e}\")\n",
      "RuntimeError: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto'): 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/entrypoints/openai/serving_base.py\", line 120, in handle_request\n",
      "    return await self._handle_non_streaming_request(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/entrypoints/openai/serving_chat.py\", line 742, in _handle_non_streaming_request\n",
      "    ret = await self.tokenizer_manager.generate_request(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 490, in generate_request\n",
      "    tokenized_obj = await self._tokenize_one_request(obj)\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 694, in _tokenize_one_request\n",
      "    mm_inputs: Dict = await self.mm_data_processor.process(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/managers/async_mm_data_processor.py\", line 99, in process\n",
      "    return await asyncio.wait_for(_invoke(), timeout=self.timeout_s)\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/managers/async_mm_data_processor.py\", line 70, in _invoke\n",
      "    return await self._proc_async(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/qwen_vl.py\", line 308, in process_mm_data_async\n",
      "    base_output = self.load_mm_data(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 672, in load_mm_data\n",
      "    raise RuntimeError(\n",
      "RuntimeError: An exception occurred while loading multimodal data: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto'): 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"object\":\"error\",\"message\":\"Internal server error: An exception occurred while loading multimodal data: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto'): 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\",\"type\":\"InternalServerError\",\"param\":null,\"code\":500}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 12:39:51] Error in request: An exception occurred while loading multimodal data: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto'): 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 406, in _load_single_item\n",
      "    img, _ = load_image(data)\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/utils/common.py\", line 859, in load_image\n",
      "    response.raise_for_status()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 623, in load_mm_data\n",
      "    result = next(futures_iter).result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 416, in _load_single_item\n",
      "    raise RuntimeError(f\"Error while loading data {data}: {e}\")\n",
      "RuntimeError: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto'): 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/entrypoints/openai/serving_base.py\", line 120, in handle_request\n",
      "    return await self._handle_non_streaming_request(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/entrypoints/openai/serving_chat.py\", line 742, in _handle_non_streaming_request\n",
      "    ret = await self.tokenizer_manager.generate_request(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 490, in generate_request\n",
      "    tokenized_obj = await self._tokenize_one_request(obj)\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 694, in _tokenize_one_request\n",
      "    mm_inputs: Dict = await self.mm_data_processor.process(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/managers/async_mm_data_processor.py\", line 99, in process\n",
      "    return await asyncio.wait_for(_invoke(), timeout=self.timeout_s)\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/managers/async_mm_data_processor.py\", line 70, in _invoke\n",
      "    return await self._proc_async(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/qwen_vl.py\", line 308, in process_mm_data_async\n",
      "    base_output = self.load_mm_data(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 672, in load_mm_data\n",
      "    raise RuntimeError(\n",
      "RuntimeError: An exception occurred while loading multimodal data: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto'): 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"object\":\"error\",\"message\":\"Internal server error: An exception occurred while loading multimodal data: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto'): 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\",\"type\":\"InternalServerError\",\"param\":null,\"code\":500}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "curl_command = f\"\"\"\n",
    "curl -s http://localhost:{port}/v1/chat/completions \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{{\n",
    "    \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    \"messages\": [\n",
    "      {{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"What’s in this image?\"\n",
    "          }},\n",
    "          {{\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {{\n",
    "              \"url\": \"https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\"\n",
    "            }}\n",
    "          }}\n",
    "        ]\n",
    "      }}\n",
    "    ],\n",
    "    \"max_tokens\": 300\n",
    "  }}'\n",
    "\"\"\"\n",
    "\n",
    "response = subprocess.check_output(curl_command, shell=True).decode()\n",
    "print_highlight(response)\n",
    "\n",
    "\n",
    "response = subprocess.check_output(curl_command, shell=True).decode()\n",
    "print_highlight(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T12:39:51.926566Z",
     "iopub.status.busy": "2025-12-30T12:39:51.926409Z",
     "iopub.status.idle": "2025-12-30T12:39:54.005897Z",
     "shell.execute_reply": "2025-12-30T12:39:54.005209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"id\":\"efb96bebef1449a2a6ac89fae827b402\",\"object\":\"chat.completion\",\"created\":1767098394,\"model\":\"Qwen/Qwen2.5-VL-7B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The image shows a man standing on the back of a yellow taxi, using an iron to press or smooth out a pair of pants draped over an ironing board. The taxi is parked on a city street, and there are other taxis and buildings in the background. The man appears to be balancing carefully while performing this task.\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":151645}],\"usage\":{\"prompt_tokens\":307,\"total_tokens\":373,\"completion_tokens\":66,\"prompt_tokens_details\":null,\"reasoning_tokens\":0},\"metadata\":{\"weight_version\":\"default\"}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = f\"http://localhost:{port}/v1/chat/completions\"\n",
    "\n",
    "data = {\n",
    "    \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\"\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 300,\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T12:39:54.007626Z",
     "iopub.status.busy": "2025-12-30T12:39:54.007466Z",
     "iopub.status.idle": "2025-12-30T12:39:55.869626Z",
     "shell.execute_reply": "2025-12-30T12:39:55.868951Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-30 12:39:54] INFO _base_client.py:1071: Retrying request to /chat/completions in 0.427400 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 12:39:54] Error in request: An exception occurred while loading multimodal data: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto'): 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 406, in _load_single_item\n",
      "    img, _ = load_image(data)\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/utils/common.py\", line 859, in load_image\n",
      "    response.raise_for_status()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 623, in load_mm_data\n",
      "    result = next(futures_iter).result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 416, in _load_single_item\n",
      "    raise RuntimeError(f\"Error while loading data {data}: {e}\")\n",
      "RuntimeError: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto'): 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/entrypoints/openai/serving_base.py\", line 120, in handle_request\n",
      "    return await self._handle_non_streaming_request(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/entrypoints/openai/serving_chat.py\", line 742, in _handle_non_streaming_request\n",
      "    ret = await self.tokenizer_manager.generate_request(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 490, in generate_request\n",
      "    tokenized_obj = await self._tokenize_one_request(obj)\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 694, in _tokenize_one_request\n",
      "    mm_inputs: Dict = await self.mm_data_processor.process(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/managers/async_mm_data_processor.py\", line 99, in process\n",
      "    return await asyncio.wait_for(_invoke(), timeout=self.timeout_s)\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/managers/async_mm_data_processor.py\", line 70, in _invoke\n",
      "    return await self._proc_async(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/qwen_vl.py\", line 308, in process_mm_data_async\n",
      "    base_output = self.load_mm_data(\n",
      "  File \"/public_sglang_ci/runner-l3b-gpu-0/_work/sglang/sglang/python/sglang/srt/multimodal/processors/base_processor.py\", line 672, in load_mm_data\n",
      "    raise RuntimeError(\n",
      "RuntimeError: An exception occurred while loading multimodal data: Error while loading data ImageData(url='https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true', detail='auto'): 503 Server Error: Service Unavailable for url: https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The image shows a man standing on the back of a yellow taxi, using an iron to press or smooth out a pair of pants draped over an ironing board. The taxi is parked on a city street, and there are other taxis and buildings in the background. The man appears to be performing an unusual task, as ironing is typically done indoors.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=f\"http://localhost:{port}/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is in this image?\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\"\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "print_highlight(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple-Image Inputs\n",
    "\n",
    "The server also supports multiple images and interleaved text and images if the model supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T12:39:55.872592Z",
     "iopub.status.busy": "2025-12-30T12:39:55.872437Z",
     "iopub.status.idle": "2025-12-30T12:39:57.044211Z",
     "shell.execute_reply": "2025-12-30T12:39:57.043544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The first image shows a man ironing clothes on the back of a taxi in a busy urban street. The second image is a stylized logo featuring the letters \"SGL\" with a book and a computer icon incorporated into the design.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=f\"http://localhost:{port}/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/examples/assets/example_image.png?raw=true\",\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://raw.githubusercontent.com/sgl-project/sglang/main/assets/logo.png\",\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"I have two very different images. They are not related at all. \"\n",
    "                    \"Please describe the first image in one sentence, and then describe the second image in another sentence.\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print_highlight(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T12:39:57.045934Z",
     "iopub.status.busy": "2025-12-30T12:39:57.045771Z",
     "iopub.status.idle": "2025-12-30T12:39:57.061456Z",
     "shell.execute_reply": "2025-12-30T12:39:57.060790Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(vision_process)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
