{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGLang Native APIs\n",
    "\n",
    "Apart from the OpenAI compatible APIs, the SGLang Runtime also provides its native server APIs. We introduce the following APIs:\n",
    "\n",
    "- `/generate` (text generation model)\n",
    "- `/get_model_info`\n",
    "- `/get_server_info`\n",
    "- `/health`\n",
    "- `/health_generate`\n",
    "- `/flush_cache`\n",
    "- `/update_weights`\n",
    "- `/encode`(embedding model)\n",
    "- `/v1/rerank`(cross encoder rerank model)\n",
    "- `/classify`(reward model)\n",
    "- `/start_expert_distribution_record`\n",
    "- `/stop_expert_distribution_record`\n",
    "- `/dump_expert_distribution_record`\n",
    "- `/tokenize`\n",
    "- `/detokenize`\n",
    "- A full list of these APIs can be found at [http_server.py](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server.py)\n",
    "\n",
    "We mainly use `requests` to test these APIs in the following examples. You can also use `curl`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch A Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:20:08.594787Z",
     "iopub.status.busy": "2025-12-12T04:20:08.594662Z",
     "iopub.status.idle": "2025-12-12T04:20:49.295547Z",
     "shell.execute_reply": "2025-12-12T04:20:49.294897Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:14] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:14] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:14] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:23] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:20:23] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:20:23] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:25] WARNING server_args.py:1414: Attention backend not explicitly specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:33] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:20:33] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:20:33] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:33] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:20:33] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:20:33] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 04:20:37.192847 642416 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 04:20:37.192862 642416 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.184.242.156 port: 12001\n",
      "I1212 04:20:37.192883 642416 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.184.242.156:15382\n",
      "I1212 04:20:37.192950 642416 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 04:20:37.195573 642416 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 04:20:37.223616 642416 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 04:20:37.224211 642416 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:c2:71\n",
      "I1212 04:20:37.251571 642416 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 04:20:37.252132 642416 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:42:71\n",
      "I1212 04:20:37.275017 642416 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 04:20:37.275593 642416 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:b8:f2:9c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 04:20:37.612816 642416 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 04:20:37.613456 642416 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:42:71\n",
      "I1212 04:20:37.639595 642416 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 04:20:37.640156 642416 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:c2:71\n",
      "I1212 04:20:37.644873 642416 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 04:20:37.645447 642416 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:42:71\n",
      "I1212 04:20:37.671620 642416 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 04:20:37.672194 642416 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:c2:71\n",
      "I1212 04:20:37.699684 642416 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 04:20:37.700295 642416 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:42:71\n",
      "I1212 04:20:37.727682 642416 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 04:20:37.728273 642416 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:c2:71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 04:20:38.428251 642416 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f6cc7fff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:40] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=57.28 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=57.28 GB):  33%|███▎      | 1/3 [00:00<00:00,  3.60it/s]\r",
      "Capturing batches (bs=2 avail_mem=57.19 GB):  33%|███▎      | 1/3 [00:00<00:00,  3.60it/s]\r",
      "Capturing batches (bs=1 avail_mem=57.16 GB):  33%|███▎      | 1/3 [00:00<00:00,  3.60it/s]\r",
      "Capturing batches (bs=1 avail_mem=57.16 GB): 100%|██████████| 3/3 [00:00<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:44] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang.test.doc_patch import launch_server_cmd\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --host 0.0.0.0 --log-level warning\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate (text generation model)\n",
    "Generate completions. This is similar to the `/v1/completions` in OpenAI API. Detailed parameters can be found in the [sampling parameters](sampling_params.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:20:49.297929Z",
     "iopub.status.busy": "2025-12-12T04:20:49.297487Z",
     "iopub.status.idle": "2025-12-12T04:20:49.843606Z",
     "shell.execute_reply": "2025-12-12T04:20:49.842839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': ' 5.0分 - 百度知道\\n\\nThe capital of France is Paris. 题目解析：Paris is the capital of France. 5.0分\\n\\n（《世纪》）第 38 页，30 题，题意：太平洋和大西洋的周长是多少？ 30. 题目描述： 中国的面积是 30 亿平方千米, 澳大利亚的大洋是指厚约１ 300 毫米的大洋 ， 是两个相同大小的大陆，位于太平洋与大西洋之间 变形的', 'output_ids': [220, 20, 13, 15, 17177, 481, 68294, 122, 26381, 99392, 271, 785, 6722, 315, 9625, 374, 12095, 13, 18137, 95, 246, 29732, 106637, 5122, 59604, 374, 279, 6722, 315, 9625, 13, 220, 20, 13, 15, 17177, 271, 9909, 26940, 101186, 25067, 7552, 29437, 220, 18, 23, 220, 18538, 3837, 18, 15, 18137, 95, 246, 3837, 33872, 36589, 5122, 109068, 33108, 26288, 113455, 9370, 40542, 45861, 111558, 11319, 220, 18, 15, 13, 18137, 95, 246, 29732, 53481, 5122, 220, 105538, 100210, 20412, 220, 18, 15, 220, 53356, 100835, 112925, 11, 6567, 122, 111, 26288, 101523, 104197, 99840, 104442, 99696, 94237, 20109, 220, 18, 15, 15, 6567, 107, 104, 72261, 104197, 99840, 41175, 54851, 100369, 102486, 92032, 9370, 102412, 3837, 103987, 109068, 57218, 26288, 113455, 101920, 26853, 246, 82699, 9370], 'meta_info': {'id': '0c60dc06efcd4403af70147303effbe8', 'finish_reason': {'type': 'length', 'length': 128}, 'prompt_tokens': 7, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 128, 'cached_tokens': 0, 'e2e_latency': 0.5380630493164062, 'response_sent_to_client_ts': 1765513249.8393214}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = f\"http://localhost:{port}/generate\"\n",
    "data = {\"text\": \"What is the capital of France?\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Info\n",
    "\n",
    "Get the information of the model.\n",
    "\n",
    "- `model_path`: The path/name of the model.\n",
    "- `is_generation`: Whether the model is used as generation model or embedding model.\n",
    "- `tokenizer_path`: The path/name of the tokenizer.\n",
    "- `preferred_sampling_params`: The default sampling params specified via `--preferred-sampling-params`. `None` is returned in this example as we did not explicitly configure it in server args.\n",
    "- `weight_version`: This field contains the version of the model weights. This is often used to track changes or updates to the model’s trained parameters.\n",
    "- `has_image_understanding`: Whether the model has image-understanding capability.\n",
    "- `has_audio_understanding`: Whether the model has audio-understanding capability.\n",
    "- `model_type`: The model type from the HuggingFace config (e.g., \"qwen2\", \"llama\").\n",
    "- `architectures`: The model architectures from the HuggingFace config (e.g., [\"Qwen2ForCausalLM\"])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:20:49.845308Z",
     "iopub.status.busy": "2025-12-12T04:20:49.845134Z",
     "iopub.status.idle": "2025-12-12T04:20:49.851626Z",
     "shell.execute_reply": "2025-12-12T04:20:49.851086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:49] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'model_path': 'qwen/qwen2.5-0.5b-instruct', 'tokenizer_path': 'qwen/qwen2.5-0.5b-instruct', 'is_generation': True, 'preferred_sampling_params': None, 'weight_version': 'default', 'has_image_understanding': False, 'has_audio_understanding': False, 'model_type': 'qwen2', 'architectures': ['Qwen2ForCausalLM']}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/get_model_info\"\n",
    "\n",
    "response = requests.get(url)\n",
    "response_json = response.json()\n",
    "print_highlight(response_json)\n",
    "assert response_json[\"model_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\n",
    "assert response_json[\"is_generation\"] is True\n",
    "assert response_json[\"tokenizer_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\n",
    "assert response_json[\"preferred_sampling_params\"] is None\n",
    "assert response_json.keys() == {\n",
    "    \"model_path\",\n",
    "    \"is_generation\",\n",
    "    \"tokenizer_path\",\n",
    "    \"preferred_sampling_params\",\n",
    "    \"weight_version\",\n",
    "    \"has_image_understanding\",\n",
    "    \"has_audio_understanding\",\n",
    "    \"model_type\",\n",
    "    \"architectures\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Server Info\n",
    "Gets the server information including CLI arguments, token limits, and memory pool sizes.\n",
    "- Note: `get_server_info` merges the following deprecated endpoints:\n",
    "  - `get_server_args`\n",
    "  - `get_memory_pool_size` \n",
    "  - `get_max_total_num_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:20:49.853024Z",
     "iopub.status.busy": "2025-12-12T04:20:49.852784Z",
     "iopub.status.idle": "2025-12-12T04:20:49.862304Z",
     "shell.execute_reply": "2025-12-12T04:20:49.861790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:49] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"model_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_mode\":\"auto\",\"tokenizer_worker_num\":1,\"skip_tokenizer_init\":false,\"load_format\":\"auto\",\"model_loader_extra_config\":\"{}\",\"rl_quant_profile\":null,\"trust_remote_code\":false,\"context_length\":null,\"is_embedding\":false,\"enable_multimodal\":null,\"revision\":null,\"model_impl\":\"auto\",\"host\":\"0.0.0.0\",\"port\":30853,\"fastapi_root_path\":\"\",\"grpc_mode\":false,\"skip_server_warmup\":false,\"warmups\":null,\"nccl_port\":null,\"checkpoint_engine_wait_weights_before_ready\":false,\"dtype\":\"auto\",\"quantization\":null,\"quantization_param_path\":null,\"kv_cache_dtype\":\"auto\",\"enable_fp32_lm_head\":false,\"modelopt_quant\":null,\"modelopt_checkpoint_restore_path\":null,\"modelopt_checkpoint_save_path\":null,\"modelopt_export_path\":null,\"quantize_and_serve\":false,\"mem_fraction_static\":0.841,\"max_running_requests\":128,\"max_queued_requests\":null,\"max_total_tokens\":20480,\"chunked_prefill_size\":8192,\"max_prefill_tokens\":16384,\"schedule_policy\":\"fcfs\",\"enable_priority_scheduling\":false,\"abort_on_priority_when_disabled\":false,\"schedule_low_priority_values_first\":false,\"priority_scheduling_preemption_threshold\":10,\"schedule_conservativeness\":1.0,\"page_size\":1,\"hybrid_kvcache_ratio\":null,\"swa_full_tokens_ratio\":0.8,\"disable_hybrid_swa_memory\":false,\"radix_eviction_policy\":\"lru\",\"device\":\"cuda\",\"tp_size\":1,\"pp_size\":1,\"pp_max_micro_batch_size\":null,\"stream_interval\":1,\"stream_output\":false,\"random_seed\":813464405,\"constrained_json_whitespace_pattern\":null,\"constrained_json_disable_any_whitespace\":false,\"watchdog_timeout\":300,\"dist_timeout\":null,\"download_dir\":null,\"base_gpu_id\":0,\"gpu_id_step\":1,\"sleep_on_idle\":false,\"mm_process_config\":{},\"log_level\":\"warning\",\"log_level_http\":null,\"log_requests\":false,\"log_requests_level\":2,\"crash_dump_folder\":null,\"show_time_cost\":false,\"enable_metrics\":false,\"enable_metrics_for_all_schedulers\":false,\"tokenizer_metrics_custom_labels_header\":\"x-custom-labels\",\"tokenizer_metrics_allowed_custom_labels\":null,\"bucket_time_to_first_token\":null,\"bucket_inter_token_latency\":null,\"bucket_e2e_request_latency\":null,\"collect_tokens_histogram\":false,\"prompt_tokens_buckets\":null,\"generation_tokens_buckets\":null,\"gc_warning_threshold_secs\":0.0,\"decode_log_interval\":40,\"enable_request_time_stats_logging\":false,\"kv_events_config\":null,\"enable_trace\":false,\"otlp_traces_endpoint\":\"localhost:4317\",\"export_metrics_to_file\":false,\"export_metrics_to_file_dir\":null,\"api_key\":null,\"served_model_name\":\"qwen/qwen2.5-0.5b-instruct\",\"weight_version\":\"default\",\"chat_template\":null,\"completion_template\":null,\"file_storage_path\":\"sglang_storage\",\"enable_cache_report\":false,\"reasoning_parser\":null,\"tool_call_parser\":null,\"tool_server\":null,\"sampling_defaults\":\"model\",\"dp_size\":1,\"load_balance_method\":\"round_robin\",\"load_watch_interval\":0.1,\"prefill_round_robin_balance\":false,\"dist_init_addr\":null,\"nnodes\":1,\"node_rank\":0,\"json_model_override_args\":\"{}\",\"preferred_sampling_params\":null,\"enable_lora\":null,\"max_lora_rank\":null,\"lora_target_modules\":null,\"lora_paths\":null,\"max_loaded_loras\":null,\"max_loras_per_batch\":8,\"lora_eviction_policy\":\"lru\",\"lora_backend\":\"csgmv\",\"max_lora_chunk_size\":16,\"attention_backend\":\"fa3\",\"decode_attention_backend\":null,\"prefill_attention_backend\":null,\"sampling_backend\":\"flashinfer\",\"grammar_backend\":\"xgrammar\",\"mm_attention_backend\":null,\"fp8_gemm_runner_backend\":\"auto\",\"nsa_prefill_backend\":\"flashmla_sparse\",\"nsa_decode_backend\":\"fa3\",\"enable_flashinfer_autotune\":false,\"speculative_algorithm\":null,\"speculative_draft_model_path\":null,\"speculative_draft_model_revision\":null,\"speculative_draft_load_format\":null,\"speculative_num_steps\":null,\"speculative_eagle_topk\":null,\"speculative_num_draft_tokens\":null,\"speculative_accept_threshold_single\":1.0,\"speculative_accept_threshold_acc\":1.0,\"speculative_token_map\":null,\"speculative_attention_mode\":\"prefill\",\"speculative_moe_runner_backend\":null,\"speculative_moe_a2a_backend\":null,\"speculative_ngram_min_match_window_size\":1,\"speculative_ngram_max_match_window_size\":12,\"speculative_ngram_min_bfs_breadth\":1,\"speculative_ngram_max_bfs_breadth\":10,\"speculative_ngram_match_type\":\"BFS\",\"speculative_ngram_branch_length\":18,\"speculative_ngram_capacity\":10000000,\"ep_size\":1,\"moe_a2a_backend\":\"none\",\"moe_runner_backend\":\"auto\",\"flashinfer_mxfp4_moe_precision\":\"default\",\"enable_flashinfer_allreduce_fusion\":false,\"deepep_mode\":\"auto\",\"ep_num_redundant_experts\":0,\"ep_dispatch_algorithm\":null,\"init_expert_location\":\"trivial\",\"enable_eplb\":false,\"eplb_algorithm\":\"auto\",\"eplb_rebalance_num_iterations\":1000,\"eplb_rebalance_layers_per_chunk\":null,\"eplb_min_rebalancing_utilization_threshold\":1.0,\"expert_distribution_recorder_mode\":null,\"expert_distribution_recorder_buffer_size\":1000,\"enable_expert_distribution_metrics\":false,\"deepep_config\":null,\"moe_dense_tp_size\":null,\"elastic_ep_backend\":null,\"mooncake_ib_device\":null,\"max_mamba_cache_size\":null,\"mamba_ssm_dtype\":\"float32\",\"mamba_full_memory_ratio\":0.9,\"enable_hierarchical_cache\":false,\"hicache_ratio\":2.0,\"hicache_size\":0,\"hicache_write_policy\":\"write_through\",\"hicache_io_backend\":\"kernel\",\"hicache_mem_layout\":\"layer_first\",\"hicache_storage_backend\":null,\"hicache_storage_prefetch_policy\":\"best_effort\",\"hicache_storage_backend_extra_config\":null,\"enable_lmcache\":false,\"kt_weight_path\":null,\"kt_method\":\"AMXINT4\",\"kt_cpuinfer\":null,\"kt_threadpool_count\":2,\"kt_num_gpu_experts\":null,\"kt_max_deferred_experts_per_token\":null,\"dllm_algorithm\":null,\"dllm_algorithm_config\":null,\"enable_double_sparsity\":false,\"ds_channel_config_path\":null,\"ds_heavy_channel_num\":32,\"ds_heavy_token_num\":256,\"ds_heavy_channel_type\":\"qk\",\"ds_sparse_decode_threshold\":4096,\"cpu_offload_gb\":0,\"offload_group_size\":-1,\"offload_num_in_group\":1,\"offload_prefetch_step\":1,\"offload_mode\":\"cpu\",\"multi_item_scoring_delimiter\":null,\"disable_radix_cache\":false,\"cuda_graph_max_bs\":4,\"cuda_graph_bs\":[1,2,4],\"disable_cuda_graph\":false,\"disable_cuda_graph_padding\":false,\"enable_profile_cuda_graph\":false,\"enable_cudagraph_gc\":false,\"enable_layerwise_nvtx_marker\":false,\"enable_nccl_nvls\":false,\"enable_symm_mem\":false,\"disable_flashinfer_cutlass_moe_fp4_allgather\":false,\"enable_tokenizer_batch_encode\":false,\"disable_tokenizer_batch_decode\":false,\"disable_outlines_disk_cache\":false,\"disable_custom_all_reduce\":false,\"enable_mscclpp\":false,\"enable_torch_symm_mem\":false,\"disable_overlap_schedule\":false,\"enable_mixed_chunk\":false,\"enable_dp_attention\":false,\"enable_dp_lm_head\":false,\"enable_two_batch_overlap\":false,\"enable_single_batch_overlap\":false,\"tbo_token_distribution_threshold\":0.48,\"enable_torch_compile\":false,\"enable_piecewise_cuda_graph\":false,\"enable_torch_compile_debug_mode\":false,\"torch_compile_max_bs\":32,\"piecewise_cuda_graph_max_tokens\":4096,\"piecewise_cuda_graph_tokens\":[4,8,12,16,20,24,28,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256,288,320,352,384,416,448,480,512,640,768,896,1024,1152,1280,1408,1536,1664,1792,1920,2048,2176,2304,2432,2560,2688,2816,2944,3072,3200,3328,3456,3584,3712,3840,3968,4096],\"piecewise_cuda_graph_compiler\":\"eager\",\"torchao_config\":\"\",\"enable_nan_detection\":false,\"enable_p2p_check\":false,\"triton_attention_reduce_in_fp32\":false,\"triton_attention_num_kv_splits\":8,\"triton_attention_split_tile_size\":null,\"num_continuous_decode_steps\":1,\"delete_ckpt_after_loading\":false,\"enable_memory_saver\":false,\"enable_weights_cpu_backup\":false,\"enable_draft_weights_cpu_backup\":false,\"allow_auto_truncate\":false,\"enable_custom_logit_processor\":false,\"flashinfer_mla_disable_ragged\":false,\"disable_shared_experts_fusion\":false,\"disable_chunked_prefix_cache\":false,\"disable_fast_image_processor\":false,\"keep_mm_feature_on_device\":false,\"enable_return_hidden_states\":false,\"scheduler_recv_interval\":1,\"numa_node\":null,\"enable_deterministic_inference\":false,\"rl_on_policy_target\":null,\"enable_attn_tp_input_scattered\":false,\"enable_nsa_prefill_context_parallel\":false,\"enable_fused_qk_norm_rope\":false,\"enable_dynamic_batch_tokenizer\":false,\"dynamic_batch_tokenizer_batch_size\":32,\"dynamic_batch_tokenizer_batch_timeout\":0.002,\"debug_tensor_dump_output_folder\":null,\"debug_tensor_dump_layers\":null,\"debug_tensor_dump_input_file\":null,\"debug_tensor_dump_inject\":false,\"disaggregation_mode\":\"null\",\"disaggregation_transfer_backend\":\"mooncake\",\"disaggregation_bootstrap_port\":8998,\"disaggregation_decode_tp\":null,\"disaggregation_decode_dp\":null,\"disaggregation_prefill_pp\":1,\"disaggregation_ib_device\":null,\"disaggregation_decode_enable_offload_kvcache\":false,\"num_reserved_decode_tokens\":512,\"disaggregation_decode_polling_interval\":1,\"custom_weight_loader\":[],\"weight_loader_disable_mmap\":false,\"remote_instance_weight_loader_seed_instance_ip\":null,\"remote_instance_weight_loader_seed_instance_service_port\":null,\"remote_instance_weight_loader_send_weights_group_ports\":null,\"remote_instance_weight_loader_backend\":\"nccl\",\"remote_instance_weight_loader_support_transfer_engine\":true,\"enable_pdmux\":false,\"pdmux_config_path\":null,\"sm_group_num\":8,\"mm_max_concurrent_calls\":32,\"mm_per_request_timeout\":10.0,\"enable_broadcast_mm_inputs_process\":false,\"decrypted_config_file\":null,\"decrypted_draft_config_file\":null,\"mm_enable_dp_encoder\":false,\"forward_hooks\":null,\"status\":\"ready\",\"max_total_num_tokens\":20480,\"max_req_input_len\":20474,\"tp_rank\":0,\"remote_instance_transfer_engine_session_id\":\"10.184.242.156:15382\",\"remote_instance_transfer_engine_weights_info_dict\":{\"model.embed_tokens.weight\":[140103880015872,136134656,2],\"model.layers.0.self_attn.qkv_proj.weight\":[140104152645632,1032192,2],\"model.layers.0.self_attn.qkv_proj.bias\":[140104173617152,1152,2],\"model.layers.0.self_attn.o_proj.weight\":[140104154710016,802816,2],\"model.layers.0.mlp.gate_up_proj.weight\":[140116798472192,8716288,2],\"model.layers.0.mlp.down_proj.weight\":[140116983021568,4358144,2],\"model.layers.0.input_layernorm.weight\":[140104173619712,896,2],\"model.layers.0.post_attention_layernorm.weight\":[140104173621760,896,2],\"model.layers.1.self_attn.qkv_proj.weight\":[140116991737856,1032192,2],\"model.layers.1.self_attn.qkv_proj.bias\":[140104173623808,1152,2],\"model.layers.1.self_attn.o_proj.weight\":[140116993802240,802816,2],\"model.layers.1.mlp.gate_up_proj.weight\":[140116764917760,8716288,2],\"model.layers.1.mlp.down_proj.weight\":[140104156315648,4358144,2],\"model.layers.1.input_layernorm.weight\":[140104173626368,896,2],\"model.layers.1.post_attention_layernorm.weight\":[140104173628416,896,2],\"model.layers.2.self_attn.qkv_proj.weight\":[140104165031936,1032192,2],\"model.layers.2.self_attn.qkv_proj.bias\":[140104173630464,1152,2],\"model.layers.2.self_attn.o_proj.weight\":[140104167096320,802816,2],\"model.layers.2.mlp.gate_up_proj.weight\":[140116731363328,8716288,2],\"model.layers.2.mlp.down_proj.weight\":[140116697808896,4358144,2],\"model.layers.2.input_layernorm.weight\":[140104173633024,896,2],\"model.layers.2.post_attention_layernorm.weight\":[140104173635072,896,2],\"model.layers.3.self_attn.qkv_proj.weight\":[140104168701952,1032192,2],\"model.layers.3.self_attn.qkv_proj.bias\":[140104173637120,1152,2],\"model.layers.3.self_attn.o_proj.weight\":[140104170766336,802816,2],\"model.layers.3.mlp.gate_up_proj.weight\":[140116664254464,8716288,2],\"model.layers.3.mlp.down_proj.weight\":[140116706525184,4358144,2],\"model.layers.3.input_layernorm.weight\":[140104173639680,896,2],\"model.layers.3.post_attention_layernorm.weight\":[140104173641728,896,2],\"model.layers.4.self_attn.qkv_proj.weight\":[140116715241472,1032192,2],\"model.layers.4.self_attn.qkv_proj.bias\":[140104173643776,1152,2],\"model.layers.4.self_attn.o_proj.weight\":[140116630700032,802816,2],\"model.layers.4.mlp.gate_up_proj.weight\":[140116632305664,8716288,2],\"model.layers.4.mlp.down_proj.weight\":[140116597145600,4358144,2],\"model.layers.4.input_layernorm.weight\":[140104173646336,896,2],\"model.layers.4.post_attention_layernorm.weight\":[140104173648384,896,2],\"model.layers.5.self_attn.qkv_proj.weight\":[140116605861888,1032192,2],\"model.layers.5.self_attn.qkv_proj.bias\":[140104173650432,1152,2],\"model.layers.5.self_attn.o_proj.weight\":[140116649738240,802816,2],\"model.layers.5.mlp.gate_up_proj.weight\":[140116563591168,8716288,2],\"model.layers.5.mlp.down_proj.weight\":[140116607926272,4358144,2],\"model.layers.5.input_layernorm.weight\":[140104173652992,896,2],\"model.layers.5.post_attention_layernorm.weight\":[140104173655040,896,2],\"model.layers.6.self_attn.qkv_proj.weight\":[140116530036736,1032192,2],\"model.layers.6.self_attn.qkv_proj.bias\":[140104173657088,1152,2],\"model.layers.6.self_attn.o_proj.weight\":[140116532101120,802816,2],\"model.layers.6.mlp.gate_up_proj.weight\":[140116496482304,8716288,2],\"model.layers.6.mlp.down_proj.weight\":[140116533706752,4358144,2],\"model.layers.6.input_layernorm.weight\":[140104173659648,896,2],\"model.layers.6.post_attention_layernorm.weight\":[140104173661696,896,2],\"model.layers.7.self_attn.qkv_proj.weight\":[140116542423040,1032192,2],\"model.layers.7.self_attn.qkv_proj.bias\":[140104173663744,1152,2],\"model.layers.7.self_attn.o_proj.weight\":[140116544487424,802816,2],\"model.layers.7.mlp.gate_up_proj.weight\":[140115926056960,8716288,2],\"model.layers.7.mlp.down_proj.weight\":[140115724730368,4358144,2],\"model.layers.7.input_layernorm.weight\":[140104173666304,896,2],\"model.layers.7.post_attention_layernorm.weight\":[140104173668352,896,2],\"model.layers.8.self_attn.qkv_proj.weight\":[140116546093056,1032192,2],\"model.layers.8.self_attn.qkv_proj.bias\":[140104173670400,1152,2],\"model.layers.8.self_attn.o_proj.weight\":[140116548157440,802816,2],\"model.layers.8.mlp.gate_up_proj.weight\":[140112235069440,8716288,2],\"model.layers.8.mlp.down_proj.weight\":[140115733446656,4358144,2],\"model.layers.8.input_layernorm.weight\":[140104173672960,896,2],\"model.layers.8.post_attention_layernorm.weight\":[140104173675008,896,2],\"model.layers.9.self_attn.qkv_proj.weight\":[140115742162944,1032192,2],\"model.layers.9.self_attn.qkv_proj.bias\":[140104173677056,1152,2],\"model.layers.9.self_attn.o_proj.weight\":[140111765307392,802816,2],\"model.layers.9.mlp.gate_up_proj.weight\":[140111766913024,8716288,2],\"model.layers.9.mlp.down_proj.weight\":[140107436785664,4358144,2],\"model.layers.9.input_layernorm.weight\":[140104173679616,896,2],\"model.layers.9.post_attention_layernorm.weight\":[140104173681664,896,2],\"model.layers.10.self_attn.qkv_proj.weight\":[140107445501952,1032192,2],\"model.layers.10.self_attn.qkv_proj.bias\":[140104173683712,1152,2],\"model.layers.10.self_attn.o_proj.weight\":[140111784345600,802816,2],\"model.layers.10.mlp.gate_up_proj.weight\":[140103309590528,8716288,2],\"model.layers.10.mlp.down_proj.weight\":[140107447566336,4358144,2],\"model.layers.10.input_layernorm.weight\":[140104173686272,896,2],\"model.layers.10.post_attention_layernorm.weight\":[140104173688320,896,2],\"model.layers.11.self_attn.qkv_proj.weight\":[140103276036096,1032192,2],\"model.layers.11.self_attn.qkv_proj.bias\":[140104173690368,1152,2],\"model.layers.11.self_attn.o_proj.weight\":[140103278100480,802816,2],\"model.layers.11.mlp.gate_up_proj.weight\":[140103242481664,8716288,2],\"model.layers.11.mlp.down_proj.weight\":[140103279706112,4358144,2],\"model.layers.11.input_layernorm.weight\":[140104173692928,896,2],\"model.layers.11.post_attention_layernorm.weight\":[140104173694976,896,2],\"model.layers.12.self_attn.qkv_proj.weight\":[140103288422400,1032192,2],\"model.layers.12.self_attn.qkv_proj.bias\":[140104173697024,1152,2],\"model.layers.12.self_attn.o_proj.weight\":[140103290486784,802816,2],\"model.layers.12.mlp.gate_up_proj.weight\":[140103208927232,8716288,2],\"model.layers.12.mlp.down_proj.weight\":[140103175372800,4358144,2],\"model.layers.12.input_layernorm.weight\":[140104173699584,896,2],\"model.layers.12.post_attention_layernorm.weight\":[140104173701632,896,2],\"model.layers.13.self_attn.qkv_proj.weight\":[140103292092416,1032192,2],\"model.layers.13.self_attn.qkv_proj.bias\":[140104173703680,1152,2],\"model.layers.13.self_attn.o_proj.weight\":[140103294156800,802816,2],\"model.layers.13.mlp.gate_up_proj.weight\":[140103141818368,8716288,2],\"model.layers.13.mlp.down_proj.weight\":[140103184089088,4358144,2],\"model.layers.13.input_layernorm.weight\":[140104173706240,896,2],\"model.layers.13.post_attention_layernorm.weight\":[140104173708288,896,2],\"model.layers.14.self_attn.qkv_proj.weight\":[140103192805376,1032192,2],\"model.layers.14.self_attn.qkv_proj.bias\":[140104173710336,1152,2],\"model.layers.14.self_attn.o_proj.weight\":[140103108263936,802816,2],\"model.layers.14.mlp.gate_up_proj.weight\":[140103109869568,8716288,2],\"model.layers.14.mlp.down_proj.weight\":[140103074709504,4358144,2],\"model.layers.14.input_layernorm.weight\":[140104173712896,896,2],\"model.layers.14.post_attention_layernorm.weight\":[140104173714944,896,2],\"model.layers.15.self_attn.qkv_proj.weight\":[140103083425792,1032192,2],\"model.layers.15.self_attn.qkv_proj.bias\":[140104173716992,1152,2],\"model.layers.15.self_attn.o_proj.weight\":[140103127302144,802816,2],\"model.layers.15.mlp.gate_up_proj.weight\":[140103041155072,8716288,2],\"model.layers.15.mlp.down_proj.weight\":[140103085490176,4358144,2],\"model.layers.15.input_layernorm.weight\":[140104173719552,896,2],\"model.layers.15.post_attention_layernorm.weight\":[140104173721600,896,2],\"model.layers.16.self_attn.qkv_proj.weight\":[140103007600640,1032192,2],\"model.layers.16.self_attn.qkv_proj.bias\":[140104173723648,1152,2],\"model.layers.16.self_attn.o_proj.weight\":[140103009665024,802816,2],\"model.layers.16.mlp.gate_up_proj.weight\":[140102974046208,8716288,2],\"model.layers.16.mlp.down_proj.weight\":[140103011270656,4358144,2],\"model.layers.16.input_layernorm.weight\":[140104173726208,896,2],\"model.layers.16.post_attention_layernorm.weight\":[140104173728256,896,2],\"model.layers.17.self_attn.qkv_proj.weight\":[140103019986944,1032192,2],\"model.layers.17.self_attn.qkv_proj.bias\":[140104173730304,1152,2],\"model.layers.17.self_attn.o_proj.weight\":[140103022051328,802816,2],\"model.layers.17.mlp.gate_up_proj.weight\":[140102940491776,8716288,2],\"model.layers.17.mlp.down_proj.weight\":[140102906937344,4358144,2],\"model.layers.17.input_layernorm.weight\":[140104173732864,896,2],\"model.layers.17.post_attention_layernorm.weight\":[140104173734912,896,2],\"model.layers.18.self_attn.qkv_proj.weight\":[140103023656960,1032192,2],\"model.layers.18.self_attn.qkv_proj.bias\":[140104173736960,1152,2],\"model.layers.18.self_attn.o_proj.weight\":[140103025721344,802816,2],\"model.layers.18.mlp.gate_up_proj.weight\":[140102873382912,8716288,2],\"model.layers.18.mlp.down_proj.weight\":[140102915653632,4358144,2],\"model.layers.18.input_layernorm.weight\":[140104173739520,896,2],\"model.layers.18.post_attention_layernorm.weight\":[140104173741568,896,2],\"model.layers.19.self_attn.qkv_proj.weight\":[140102924369920,1032192,2],\"model.layers.19.self_attn.qkv_proj.bias\":[140104173743616,1152,2],\"model.layers.19.self_attn.o_proj.weight\":[140102839828480,802816,2],\"model.layers.19.mlp.gate_up_proj.weight\":[140102841434112,8716288,2],\"model.layers.19.mlp.down_proj.weight\":[140102806274048,4358144,2],\"model.layers.19.input_layernorm.weight\":[140104173746176,896,2],\"model.layers.19.post_attention_layernorm.weight\":[140104173748224,896,2],\"model.layers.20.self_attn.qkv_proj.weight\":[140102814990336,1032192,2],\"model.layers.20.self_attn.qkv_proj.bias\":[140104173750272,1152,2],\"model.layers.20.self_attn.o_proj.weight\":[140102858866688,802816,2],\"model.layers.20.mlp.gate_up_proj.weight\":[140102772719616,8716288,2],\"model.layers.20.mlp.down_proj.weight\":[140102817054720,4358144,2],\"model.layers.20.input_layernorm.weight\":[140104173752832,896,2],\"model.layers.20.post_attention_layernorm.weight\":[140104173754880,896,2],\"model.layers.21.self_attn.qkv_proj.weight\":[140102739165184,1032192,2],\"model.layers.21.self_attn.qkv_proj.bias\":[140104173756928,1152,2],\"model.layers.21.self_attn.o_proj.weight\":[140102741229568,802816,2],\"model.layers.21.mlp.gate_up_proj.weight\":[140102705610752,8716288,2],\"model.layers.21.mlp.down_proj.weight\":[140102742835200,4358144,2],\"model.layers.21.input_layernorm.weight\":[140104173759488,896,2],\"model.layers.21.post_attention_layernorm.weight\":[140104173761536,896,2],\"model.layers.22.self_attn.qkv_proj.weight\":[140102751551488,1032192,2],\"model.layers.22.self_attn.qkv_proj.bias\":[140104173763584,1152,2],\"model.layers.22.self_attn.o_proj.weight\":[140102753615872,802816,2],\"model.layers.22.mlp.gate_up_proj.weight\":[140102672056320,8716288,2],\"model.layers.22.mlp.down_proj.weight\":[140102638501888,4358144,2],\"model.layers.22.input_layernorm.weight\":[140104173766144,896,2],\"model.layers.22.post_attention_layernorm.weight\":[140104173768192,896,2],\"model.layers.23.self_attn.qkv_proj.weight\":[140102755221504,1032192,2],\"model.layers.23.self_attn.qkv_proj.bias\":[140104173770240,1152,2],\"model.layers.23.self_attn.o_proj.weight\":[140102757285888,802816,2],\"model.layers.23.mlp.gate_up_proj.weight\":[140102604947456,8716288,2],\"model.layers.23.mlp.down_proj.weight\":[140102647218176,4358144,2],\"model.layers.23.input_layernorm.weight\":[140104173772800,896,2],\"model.layers.23.post_attention_layernorm.weight\":[140104173774848,896,2],\"model.norm.weight\":[140104173776896,896,2]},\"internal_states\":[{\"model_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_path\":\"qwen/qwen2.5-0.5b-instruct\",\"tokenizer_mode\":\"auto\",\"tokenizer_worker_num\":1,\"skip_tokenizer_init\":false,\"load_format\":\"auto\",\"model_loader_extra_config\":\"{}\",\"rl_quant_profile\":null,\"trust_remote_code\":false,\"context_length\":null,\"is_embedding\":false,\"enable_multimodal\":null,\"revision\":null,\"model_impl\":\"auto\",\"host\":\"0.0.0.0\",\"port\":30853,\"fastapi_root_path\":\"\",\"grpc_mode\":false,\"skip_server_warmup\":false,\"warmups\":null,\"nccl_port\":null,\"checkpoint_engine_wait_weights_before_ready\":false,\"dtype\":\"auto\",\"quantization\":null,\"quantization_param_path\":null,\"kv_cache_dtype\":\"auto\",\"enable_fp32_lm_head\":false,\"modelopt_quant\":null,\"modelopt_checkpoint_restore_path\":null,\"modelopt_checkpoint_save_path\":null,\"modelopt_export_path\":null,\"quantize_and_serve\":false,\"mem_fraction_static\":0.841,\"max_running_requests\":128,\"max_queued_requests\":null,\"max_total_tokens\":20480,\"chunked_prefill_size\":8192,\"max_prefill_tokens\":16384,\"schedule_policy\":\"fcfs\",\"enable_priority_scheduling\":false,\"abort_on_priority_when_disabled\":false,\"schedule_low_priority_values_first\":false,\"priority_scheduling_preemption_threshold\":10,\"schedule_conservativeness\":1.0,\"page_size\":1,\"hybrid_kvcache_ratio\":null,\"swa_full_tokens_ratio\":0.8,\"disable_hybrid_swa_memory\":false,\"radix_eviction_policy\":\"lru\",\"device\":\"cuda\",\"tp_size\":1,\"pp_size\":1,\"pp_max_micro_batch_size\":128,\"stream_interval\":1,\"stream_output\":false,\"random_seed\":813464405,\"constrained_json_whitespace_pattern\":null,\"constrained_json_disable_any_whitespace\":false,\"watchdog_timeout\":300,\"dist_timeout\":null,\"download_dir\":null,\"base_gpu_id\":0,\"gpu_id_step\":1,\"sleep_on_idle\":false,\"mm_process_config\":{},\"log_level\":\"warning\",\"log_level_http\":null,\"log_requests\":false,\"log_requests_level\":2,\"crash_dump_folder\":null,\"show_time_cost\":false,\"enable_metrics\":false,\"enable_metrics_for_all_schedulers\":false,\"tokenizer_metrics_custom_labels_header\":\"x-custom-labels\",\"tokenizer_metrics_allowed_custom_labels\":null,\"bucket_time_to_first_token\":null,\"bucket_inter_token_latency\":null,\"bucket_e2e_request_latency\":null,\"collect_tokens_histogram\":false,\"prompt_tokens_buckets\":null,\"generation_tokens_buckets\":null,\"gc_warning_threshold_secs\":0.0,\"decode_log_interval\":40,\"enable_request_time_stats_logging\":false,\"kv_events_config\":null,\"enable_trace\":false,\"otlp_traces_endpoint\":\"localhost:4317\",\"export_metrics_to_file\":false,\"export_metrics_to_file_dir\":null,\"api_key\":null,\"served_model_name\":\"qwen/qwen2.5-0.5b-instruct\",\"weight_version\":\"default\",\"chat_template\":null,\"completion_template\":null,\"file_storage_path\":\"sglang_storage\",\"enable_cache_report\":false,\"reasoning_parser\":null,\"tool_call_parser\":null,\"tool_server\":null,\"sampling_defaults\":\"model\",\"dp_size\":1,\"load_balance_method\":\"round_robin\",\"load_watch_interval\":0.1,\"prefill_round_robin_balance\":false,\"dist_init_addr\":null,\"nnodes\":1,\"node_rank\":0,\"json_model_override_args\":\"{}\",\"preferred_sampling_params\":null,\"enable_lora\":null,\"max_lora_rank\":null,\"lora_target_modules\":null,\"lora_paths\":null,\"max_loaded_loras\":null,\"max_loras_per_batch\":8,\"lora_eviction_policy\":\"lru\",\"lora_backend\":\"csgmv\",\"max_lora_chunk_size\":16,\"attention_backend\":\"fa3\",\"decode_attention_backend\":\"fa3\",\"prefill_attention_backend\":\"fa3\",\"sampling_backend\":\"flashinfer\",\"grammar_backend\":\"xgrammar\",\"mm_attention_backend\":null,\"fp8_gemm_runner_backend\":\"auto\",\"nsa_prefill_backend\":\"flashmla_sparse\",\"nsa_decode_backend\":\"fa3\",\"enable_flashinfer_autotune\":false,\"speculative_algorithm\":null,\"speculative_draft_model_path\":null,\"speculative_draft_model_revision\":null,\"speculative_draft_load_format\":null,\"speculative_num_steps\":null,\"speculative_eagle_topk\":null,\"speculative_num_draft_tokens\":null,\"speculative_accept_threshold_single\":1.0,\"speculative_accept_threshold_acc\":1.0,\"speculative_token_map\":null,\"speculative_attention_mode\":\"prefill\",\"speculative_moe_runner_backend\":null,\"speculative_moe_a2a_backend\":null,\"speculative_ngram_min_match_window_size\":1,\"speculative_ngram_max_match_window_size\":12,\"speculative_ngram_min_bfs_breadth\":1,\"speculative_ngram_max_bfs_breadth\":10,\"speculative_ngram_match_type\":\"BFS\",\"speculative_ngram_branch_length\":18,\"speculative_ngram_capacity\":10000000,\"ep_size\":1,\"moe_a2a_backend\":\"none\",\"moe_runner_backend\":\"auto\",\"flashinfer_mxfp4_moe_precision\":\"default\",\"enable_flashinfer_allreduce_fusion\":false,\"deepep_mode\":\"auto\",\"ep_num_redundant_experts\":0,\"ep_dispatch_algorithm\":null,\"init_expert_location\":\"trivial\",\"enable_eplb\":false,\"eplb_algorithm\":\"auto\",\"eplb_rebalance_num_iterations\":1000,\"eplb_rebalance_layers_per_chunk\":null,\"eplb_min_rebalancing_utilization_threshold\":1.0,\"expert_distribution_recorder_mode\":null,\"expert_distribution_recorder_buffer_size\":1000,\"enable_expert_distribution_metrics\":false,\"deepep_config\":null,\"moe_dense_tp_size\":null,\"elastic_ep_backend\":null,\"mooncake_ib_device\":null,\"max_mamba_cache_size\":null,\"mamba_ssm_dtype\":\"float32\",\"mamba_full_memory_ratio\":0.9,\"enable_hierarchical_cache\":false,\"hicache_ratio\":2.0,\"hicache_size\":0,\"hicache_write_policy\":\"write_through\",\"hicache_io_backend\":\"kernel\",\"hicache_mem_layout\":\"layer_first\",\"hicache_storage_backend\":null,\"hicache_storage_prefetch_policy\":\"best_effort\",\"hicache_storage_backend_extra_config\":null,\"enable_lmcache\":false,\"kt_weight_path\":null,\"kt_method\":\"AMXINT4\",\"kt_cpuinfer\":null,\"kt_threadpool_count\":2,\"kt_num_gpu_experts\":null,\"kt_max_deferred_experts_per_token\":null,\"dllm_algorithm\":null,\"dllm_algorithm_config\":null,\"enable_double_sparsity\":false,\"ds_channel_config_path\":null,\"ds_heavy_channel_num\":32,\"ds_heavy_token_num\":256,\"ds_heavy_channel_type\":\"qk\",\"ds_sparse_decode_threshold\":4096,\"cpu_offload_gb\":0,\"offload_group_size\":-1,\"offload_num_in_group\":1,\"offload_prefetch_step\":1,\"offload_mode\":\"cpu\",\"multi_item_scoring_delimiter\":null,\"disable_radix_cache\":false,\"cuda_graph_max_bs\":4,\"cuda_graph_bs\":[1,2,4],\"disable_cuda_graph\":false,\"disable_cuda_graph_padding\":false,\"enable_profile_cuda_graph\":false,\"enable_cudagraph_gc\":false,\"enable_layerwise_nvtx_marker\":false,\"enable_nccl_nvls\":false,\"enable_symm_mem\":false,\"disable_flashinfer_cutlass_moe_fp4_allgather\":false,\"enable_tokenizer_batch_encode\":false,\"disable_tokenizer_batch_decode\":false,\"disable_outlines_disk_cache\":false,\"disable_custom_all_reduce\":false,\"enable_mscclpp\":false,\"enable_torch_symm_mem\":false,\"disable_overlap_schedule\":false,\"enable_mixed_chunk\":false,\"enable_dp_attention\":false,\"enable_dp_lm_head\":false,\"enable_two_batch_overlap\":false,\"enable_single_batch_overlap\":false,\"tbo_token_distribution_threshold\":0.48,\"enable_torch_compile\":false,\"enable_piecewise_cuda_graph\":false,\"enable_torch_compile_debug_mode\":false,\"torch_compile_max_bs\":32,\"piecewise_cuda_graph_max_tokens\":4096,\"piecewise_cuda_graph_tokens\":[4,8,12,16,20,24,28,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256,288,320,352,384,416,448,480,512,640,768,896,1024,1152,1280,1408,1536,1664,1792,1920,2048,2176,2304,2432,2560,2688,2816,2944,3072,3200,3328,3456,3584,3712,3840,3968,4096],\"piecewise_cuda_graph_compiler\":\"eager\",\"torchao_config\":\"\",\"enable_nan_detection\":false,\"enable_p2p_check\":false,\"triton_attention_reduce_in_fp32\":false,\"triton_attention_num_kv_splits\":8,\"triton_attention_split_tile_size\":null,\"num_continuous_decode_steps\":1,\"delete_ckpt_after_loading\":false,\"enable_memory_saver\":false,\"enable_weights_cpu_backup\":false,\"enable_draft_weights_cpu_backup\":false,\"allow_auto_truncate\":false,\"enable_custom_logit_processor\":false,\"flashinfer_mla_disable_ragged\":false,\"disable_shared_experts_fusion\":false,\"disable_chunked_prefix_cache\":true,\"disable_fast_image_processor\":false,\"keep_mm_feature_on_device\":false,\"enable_return_hidden_states\":false,\"scheduler_recv_interval\":1,\"numa_node\":null,\"enable_deterministic_inference\":false,\"rl_on_policy_target\":null,\"enable_attn_tp_input_scattered\":false,\"enable_nsa_prefill_context_parallel\":false,\"enable_fused_qk_norm_rope\":false,\"enable_dynamic_batch_tokenizer\":false,\"dynamic_batch_tokenizer_batch_size\":32,\"dynamic_batch_tokenizer_batch_timeout\":0.002,\"debug_tensor_dump_output_folder\":null,\"debug_tensor_dump_layers\":null,\"debug_tensor_dump_input_file\":null,\"debug_tensor_dump_inject\":false,\"disaggregation_mode\":\"null\",\"disaggregation_transfer_backend\":\"mooncake\",\"disaggregation_bootstrap_port\":8998,\"disaggregation_decode_tp\":null,\"disaggregation_decode_dp\":null,\"disaggregation_prefill_pp\":1,\"disaggregation_ib_device\":null,\"disaggregation_decode_enable_offload_kvcache\":false,\"num_reserved_decode_tokens\":512,\"disaggregation_decode_polling_interval\":1,\"custom_weight_loader\":[],\"weight_loader_disable_mmap\":false,\"remote_instance_weight_loader_seed_instance_ip\":null,\"remote_instance_weight_loader_seed_instance_service_port\":null,\"remote_instance_weight_loader_send_weights_group_ports\":null,\"remote_instance_weight_loader_backend\":\"nccl\",\"remote_instance_weight_loader_support_transfer_engine\":true,\"enable_pdmux\":false,\"pdmux_config_path\":null,\"sm_group_num\":8,\"mm_max_concurrent_calls\":32,\"mm_per_request_timeout\":10.0,\"enable_broadcast_mm_inputs_process\":false,\"decrypted_config_file\":null,\"decrypted_draft_config_file\":null,\"mm_enable_dp_encoder\":false,\"forward_hooks\":null,\"use_mla_backend\":false,\"last_gen_throughput\":307.86772124833726,\"memory_usage\":{\"weight\":1.66,\"kvcache\":0.23,\"token_capacity\":20480,\"graph\":0.2}}],\"version\":\"0.5.6.post2\"}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/get_server_info\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Check\n",
    "- `/health`: Check the health of the server.\n",
    "- `/health_generate`: Check the health of the server by generating one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:20:49.864228Z",
     "iopub.status.busy": "2025-12-12T04:20:49.864090Z",
     "iopub.status.idle": "2025-12-12T04:20:50.870374Z",
     "shell.execute_reply": "2025-12-12T04:20:50.869868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/health_generate\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:20:50.871771Z",
     "iopub.status.busy": "2025-12-12T04:20:50.871629Z",
     "iopub.status.idle": "2025-12-12T04:20:51.878016Z",
     "shell.execute_reply": "2025-12-12T04:20:51.877389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/health\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flush Cache\n",
    "\n",
    "Flush the radix cache. It will be automatically triggered when the model weights are updated by the `/update_weights` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:20:51.879746Z",
     "iopub.status.busy": "2025-12-12T04:20:51.879589Z",
     "iopub.status.idle": "2025-12-12T04:20:51.888358Z",
     "shell.execute_reply": "2025-12-12T04:20:51.887837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Cache flushed.<br>Please check backend logs for more details. (When there are running or waiting requests, the operation will not be performed.)<br></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"http://localhost:{port}/flush_cache\"\n",
    "\n",
    "response = requests.post(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Weights From Disk\n",
    "\n",
    "Update model weights from disk without restarting the server. Only applicable for models with the same architecture and parameter size.\n",
    "\n",
    "SGLang support `update_weights_from_disk` API for continuous evaluation during training (save checkpoint to disk and update weights from disk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:20:51.889693Z",
     "iopub.status.busy": "2025-12-12T04:20:51.889553Z",
     "iopub.status.idle": "2025-12-12T04:20:52.453054Z",
     "shell.execute_reply": "2025-12-12T04:20:52.452406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.40it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.40it/s]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"success\":true,\"message\":\"Succeeded to update model weights.\",\"num_paused_requests\":0}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# successful update with same architecture and size\n",
    "\n",
    "url = f\"http://localhost:{port}/update_weights_from_disk\"\n",
    "data = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.text)\n",
    "assert response.json()[\"success\"] is True\n",
    "assert response.json()[\"message\"] == \"Succeeded to update model weights.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:20:52.454565Z",
     "iopub.status.busy": "2025-12-12T04:20:52.454412Z",
     "iopub.status.idle": "2025-12-12T04:20:52.576877Z",
     "shell.execute_reply": "2025-12-12T04:20:52.576399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:52] Failed to get weights iterator: qwen/qwen2.5-0.5b-instruct-wrong (repository not found).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'success': False, 'message': 'Failed to get weights iterator: qwen/qwen2.5-0.5b-instruct-wrong (repository not found).', 'num_paused_requests': 0}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# failed update with different parameter size or wrong name\n",
    "\n",
    "url = f\"http://localhost:{port}/update_weights_from_disk\"\n",
    "data = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct-wrong\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "print_highlight(response_json)\n",
    "assert response_json[\"success\"] is False\n",
    "assert response_json[\"message\"] == (\n",
    "    \"Failed to get weights iterator: \"\n",
    "    \"qwen/qwen2.5-0.5b-instruct-wrong\"\n",
    "    \" (repository not found).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:20:52.578370Z",
     "iopub.status.busy": "2025-12-12T04:20:52.578220Z",
     "iopub.status.idle": "2025-12-12T04:20:52.594597Z",
     "shell.execute_reply": "2025-12-12T04:20:52.593891Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode (embedding model)\n",
    "\n",
    "Encode text into embeddings. Note that this API is only available for [embedding models](openai_api_embeddings.ipynb) and will raise an error for generation models.\n",
    "Therefore, we launch a new server to server an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:20:52.596642Z",
     "iopub.status.busy": "2025-12-12T04:20:52.596484Z",
     "iopub.status.idle": "2025-12-12T04:21:23.672139Z",
     "shell.execute_reply": "2025-12-12T04:21:23.671245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:57] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:20:57] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:20:57] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:20:59] INFO model_config.py:913: Downcasting torch.float32 to torch.float16.\n",
      "[2025-12-12 04:20:59] WARNING server_args.py:1414: Attention backend not explicitly specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:21:06] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:21:06] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:21:06] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 04:21:06] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:21:06] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:21:06] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 04:21:10.356712 644152 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 04:21:10.356731 644152 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.184.242.156 port: 12001\n",
      "I1212 04:21:10.356753 644152 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.184.242.156:16588\n",
      "I1212 04:21:10.356828 644152 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 04:21:10.359642 644152 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 04:21:10.387849 644152 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 04:21:10.388520 644152 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:c2:71\n",
      "I1212 04:21:10.415782 644152 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 04:21:10.416415 644152 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:42:71\n",
      "I1212 04:21:10.444136 644152 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 04:21:10.444989 644152 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:b8:f2:9c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 04:21:10.680810 644152 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 04:21:10.681450 644152 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:42:71\n",
      "I1212 04:21:10.687594 644152 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 04:21:10.688200 644152 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:c2:71\n",
      "I1212 04:21:10.715751 644152 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 04:21:10.716362 644152 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:42:71\n",
      "I1212 04:21:10.739760 644152 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 04:21:10.740384 644152 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:c2:71\n",
      "I1212 04:21:10.767760 644152 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 04:21:10.768371 644152 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:42:71\n",
      "I1212 04:21:10.795732 644152 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 04:21:10.796365 644152 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:c2:71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 04:21:11.503931 644152 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f5513fff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:21:13] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.17s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:21:19] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-1.5B-instruct \\\n",
    "    --host 0.0.0.0 --is-embedding --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:21:23.674664Z",
     "iopub.status.busy": "2025-12-12T04:21:23.674487Z",
     "iopub.status.idle": "2025-12-12T04:21:23.696306Z",
     "shell.execute_reply": "2025-12-12T04:21:23.695757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Text embedding (first 10): [-0.00023102760314941406, -0.04986572265625, -0.0032711029052734375, 0.011077880859375, -0.0140533447265625, 0.0159912109375, -0.01441192626953125, 0.0059051513671875, -0.0228424072265625, 0.0272979736328125]</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# successful encode for embedding model\n",
    "\n",
    "url = f\"http://localhost:{port}/encode\"\n",
    "data = {\"model\": \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", \"text\": \"Once upon a time\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "print_highlight(f\"Text embedding (first 10): {response_json['embedding'][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:21:23.704617Z",
     "iopub.status.busy": "2025-12-12T04:21:23.704457Z",
     "iopub.status.idle": "2025-12-12T04:21:23.713951Z",
     "shell.execute_reply": "2025-12-12T04:21:23.713406Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(embedding_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1/rerank (cross encoder rerank model)\n",
    "Rerank a list of documents given a query using a cross-encoder model. Note that this API is only available for cross encoder model like [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) with `attention-backend` `triton` and `torch_native`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:21:23.715808Z",
     "iopub.status.busy": "2025-12-12T04:21:23.715651Z",
     "iopub.status.idle": "2025-12-12T04:21:52.779698Z",
     "shell.execute_reply": "2025-12-12T04:21:52.778912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:21:29] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:21:29] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:21:29] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:21:31] INFO model_config.py:913: Downcasting torch.float32 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:21:37] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:21:37] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:21:37] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:21:37] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:21:37] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 04:21:37] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 04:21:40.947930 646153 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 04:21:40.947950 646153 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.184.242.156 port: 12001\n",
      "I1212 04:21:40.947970 646153 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.184.242.156:16187\n",
      "I1212 04:21:40.948040 646153 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 04:21:40.950951 646153 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 04:21:40.975925 646153 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 04:21:40.976769 646153 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:c2:71\n",
      "I1212 04:21:41.007884 646153 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 04:21:41.008700 646153 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:42:71\n",
      "I1212 04:21:41.036585 646153 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 04:21:41.037748 646153 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:b8:f2:9c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 04:21:41.212809 646153 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 04:21:41.213631 646153 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:42:71\n",
      "I1212 04:21:41.239900 646153 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 04:21:41.240727 646153 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:c2:71\n",
      "I1212 04:21:41.267840 646153 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 04:21:41.268656 646153 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:42:71\n",
      "I1212 04:21:41.291829 646153 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 04:21:41.292624 646153 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:c2:71\n",
      "I1212 04:21:41.319849 646153 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 04:21:41.320657 646153 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:42:71\n",
      "I1212 04:21:41.347899 646153 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 04:21:41.348748 646153 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:c2:71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 04:21:42.128109 646153 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f675bfff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:21:44] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.71it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.71it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:21:48] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reranker_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path BAAI/bge-reranker-v2-m3 \\\n",
    "    --host 0.0.0.0 --disable-radix-cache --chunked-prefill-size -1 --attention-backend triton --is-embedding --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:21:52.781638Z",
     "iopub.status.busy": "2025-12-12T04:21:52.781470Z",
     "iopub.status.idle": "2025-12-12T04:21:52.862286Z",
     "shell.execute_reply": "2025-12-12T04:21:52.861631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Score: 5.26 - Document: 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Score: -8.19 - Document: 'hi'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute rerank scores for query and documents\n",
    "\n",
    "url = f\"http://localhost:{port}/v1/rerank\"\n",
    "data = {\n",
    "    \"model\": \"BAAI/bge-reranker-v2-m3\",\n",
    "    \"query\": \"what is panda?\",\n",
    "    \"documents\": [\n",
    "        \"hi\",\n",
    "        \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "for item in response_json:\n",
    "    print_highlight(f\"Score: {item['score']:.2f} - Document: '{item['document']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:21:52.863768Z",
     "iopub.status.busy": "2025-12-12T04:21:52.863619Z",
     "iopub.status.idle": "2025-12-12T04:21:52.871583Z",
     "shell.execute_reply": "2025-12-12T04:21:52.870918Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(reranker_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify (reward model)\n",
    "\n",
    "SGLang Runtime also supports reward models. Here we use a reward model to classify the quality of pairwise generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:21:52.874015Z",
     "iopub.status.busy": "2025-12-12T04:21:52.873862Z",
     "iopub.status.idle": "2025-12-12T04:22:23.943593Z",
     "shell.execute_reply": "2025-12-12T04:22:23.942893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:21:58] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:21:58] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:21:58] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:22:00] WARNING server_args.py:1414: Attention backend not explicitly specified. Use flashinfer backend by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:22:05] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:22:05] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:22:05] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:22:05] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 04:22:05] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:22:05] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 04:22:08.516755 647767 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 04:22:08.516772 647767 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.184.242.156 port: 12001\n",
      "I1212 04:22:08.516808 647767 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.184.242.156:15172\n",
      "I1212 04:22:08.516896 647767 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 04:22:08.519812 647767 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 04:22:08.548719 647767 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 04:22:08.549461 647767 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:c2:71\n",
      "I1212 04:22:08.575778 647767 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 04:22:08.576467 647767 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:42:71\n",
      "I1212 04:22:08.604467 647767 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 04:22:08.605491 647767 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:b8:f2:9c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 04:22:08.724835 647767 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 04:22:08.740823 647767 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:42:71\n",
      "I1212 04:22:08.767944 647767 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 04:22:08.768561 647767 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:c2:71\n",
      "I1212 04:22:08.792191 647767 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 04:22:08.792970 647767 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:42:71\n",
      "I1212 04:22:08.797369 647767 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 04:22:08.798058 647767 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:c2:71\n",
      "I1212 04:22:08.802609 647767 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 04:22:08.803355 647767 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:42:71\n",
      "I1212 04:22:08.817159 647767 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 04:22:08.817879 647767 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:c2:71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 04:22:09.446904 647767 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7fe40fffc010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:22:11] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.52it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.32it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:22:19] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that SGLang now treats embedding models and reward models as the same type of models.\n",
    "# This will be updated in the future.\n",
    "\n",
    "reward_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path Skywork/Skywork-Reward-Llama-3.1-8B-v0.2 --host 0.0.0.0 --is-embedding --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:22:23.945531Z",
     "iopub.status.busy": "2025-12-12T04:22:23.945256Z",
     "iopub.status.idle": "2025-12-12T04:22:24.529213Z",
     "shell.execute_reply": "2025-12-12T04:22:24.528530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reward: -24.125</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reward: 1.0703125</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "PROMPT = (\n",
    "    \"What is the range of the numeric output of a sigmoid node in a neural network?\"\n",
    ")\n",
    "\n",
    "RESPONSE1 = \"The output of a sigmoid node is bounded between -1 and 1.\"\n",
    "RESPONSE2 = \"The output of a sigmoid node is bounded between 0 and 1.\"\n",
    "\n",
    "CONVS = [\n",
    "    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", \"content\": RESPONSE1}],\n",
    "    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", \"content\": RESPONSE2}],\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\")\n",
    "prompts = tokenizer.apply_chat_template(CONVS, tokenize=False, return_dict=False)\n",
    "\n",
    "url = f\"http://localhost:{port}/classify\"\n",
    "data = {\"model\": \"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\", \"text\": prompts}\n",
    "\n",
    "responses = requests.post(url, json=data).json()\n",
    "for response in responses:\n",
    "    print_highlight(f\"reward: {response['embedding'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:22:24.530816Z",
     "iopub.status.busy": "2025-12-12T04:22:24.530652Z",
     "iopub.status.idle": "2025-12-12T04:22:24.538233Z",
     "shell.execute_reply": "2025-12-12T04:22:24.537646Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(reward_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture expert selection distribution in MoE models\n",
    "\n",
    "SGLang Runtime supports recording the number of times an expert is selected in a MoE model run for each expert in the model. This is useful when analyzing the throughput of the model and plan for optimization.\n",
    "\n",
    "*Note: We only print out the first 10 lines of the csv below for better readability. Please adjust accordingly if you want to analyze the results more deeply.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:22:24.539662Z",
     "iopub.status.busy": "2025-12-12T04:22:24.539517Z",
     "iopub.status.idle": "2025-12-12T04:23:13.642730Z",
     "shell.execute_reply": "2025-12-12T04:23:13.642016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:22:37] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:22:37] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:22:37] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:22:39] WARNING server_args.py:1414: Attention backend not explicitly specified. Use flashinfer backend by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:22:44] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:22:44] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:22:44] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:22:44] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:22:44] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 04:22:44] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 04:22:47.407020 649412 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 04:22:47.407039 649412 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.184.242.156 port: 12001\n",
      "I1212 04:22:47.407064 649412 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.184.242.156:15129\n",
      "I1212 04:22:47.407140 649412 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 04:22:47.409899 649412 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 04:22:47.435914 649412 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 04:22:47.436587 649412 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:c2:71\n",
      "I1212 04:22:47.463819 649412 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 04:22:47.464433 649412 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:42:71\n",
      "I1212 04:22:47.492288 649412 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 04:22:47.493101 649412 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:b8:f2:9c\n",
      "I1212 04:22:47.580808 649412 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 04:22:47.581437 649412 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:42:71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 04:22:47.607833 649412 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 04:22:47.608453 649412 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:c2:71\n",
      "I1212 04:22:47.635772 649412 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 04:22:47.636385 649412 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:42:71\n",
      "I1212 04:22:47.663764 649412 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 04:22:47.664359 649412 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:c2:71\n",
      "I1212 04:22:47.691793 649412 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 04:22:47.692397 649412 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:42:71\n",
      "I1212 04:22:47.719825 649412 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 04:22:47.720420 649412 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:c2:71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 04:22:48.500942 649412 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f9b1ffff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:22:50] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:02<00:18,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:05<00:15,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:07<00:12,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:08<00:07,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:09<00:04,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:10<00:02,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:11<00:01,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:11<00:00,  1.10it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:11<00:00,  1.46s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=32.37 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:05] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /public_sglang_ci/runner-l2a-gpu-1/_work/sglang/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=60,N=1408,device_name=NVIDIA_H100_80GB_HBM3.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-12 04:23:05] Using MoE kernel config with down_moe=False. Performance might be sub-optimal! Config file not found at /public_sglang_ci/runner-l2a-gpu-1/_work/sglang/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=60,N=1408,device_name=NVIDIA_H100_80GB_HBM3_down.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=32.37 GB):  33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]\r",
      "Capturing batches (bs=2 avail_mem=30.79 GB):  33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=2 avail_mem=30.79 GB):  67%|██████▋   | 2/3 [00:02<00:01,  1.08s/it]\r",
      "Capturing batches (bs=1 avail_mem=30.78 GB):  67%|██████▋   | 2/3 [00:02<00:01,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=1 avail_mem=30.78 GB): 100%|██████████| 3/3 [00:02<00:00,  1.39it/s]\r",
      "Capturing batches (bs=1 avail_mem=30.78 GB): 100%|██████████| 3/3 [00:02<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:09] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expert_record_server_process, port = launch_server_cmd(\n",
    "    \"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0 --expert-distribution-recorder-mode stat --log-level warning\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:23:13.644407Z",
     "iopub.status.busy": "2025-12-12T04:23:13.644254Z",
     "iopub.status.idle": "2025-12-12T04:23:15.359248Z",
     "shell.execute_reply": "2025-12-12T04:23:15.358706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><Response [200]></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': ' persuade persuade\\nMarseille', 'output_ids': [49941, 49941, 198, 44, 61160, 151643], 'meta_info': {'id': 'fe2d0456d0b648908d38f9434a192a32', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 7, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 6, 'cached_tokens': 0, 'e2e_latency': 0.16898822784423828, 'response_sent_to_client_ts': 1765513393.8198047}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><Response [200]></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><Response [200]></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = requests.post(f\"http://localhost:{port}/start_expert_distribution_record\")\n",
    "print_highlight(response)\n",
    "\n",
    "url = f\"http://localhost:{port}/generate\"\n",
    "data = {\"text\": \"What is the capital of France?\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.json())\n",
    "\n",
    "response = requests.post(f\"http://localhost:{port}/stop_expert_distribution_record\")\n",
    "print_highlight(response)\n",
    "\n",
    "response = requests.post(f\"http://localhost:{port}/dump_expert_distribution_record\")\n",
    "print_highlight(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:23:15.360638Z",
     "iopub.status.busy": "2025-12-12T04:23:15.360476Z",
     "iopub.status.idle": "2025-12-12T04:23:15.376257Z",
     "shell.execute_reply": "2025-12-12T04:23:15.375531Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(expert_record_server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize/Detokenize Example (Round Trip)\n",
    "\n",
    "This example demonstrates how to use the /tokenize and /detokenize endpoints together. We first tokenize a string, then detokenize the resulting IDs to reconstruct the original text. This workflow is useful when you need to handle tokenization externally but still leverage the server for detokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:23:15.378307Z",
     "iopub.status.busy": "2025-12-12T04:23:15.377969Z",
     "iopub.status.idle": "2025-12-12T04:23:47.447480Z",
     "shell.execute_reply": "2025-12-12T04:23:47.446850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:22] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:23:22] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:23:22] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:24] WARNING server_args.py:1414: Attention backend not explicitly specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:25] server_args=ServerArgs(model_path='qwen/qwen2.5-0.5b-instruct', tokenizer_path='qwen/qwen2.5-0.5b-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', rl_quant_profile=None, trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30422, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.841, max_running_requests=128, max_queued_requests=None, max_total_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=55093297, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, mm_process_config={}, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='qwen/qwen2.5-0.5b-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='fa3', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_moe_a2a_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_support_transfer_engine=True, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None, mm_enable_dp_encoder=False, forward_hooks=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:25] Using default HuggingFace chat template with detected content format: string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:33] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:23:33] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:23:33] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 04:23:33] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 04:23:33] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 04:23:33] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:35] Init torch distributed begin.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[2025-12-12 04:23:35] Init torch distributed ends. mem usage=0.00 GB\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 04:23:35.996011 651310 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 04:23:35.996030 651310 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.184.242.156 port: 12001\n",
      "I1212 04:23:35.996052 651310 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.184.242.156:15754\n",
      "I1212 04:23:35.996122 651310 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 04:23:35.998908 651310 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 04:23:36.027667 651310 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 04:23:36.028254 651310 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:c2:71\n",
      "I1212 04:23:36.055737 651310 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 04:23:36.056309 651310 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:48:42:71\n",
      "I1212 04:23:36.058995 651310 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 04:23:36.059487 651310 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:b8:f2:9c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 04:23:36.400812 651310 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 04:23:36.401422 651310 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:42:71\n",
      "I1212 04:23:36.427634 651310 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 04:23:36.428187 651310 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:49:c2:71\n",
      "I1212 04:23:36.432869 651310 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 04:23:36.433449 651310 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:42:71\n",
      "I1212 04:23:36.438156 651310 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 04:23:36.438702 651310 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4a:c2:71\n",
      "I1212 04:23:36.463629 651310 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 04:23:36.464180 651310 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:42:71\n",
      "I1212 04:23:36.491614 651310 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 04:23:36.492204 651310 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4b:c2:71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 04:23:37.135118 651310 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f2a87fff010, len: 2147483648: Operation not permitted [1]\n",
      "[2025-12-12 04:23:37] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:39] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-12-12 04:23:39] Load weight begin. avail mem=78.07 GB\n",
      "[2025-12-12 04:23:39] Found local HF snapshot for qwen/qwen2.5-0.5b-instruct at /hf_home/hub/models--qwen--qwen2.5-0.5b-instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775; skipping download.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:39] No model.safetensors.index.json found in remote.\n",
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.52it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.51it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:39] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=77.01 GB, mem usage=1.06 GB.\n",
      "[2025-12-12 04:23:40] Using KV cache dtype: torch.bfloat16\n",
      "[2025-12-12 04:23:40] KV Cache is allocated. #tokens: 20480, K size: 0.12 GB, V size: 0.12 GB\n",
      "[2025-12-12 04:23:40] Memory pool end. avail mem=76.61 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:40] Capture cuda graph begin. This can take up to several minutes. avail mem=76.51 GB\n",
      "[2025-12-12 04:23:40] Capture cuda graph bs [1, 2, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=76.51 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=76.51 GB):  33%|███▎      | 1/3 [00:00<00:00,  4.50it/s]\r",
      "Capturing batches (bs=2 avail_mem=76.45 GB):  33%|███▎      | 1/3 [00:00<00:00,  4.50it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.45 GB):  33%|███▎      | 1/3 [00:00<00:00,  4.50it/s]\r",
      "Capturing batches (bs=1 avail_mem=76.45 GB): 100%|██████████| 3/3 [00:00<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:40] Capture cuda graph end. Time elapsed: 0.75 s. mem usage=15.86 GB. avail mem=60.66 GB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:41] max_total_num_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=128, context_len=32768, available_gpu_mem=60.66 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:41] INFO:     Started server process [650983]\n",
      "[2025-12-12 04:23:41] INFO:     Waiting for application startup.\n",
      "[2025-12-12 04:23:41] Using default chat sampling params from model generation config: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
      "[2025-12-12 04:23:41] Using default chat sampling params from model generation config: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
      "[2025-12-12 04:23:41] INFO:     Application startup complete.\n",
      "[2025-12-12 04:23:41] INFO:     Uvicorn running on http://127.0.0.1:30422 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:42] INFO:     127.0.0.1:59186 - \"GET /v1/models HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:42] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n",
      "[2025-12-12 04:23:42] INFO:     127.0.0.1:59198 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-12-12 04:23:42] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_free_server_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:23:47.449143Z",
     "iopub.status.busy": "2025-12-12T04:23:47.448997Z",
     "iopub.status.idle": "2025-12-12T04:23:47.464072Z",
     "shell.execute_reply": "2025-12-12T04:23:47.463574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Original Input Text:<br>'SGLang provides efficient tokenization endpoints.'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:47] INFO:     127.0.0.1:40018 - \"POST /tokenize HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br>Tokenized Output (IDs):<br>[50, 3825, 524, 5707, 11050, 3950, 2022, 36342, 13]</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Token Count: 9</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Max Model Length: 131072</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 04:23:47] INFO:     127.0.0.1:40026 - \"POST /detokenize HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br>Detokenized Output (Text):<br>'SGLang provides efficient tokenization endpoints.'</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br>Round Trip Successful: Original and reconstructed text match.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from sglang.utils import print_highlight\n",
    "\n",
    "base_url = f\"http://localhost:{port}\"\n",
    "tokenize_url = f\"{base_url}/tokenize\"\n",
    "detokenize_url = f\"{base_url}/detokenize\"\n",
    "\n",
    "model_name = \"qwen/qwen2.5-0.5b-instruct\"\n",
    "input_text = \"SGLang provides efficient tokenization endpoints.\"\n",
    "print_highlight(f\"Original Input Text:\\n'{input_text}'\")\n",
    "\n",
    "# --- tokenize the input text ---\n",
    "tokenize_payload = {\n",
    "    \"model\": model_name,\n",
    "    \"prompt\": input_text,\n",
    "    \"add_special_tokens\": False,\n",
    "}\n",
    "try:\n",
    "    tokenize_response = requests.post(tokenize_url, json=tokenize_payload)\n",
    "    tokenize_response.raise_for_status()\n",
    "    tokenization_result = tokenize_response.json()\n",
    "    token_ids = tokenization_result.get(\"tokens\")\n",
    "\n",
    "    if not token_ids:\n",
    "        raise ValueError(\"Tokenization returned empty tokens.\")\n",
    "\n",
    "    print_highlight(f\"\\nTokenized Output (IDs):\\n{token_ids}\")\n",
    "    print_highlight(f\"Token Count: {tokenization_result.get('count')}\")\n",
    "    print_highlight(f\"Max Model Length: {tokenization_result.get('max_model_len')}\")\n",
    "\n",
    "    # --- detokenize the obtained token IDs ---\n",
    "    detokenize_payload = {\n",
    "        \"model\": model_name,\n",
    "        \"tokens\": token_ids,\n",
    "        \"skip_special_tokens\": True,\n",
    "    }\n",
    "\n",
    "    detokenize_response = requests.post(detokenize_url, json=detokenize_payload)\n",
    "    detokenize_response.raise_for_status()\n",
    "    detokenization_result = detokenize_response.json()\n",
    "    reconstructed_text = detokenization_result.get(\"text\")\n",
    "\n",
    "    print_highlight(f\"\\nDetokenized Output (Text):\\n'{reconstructed_text}'\")\n",
    "\n",
    "    if input_text == reconstructed_text:\n",
    "        print_highlight(\n",
    "            \"\\nRound Trip Successful: Original and reconstructed text match.\"\n",
    "        )\n",
    "    else:\n",
    "        print_highlight(\n",
    "            \"\\nRound Trip Mismatch: Original and reconstructed text differ.\"\n",
    "        )\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print_highlight(f\"\\nHTTP Request Error: {e}\")\n",
    "except Exception as e:\n",
    "    print_highlight(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T04:23:47.465390Z",
     "iopub.status.busy": "2025-12-12T04:23:47.465253Z",
     "iopub.status.idle": "2025-12-12T04:23:47.473303Z",
     "shell.execute_reply": "2025-12-12T04:23:47.472804Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(tokenizer_free_server_process)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
