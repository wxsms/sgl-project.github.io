{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI APIs - Vision\n",
    "\n",
    "SGLang provides OpenAI-compatible APIs to enable a smooth transition from OpenAI services to self-hosted local models.\n",
    "A complete reference for the API is available in the [OpenAI API Reference](https://platform.openai.com/docs/guides/vision).\n",
    "This tutorial covers the vision APIs for vision language models.\n",
    "\n",
    "SGLang supports various vision language models such as Llama 3.2, LLaVA-OneVision, Qwen2.5-VL, Gemma3 and [more](https://docs.sglang.ai/supported_models/vision_language_models): \n",
    "- [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)  \n",
    "- [lmms-lab/llava-onevision-qwen2-72b-ov-chat](https://huggingface.co/lmms-lab/llava-onevision-qwen2-72b-ov-chat)  \n",
    "- [Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)\n",
    "- [google/gemma-3-4b-it](https://huggingface.co/google/gemma-3-4b-it)\n",
    "- [openbmb/MiniCPM-V](https://huggingface.co/openbmb/MiniCPM-V)\n",
    "- [deepseek-ai/deepseek-vl2](https://huggingface.co/deepseek-ai/deepseek-vl2)\n",
    "\n",
    "As an alternative to the OpenAI API, you can also use the [SGLang offline engine](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/offline_batch_inference_vlm.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch A Server\n",
    "\n",
    "Launch the server in your terminal and wait for it to initialize.\n",
    "\n",
    "**Remember to add** `--chat-template` **for example** `--chat-template=qwen2-vl` **to specify the [vision chat template](https://docs.sglang.ai/backend/openai_api_vision.html#Chat-Template), otherwise, the server will only support text (images won’t be passed in), which can lead to degraded performance.**\n",
    "\n",
    "We need to specify `--chat-template` for vision language models because the chat template provided in Hugging Face tokenizer only supports text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T18:28:39.460464Z",
     "iopub.status.busy": "2025-04-24T18:28:39.460247Z",
     "iopub.status.idle": "2025-04-24T18:29:17.690259Z",
     "shell.execute_reply": "2025-04-24T18:29:17.689720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:28:54] server_args=ServerArgs(model_path='Qwen/Qwen2.5-VL-7B-Instruct', tokenizer_path='Qwen/Qwen2.5-VL-7B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, enable_tokenizer_batch_encode=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='Qwen/Qwen2.5-VL-7B-Instruct', chat_template='qwen2-vl', completion_template=None, is_embedding=False, revision=None, host='127.0.0.1', port=39254, mem_fraction_static=0.88, max_running_requests=200, max_total_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, stream_interval=1, stream_output=False, random_seed=143725474, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:28:56] Use chat template for the OpenAI-compatible API server: qwen2-vl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:05 TP0] Overlap scheduler is disabled for multimodal models.\n",
      "[2025-04-24 18:29:05 TP0] Attention backend not set. Use flashinfer backend by default.\n",
      "[2025-04-24 18:29:05 TP0] Automatically reduce --mem-fraction-static to 0.792 because this is a multimodal model.\n",
      "[2025-04-24 18:29:05 TP0] Automatically turn off --chunked-prefill-size for multimodal model.\n",
      "[2025-04-24 18:29:05 TP0] Init torch distributed begin.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:05 TP0] Init torch distributed ends. mem usage=0.00 GB\n",
      "[2025-04-24 18:29:05 TP0] Load weight begin. avail mem=43.49 GB\n",
      "[2025-04-24 18:29:06 TP0] Ignore import error when loading sglang.srt.models.llama4. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:07 TP0] Using model weights format ['*.safetensors']\n",
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.83it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.62it/s]\n",
      "\n",
      "[2025-04-24 18:29:10 TP0] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=27.70 GB, mem usage=15.79 GB.\n",
      "[2025-04-24 18:29:10 TP0] KV Cache is allocated. #tokens: 20480, K size: 0.55 GB, V size: 0.55 GB\n",
      "[2025-04-24 18:29:10 TP0] Memory pool end. avail mem=26.33 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:29:10,714 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
      "[2025-04-24 18:29:10 TP0] \n",
      "\n",
      "CUDA Graph is DISABLED.\n",
      "This will cause significant performance degradation.\n",
      "CUDA Graph should almost never be disabled in most usage scenarios.\n",
      "If you encounter OOM issues, please try setting --mem-fraction-static to a lower value (such as 0.8 or 0.7) instead of disabling CUDA Graph.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:11 TP0] max_total_num_tokens=20480, chunked_prefill_size=-1, max_prefill_tokens=16384, max_running_requests=200, context_len=128000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:12] INFO:     Started server process [2235818]\n",
      "[2025-04-24 18:29:12] INFO:     Waiting for application startup.\n",
      "[2025-04-24 18:29:12] INFO:     Application startup complete.\n",
      "[2025-04-24 18:29:12] INFO:     Uvicorn running on http://127.0.0.1:39254 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:12] INFO:     127.0.0.1:34124 - \"GET /v1/models HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:13] INFO:     127.0.0.1:34128 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-04-24 18:29:13 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:29:14,101 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n",
      "2025-04-24 18:29:14,223 - INFO - flashinfer.jit: Finished loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:16] INFO:     127.0.0.1:34144 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-04-24 18:29:16] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    We are running those notebooks in a CI parallel environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang.test.test_utils import is_in_ci\n",
    "\n",
    "if is_in_ci():\n",
    "    from patch import launch_server_cmd\n",
    "else:\n",
    "    from sglang.utils import launch_server_cmd\n",
    "\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "vision_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-VL-7B-Instruct \\\n",
    "    --chat-template=qwen2-vl\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cURL\n",
    "\n",
    "Once the server is up, you can send test requests using curl or requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T18:29:17.692330Z",
     "iopub.status.busy": "2025-04-24T18:29:17.691908Z",
     "iopub.status.idle": "2025-04-24T18:29:23.905651Z",
     "shell.execute_reply": "2025-04-24T18:29:23.905122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:18 TP0] Prefill batch. #new-seq: 1, #new-token: 307, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:19 TP0] Decode batch. #running-req: 1, #token: 340, token usage: 0.02, gen throughput (token/s): 5.37, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:20 TP0] Decode batch. #running-req: 1, #token: 380, token usage: 0.02, gen throughput (token/s): 57.62, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:20 TP0] Decode batch. #running-req: 1, #token: 420, token usage: 0.02, gen throughput (token/s): 57.43, #queue-req: 0, \n",
      "[2025-04-24 18:29:20] INFO:     127.0.0.1:40372 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"id\":\"2f1bc441648048fabe2e70adc6dd427b\",\"object\":\"chat.completion\",\"created\":1745519357,\"model\":\"Qwen/Qwen2.5-VL-7B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The image depicts a person standing facing sideways next to the open tailgate of a yellow SUV, which appears to be parked on a city street. The individual is interacting with an iron and is in the process of either using the iron to handle a piece of clothing or maintaining some kind of apparatus (possibly related to laundry or ironing). The SUV's cargo space seems to be configured specifically for this task, suggesting a setup related to ironing and handling fabrics. In the background, you can see a busy street, including a taxi cab, and storefronts lining the street.\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":151645}],\"usage\":{\"prompt_tokens\":307,\"total_tokens\":423,\"completion_tokens\":116,\"prompt_tokens_details\":null}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:21 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 306, token usage: 0.01, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:21 TP0] Decode batch. #running-req: 1, #token: 345, token usage: 0.02, gen throughput (token/s): 37.37, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:22 TP0] Decode batch. #running-req: 1, #token: 385, token usage: 0.02, gen throughput (token/s): 58.27, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:23 TP0] Decode batch. #running-req: 1, #token: 425, token usage: 0.02, gen throughput (token/s): 58.68, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:23 TP0] Decode batch. #running-req: 1, #token: 465, token usage: 0.02, gen throughput (token/s): 58.66, #queue-req: 0, \n",
      "[2025-04-24 18:29:23] INFO:     127.0.0.1:40376 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"id\":\"bae86e83f532492aa3aa237f194f0d58\",\"object\":\"chat.completion\",\"created\":1745519360,\"model\":\"Qwen/Qwen2.5-VL-7B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"This image shows a scene in what appears to be a busy urban area. A man wearing a bright yellow shirt and black pants is engaged in an activity involving two folded dress shirts placed on an ironing board. The ironing board setup rests on the rear of a yellow taxi cab. The man is attempting to iron one of the shirts while balancing it in his hands.\\n\\nThe taxi is parked on the side of the street, with traffic and pedestrians visible in the background. The setting is characteristic of a city environment with buildings, street signs, and possibly a storefront. There are also pink signage banners and an American flag in the background, which could suggest this location is in a city like New York City. Overall, the image humorously portrays a unique use of public transportation for a mundane task.\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":151645}],\"usage\":{\"prompt_tokens\":307,\"total_tokens\":467,\"completion_tokens\":160,\"prompt_tokens_details\":null}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "curl_command = f\"\"\"\n",
    "curl -s http://localhost:{port}/v1/chat/completions \\\\\n",
    "  -d '{{\n",
    "    \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    \"messages\": [\n",
    "      {{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"What’s in this image?\"\n",
    "          }},\n",
    "          {{\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {{\n",
    "              \"url\": \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\"\n",
    "            }}\n",
    "          }}\n",
    "        ]\n",
    "      }}\n",
    "    ],\n",
    "    \"max_tokens\": 300\n",
    "  }}'\n",
    "\"\"\"\n",
    "\n",
    "response = subprocess.check_output(curl_command, shell=True).decode()\n",
    "print_highlight(response)\n",
    "\n",
    "\n",
    "response = subprocess.check_output(curl_command, shell=True).decode()\n",
    "print_highlight(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T18:29:23.907452Z",
     "iopub.status.busy": "2025-04-24T18:29:23.907191Z",
     "iopub.status.idle": "2025-04-24T18:29:26.780557Z",
     "shell.execute_reply": "2025-04-24T18:29:26.780114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:25 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 306, token usage: 0.01, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:25 TP0] Decode batch. #running-req: 1, #token: 346, token usage: 0.02, gen throughput (token/s): 22.02, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:26 TP0] Decode batch. #running-req: 1, #token: 386, token usage: 0.02, gen throughput (token/s): 58.54, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:26] INFO:     127.0.0.1:40380 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"id\":\"bb75d38989ef4cedbf96a9d5cc849ccf\",\"object\":\"chat.completion\",\"created\":1745519363,\"model\":\"Qwen/Qwen2.5-VL-7B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The image shows a man in a yellow shirt, who appears to be ironing clothes directly on the trunk of a yellow taxi parked on the street. The taxi also displays an advertisement or service information. The setting suggests an urban environment with storefronts and a yellow taxi, common in places like New York City, which often features taxis in this distinctive color. The man’s posture and the setup imply he's performing the task for efficiency, perhaps to unload mail or schedule an appoint for laundry pickup/drop-off.\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":151645}],\"usage\":{\"prompt_tokens\":307,\"total_tokens\":410,\"completion_tokens\":103,\"prompt_tokens_details\":null}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = f\"http://localhost:{port}/v1/chat/completions\"\n",
    "\n",
    "data = {\n",
    "    \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\"\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 300,\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T18:29:26.782236Z",
     "iopub.status.busy": "2025-04-24T18:29:26.782001Z",
     "iopub.status.idle": "2025-04-24T18:29:32.503390Z",
     "shell.execute_reply": "2025-04-24T18:29:32.502612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:30] INFO:     127.0.0.1:52584 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\n",
      "[2025-04-24 18:29:30] ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 468, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 463, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/http/client.py\", line 1377, in getresponse\n",
      "    response.begin()\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/http/client.py\", line 320, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/http/client.py\", line 281, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/socket.py\", line 716, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/ssl.py\", line 1275, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/ssl.py\", line 1133, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 802, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/util/retry.py\", line 552, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/packages/six.py\", line 770, in reraise\n",
      "    raise value\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 716, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 470, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 358, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='github.com', port=443): Read timed out. (read timeout=3)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/managers/multimodal_processors/base_processor.py\", line 141, in _load_single_item\n",
      "    img, _ = load_image(data)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/utils.py\", line 604, in load_image\n",
      "    response = requests.get(image_file, stream=True, timeout=timeout).raw\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/sessions.py\", line 724, in send\n",
      "    history = [resp for resp in gen]\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/sessions.py\", line 724, in <listcomp>\n",
      "    history = [resp for resp in gen]\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/sessions.py\", line 265, in resolve_redirects\n",
      "    resp = self.send(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/adapters.py\", line 713, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='github.com', port=443): Read timed out. (read timeout=3)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/entrypoints/http_server.py\", line 570, in openai_v1_chat_completions\n",
      "    return await v1_chat_completions(_global_state.tokenizer_manager, raw_request)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/openai_api/adapter.py\", line 1702, in v1_chat_completions\n",
      "    ret = await tokenizer_manager.generate_request(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 383, in generate_request\n",
      "    tokenized_obj = await self._tokenize_one_request(obj)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 421, in _tokenize_one_request\n",
      "    image_inputs: Dict = await self.mm_processor.process_mm_data_async(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/managers/multimodal_processors/qwen_vl.py\", line 52, in process_mm_data_async\n",
      "    base_output = self.load_mm_data(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/managers/multimodal_processors/base_processor.py\", line 271, in load_mm_data\n",
      "    result = futures[task_ptr].result()\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/concurrent/futures/_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/managers/multimodal_processors/base_processor.py\", line 144, in _load_single_item\n",
      "    raise RuntimeError(f\"Error while loading data {data}: {e}\")\n",
      "RuntimeError: Error while loading data https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true: HTTPSConnectionPool(host='github.com', port=443): Read timed out. (read timeout=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:30 TP0] Prefill batch. #new-seq: 1, #new-token: 292, #cached-token: 15, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:31 TP0] Decode batch. #running-req: 1, #token: 324, token usage: 0.02, gen throughput (token/s): 8.31, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:31 TP0] Decode batch. #running-req: 1, #token: 364, token usage: 0.02, gen throughput (token/s): 57.07, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:32] INFO:     127.0.0.1:52592 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The image depicts a man wearing a yellow shirt, working outdoors on a sidewalk next to a taxi. The man has set up an ironing board, with a blue garment (likely a shirt) draped over it, and appears to be actively ironing the cloth. He is holding the iron in one hand. There is a yellow taxi parked nearby, and the background showcases an urban street environment, including storefronts, a few pedestrians, and outdoor lighting posts.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=f\"http://localhost:{port}/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is in this image?\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\"\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "print_highlight(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple-Image Inputs\n",
    "\n",
    "The server also supports multiple images and interleaved text and images if the model supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T18:29:32.505566Z",
     "iopub.status.busy": "2025-04-24T18:29:32.505269Z",
     "iopub.status.idle": "2025-04-24T18:29:36.019076Z",
     "shell.execute_reply": "2025-04-24T18:29:36.018550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:34 TP0] Prefill batch. #new-seq: 1, #new-token: 2532, #cached-token: 14, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:35 TP0] Decode batch. #running-req: 1, #token: 2551, token usage: 0.12, gen throughput (token/s): 11.82, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:35 TP0] Decode batch. #running-req: 1, #token: 2591, token usage: 0.13, gen throughput (token/s): 58.75, #queue-req: 0, \n",
      "[2025-04-24 18:29:36] INFO:     127.0.0.1:52604 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The first image shows a man ironing clothes on the back of a taxi in a busy urban street. The second image is a stylized logo featuring the letters \"SGL\" with a book and a computer icon incorporated into the design.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=f\"http://localhost:{port}/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\",\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://raw.githubusercontent.com/sgl-project/sglang/main/assets/logo.png\",\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"I have two very different images. They are not related at all. \"\n",
    "                    \"Please describe the first image in one sentence, and then describe the second image in another sentence.\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print_highlight(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T18:29:36.020756Z",
     "iopub.status.busy": "2025-04-24T18:29:36.020501Z",
     "iopub.status.idle": "2025-04-24T18:29:36.052738Z",
     "shell.execute_reply": "2025-04-24T18:29:36.052097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 18:29:36] Child process unexpectedly failed with an exit code 9. pid=2236413\n",
      "[2025-04-24 18:29:36] Child process unexpectedly failed with an exit code 9. pid=2236217\n"
     ]
    }
   ],
   "source": [
    "terminate_process(vision_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Template\n",
    "\n",
    "As mentioned before, if you do not specify a vision model's `--chat-template`, the server uses Hugging Face's default template, which only supports text.\n",
    "\n",
    "We list popular vision models with their chat templates:\n",
    "\n",
    "- [meta-llama/Llama-3.2-Vision](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct) uses `llama_3_vision`.\n",
    "- [Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) uses `qwen2-vl`.\n",
    "- [google/gemma-3-4b-it](https://huggingface.co/google/gemma-3-4b-it) uses `gemma-it`.\n",
    "- [openbmb/MiniCPM-V](https://huggingface.co/openbmb/MiniCPM-V) uses `minicpmv`.\n",
    "- [deepseek-ai/deepseek-vl2](https://huggingface.co/deepseek-ai/deepseek-vl2) uses `deepseek-vl2`.\n",
    "- [LlaVA-OneVision](https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov) uses `chatml-llava`.\n",
    "- [LLaVA-NeXT](https://huggingface.co/collections/lmms-lab/llava-next-6623288e2d61edba3ddbf5ff) uses `chatml-llava`.\n",
    "- [Llama3-LLaVA-NeXT](https://huggingface.co/lmms-lab/llama3-llava-next-8b) uses `llava_llama_3`.\n",
    "- [LLaVA-v1.5 / 1.6](https://huggingface.co/liuhaotian/llava-v1.6-34b) uses `vicuna_v1.1`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
