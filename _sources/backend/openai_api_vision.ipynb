{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI APIs - Vision\n",
    "\n",
    "SGLang provides OpenAI-compatible APIs to enable a smooth transition from OpenAI services to self-hosted local models.\n",
    "A complete reference for the API is available in the [OpenAI API Reference](https://platform.openai.com/docs/guides/vision).\n",
    "This tutorial covers the vision APIs for vision language models.\n",
    "\n",
    "SGLang supports various vision language models such as Llama 3.2, LLaVA-OneVision, Qwen2.5-VL, Gemma3 and [more](https://docs.sglang.ai/supported_models/vision_language_models): \n",
    "- [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)  \n",
    "- [lmms-lab/llava-onevision-qwen2-72b-ov-chat](https://huggingface.co/lmms-lab/llava-onevision-qwen2-72b-ov-chat)  \n",
    "- [Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)\n",
    "- [google/gemma-3-4b-it](https://huggingface.co/google/gemma-3-4b-it)\n",
    "- [openbmb/MiniCPM-V](https://huggingface.co/openbmb/MiniCPM-V)\n",
    "- [deepseek-ai/deepseek-vl2](https://huggingface.co/deepseek-ai/deepseek-vl2)\n",
    "\n",
    "As an alternative to the OpenAI API, you can also use the [SGLang offline engine](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/offline_batch_inference_vlm.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch A Server\n",
    "\n",
    "Launch the server in your terminal and wait for it to initialize.\n",
    "\n",
    "**Remember to add** `--chat-template` **for example** `--chat-template=qwen2-vl` **to specify the [vision chat template](https://docs.sglang.ai/backend/openai_api_vision.html#Chat-Template), otherwise, the server will only support text (images won’t be passed in), which can lead to degraded performance.**\n",
    "\n",
    "We need to specify `--chat-template` for vision language models because the chat template provided in Hugging Face tokenizer only supports text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T14:48:21.612858Z",
     "iopub.status.busy": "2025-05-07T14:48:21.612492Z",
     "iopub.status.idle": "2025-05-07T14:49:04.844629Z",
     "shell.execute_reply": "2025-05-07T14:49:04.844057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:48:41] server_args=ServerArgs(model_path='Qwen/Qwen2.5-VL-7B-Instruct', tokenizer_path='Qwen/Qwen2.5-VL-7B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, enable_tokenizer_batch_encode=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='Qwen/Qwen2.5-VL-7B-Instruct', chat_template='qwen2-vl', completion_template=None, is_embedding=False, revision=None, host='127.0.0.1', port=38926, mem_fraction_static=0.88, max_running_requests=200, max_total_tokens=20480, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=411399233, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device=None, pdlb_url=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:48:43] Use chat template for the OpenAI-compatible API server: qwen2-vl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:48:52] Attention backend not set. Use flashinfer backend by default.\n",
      "[2025-05-07 14:48:52] Automatically reduce --mem-fraction-static to 0.792 because this is a multimodal model.\n",
      "[2025-05-07 14:48:52] Automatically turn off --chunked-prefill-size for multimodal model.\n",
      "[2025-05-07 14:48:52] Init torch distributed begin.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:48:53] Init torch distributed ends. mem usage=0.00 GB\n",
      "[2025-05-07 14:48:53] Load weight begin. avail mem=51.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:48:53] Ignore import error when loading sglang.srt.models.llama4. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:48:54] Using model weights format ['*.safetensors']\n",
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:01,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  1.88it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  1.68it/s]\n",
      "\n",
      "[2025-05-07 14:48:57] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=35.46 GB, mem usage=15.79 GB.\n",
      "[2025-05-07 14:48:57] KV Cache is allocated. #tokens: 20480, K size: 0.55 GB, V size: 0.55 GB\n",
      "[2025-05-07 14:48:57] Memory pool end. avail mem=34.09 GB\n",
      "2025-05-07 14:48:57,595 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:48:58] max_total_num_tokens=20480, chunked_prefill_size=-1, max_prefill_tokens=16384, max_running_requests=200, context_len=128000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:48:59] INFO:     Started server process [2124785]\n",
      "[2025-05-07 14:48:59] INFO:     Waiting for application startup.\n",
      "[2025-05-07 14:48:59] INFO:     Application startup complete.\n",
      "[2025-05-07 14:48:59] INFO:     Uvicorn running on http://127.0.0.1:38926 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:48:59] INFO:     127.0.0.1:49862 - \"GET /v1/models HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:00] INFO:     127.0.0.1:49878 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-05-07 14:49:00] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-07 14:49:02,779 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n",
      "2025-05-07 14:49:02,928 - INFO - flashinfer.jit: Finished loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:03] INFO:     127.0.0.1:49882 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-05-07 14:49:03] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    We are running those notebooks in a CI parallel environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang.test.test_utils import is_in_ci\n",
    "\n",
    "if is_in_ci():\n",
    "    from patch import launch_server_cmd\n",
    "else:\n",
    "    from sglang.utils import launch_server_cmd\n",
    "\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "vision_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-VL-7B-Instruct \\\n",
    "    --chat-template=qwen2-vl\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cURL\n",
    "\n",
    "Once the server is up, you can send test requests using curl or requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T14:49:04.846680Z",
     "iopub.status.busy": "2025-05-07T14:49:04.846268Z",
     "iopub.status.idle": "2025-05-07T14:49:09.344507Z",
     "shell.execute_reply": "2025-05-07T14:49:09.343984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:05] Prefill batch. #new-seq: 1, #new-token: 307, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:06] Decode batch. #running-req: 1, #token: 340, token usage: 0.02, gen throughput (token/s): 5.25, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:07] Decode batch. #running-req: 1, #token: 380, token usage: 0.02, gen throughput (token/s): 59.24, #queue-req: 0\n",
      "[2025-05-07 14:49:07] INFO:     127.0.0.1:49888 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"id\":\"6742baafe7f3405dabfb8cada7f3a727\",\"object\":\"chat.completion\",\"created\":1746629344,\"model\":\"Qwen/Qwen2.5-VL-7B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"This image captures a humorous and unusual street scene involving a person engaged in an unconventional laundry activity. The individual is standing on the trunk of a yellow taxi parked on a busy urban street. They are holding an iron and what appears to be a drying rack or ladder, shunting cloth items across the car trunk, mimicking an ironing process. The scene reflects creativity in a seemingly unrelated activity amidst urban transportation.\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":151645}],\"usage\":{\"prompt_tokens\":307,\"total_tokens\":390,\"completion_tokens\":83,\"prompt_tokens_details\":null}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:07] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 306, token usage: 0.01, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:08] Decode batch. #running-req: 1, #token: 337, token usage: 0.02, gen throughput (token/s): 35.66, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:08] Decode batch. #running-req: 1, #token: 377, token usage: 0.02, gen throughput (token/s): 61.33, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:09] INFO:     127.0.0.1:53090 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{\"id\":\"73194e7b1a0f4473bb37f1bd6164e330\",\"object\":\"chat.completion\",\"created\":1746629347,\"model\":\"Qwen/Qwen2.5-VL-7B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The image shows a man wearing a yellow shirt standing on the back of a taxi. He is holding what appears to be a pair of pants draped across a makeshift clothesline attached to the taxi's rear window. The clothesline is suspended above the taxi, suggesting he is drying or airing out some laundry. The taxi and the man appear to be in an urban environment, as evidenced by the buildings, other taxis, and street banners in the background.\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":151645}],\"usage\":{\"prompt_tokens\":307,\"total_tokens\":398,\"completion_tokens\":91,\"prompt_tokens_details\":null}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "curl_command = f\"\"\"\n",
    "curl -s http://localhost:{port}/v1/chat/completions \\\\\n",
    "  -d '{{\n",
    "    \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    \"messages\": [\n",
    "      {{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"What’s in this image?\"\n",
    "          }},\n",
    "          {{\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {{\n",
    "              \"url\": \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\"\n",
    "            }}\n",
    "          }}\n",
    "        ]\n",
    "      }}\n",
    "    ],\n",
    "    \"max_tokens\": 300\n",
    "  }}'\n",
    "\"\"\"\n",
    "\n",
    "response = subprocess.check_output(curl_command, shell=True).decode()\n",
    "print_highlight(response)\n",
    "\n",
    "\n",
    "response = subprocess.check_output(curl_command, shell=True).decode()\n",
    "print_highlight(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T14:49:09.346390Z",
     "iopub.status.busy": "2025-05-07T14:49:09.346133Z",
     "iopub.status.idle": "2025-05-07T14:49:12.701640Z",
     "shell.execute_reply": "2025-05-07T14:49:12.700962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:12] INFO:     127.0.0.1:53100 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\n",
      "[2025-05-07 14:49:12] ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 468, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 463, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/http/client.py\", line 1377, in getresponse\n",
      "    response.begin()\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/http/client.py\", line 320, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/http/client.py\", line 281, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/socket.py\", line 716, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/ssl.py\", line 1275, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/ssl.py\", line 1133, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 802, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/util/retry.py\", line 552, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/packages/six.py\", line 770, in reraise\n",
      "    raise value\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 716, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 470, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 358, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=3)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/managers/multimodal_processors/base_processor.py\", line 141, in _load_single_item\n",
      "    img, _ = load_image(data)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/utils.py\", line 643, in load_image\n",
      "    response = requests.get(image_file, stream=True, timeout=timeout).raw\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/sessions.py\", line 724, in send\n",
      "    history = [resp for resp in gen]\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/sessions.py\", line 724, in <listcomp>\n",
      "    history = [resp for resp in gen]\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/sessions.py\", line 265, in resolve_redirects\n",
      "    resp = self.send(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/requests/adapters.py\", line 713, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=3)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/entrypoints/http_server.py\", line 580, in openai_v1_chat_completions\n",
      "    return await v1_chat_completions(_global_state.tokenizer_manager, raw_request)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/openai_api/adapter.py\", line 1729, in v1_chat_completions\n",
      "    ret = await tokenizer_manager.generate_request(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 387, in generate_request\n",
      "    tokenized_obj = await self._tokenize_one_request(obj)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 425, in _tokenize_one_request\n",
      "    image_inputs: Dict = await self.mm_processor.process_mm_data_async(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/managers/multimodal_processors/qwen_vl.py\", line 52, in process_mm_data_async\n",
      "    base_output = self.load_mm_data(\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/managers/multimodal_processors/base_processor.py\", line 271, in load_mm_data\n",
      "    result = futures[task_ptr].result()\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/concurrent/futures/_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/_tool/Python/3.9.21/x64/lib/python3.9/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/public_sglang_ci/runner-l1b-gpu-67/_work/sglang/sglang/python/sglang/srt/managers/multimodal_processors/base_processor.py\", line 144, in _load_single_item\n",
      "    raise RuntimeError(f\"Error while loading data {data}: {e}\")\n",
      "RuntimeError: Error while loading data https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Internal Server Error</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = f\"http://localhost:{port}/v1/chat/completions\"\n",
    "\n",
    "data = {\n",
    "    \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\"\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 300,\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T14:49:12.709055Z",
     "iopub.status.busy": "2025-05-07T14:49:12.708744Z",
     "iopub.status.idle": "2025-05-07T14:49:15.494846Z",
     "shell.execute_reply": "2025-05-07T14:49:15.494428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:13] Prefill batch. #new-seq: 1, #new-token: 292, #cached-token: 15, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:14] Decode batch. #running-req: 1, #token: 326, token usage: 0.02, gen throughput (token/s): 7.62, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:14] Decode batch. #running-req: 1, #token: 366, token usage: 0.02, gen throughput (token/s): 60.59, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:15] INFO:     127.0.0.1:53116 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The image depicts a person standing outside on a sidewalk next to a yellow taxi car. The individual is holding an iron and a piece of clothing that appears to be draped over a makeshift, foldable drying rack assembled on the back of the taxi. This setup allows the person to hang and possibly steam or press a garment while standing in a public area. The scene takes place on a busy city street with additional taxis, storefronts, and urban infrastructure in the background.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=f\"http://localhost:{port}/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is in this image?\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\"\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "print_highlight(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple-Image Inputs\n",
    "\n",
    "The server also supports multiple images and interleaved text and images if the model supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T14:49:15.496443Z",
     "iopub.status.busy": "2025-05-07T14:49:15.496210Z",
     "iopub.status.idle": "2025-05-07T14:49:17.699407Z",
     "shell.execute_reply": "2025-05-07T14:49:17.698906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:16] Prefill batch. #new-seq: 1, #new-token: 2532, #cached-token: 14, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:16] Decode batch. #running-req: 1, #token: 2551, token usage: 0.12, gen throughput (token/s): 19.13, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:17] Decode batch. #running-req: 1, #token: 2591, token usage: 0.13, gen throughput (token/s): 62.87, #queue-req: 0\n",
      "[2025-05-07 14:49:17] INFO:     127.0.0.1:59434 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The first image shows a man ironing clothes on the back of a taxi in a busy urban street. The second image is a stylized logo featuring the letters \"SGL\" with a book and a computer icon incorporated into the design.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=f\"http://localhost:{port}/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\",\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://raw.githubusercontent.com/sgl-project/sglang/main/assets/logo.png\",\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"I have two very different images. They are not related at all. \"\n",
    "                    \"Please describe the first image in one sentence, and then describe the second image in another sentence.\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print_highlight(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T14:49:17.701051Z",
     "iopub.status.busy": "2025-05-07T14:49:17.700798Z",
     "iopub.status.idle": "2025-05-07T14:49:17.736698Z",
     "shell.execute_reply": "2025-05-07T14:49:17.736230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:49:17] Child process unexpectedly failed with an exit code 9. pid=2125276\n",
      "[2025-05-07 14:49:17] Child process unexpectedly failed with an exit code 9. pid=2125210\n"
     ]
    }
   ],
   "source": [
    "terminate_process(vision_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Template\n",
    "\n",
    "As mentioned before, if you do not specify a vision model's `--chat-template`, the server uses Hugging Face's default template, which only supports text.\n",
    "\n",
    "We list popular vision models with their chat templates:\n",
    "\n",
    "- [meta-llama/Llama-3.2-Vision](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct) uses `llama_3_vision`.\n",
    "- [Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) uses `qwen2-vl`.\n",
    "- [google/gemma-3-4b-it](https://huggingface.co/google/gemma-3-4b-it) uses `gemma-it`.\n",
    "- [openbmb/MiniCPM-V](https://huggingface.co/openbmb/MiniCPM-V) uses `minicpmv`.\n",
    "- [deepseek-ai/deepseek-vl2](https://huggingface.co/deepseek-ai/deepseek-vl2) uses `deepseek-vl2`.\n",
    "- [LlaVA-OneVision](https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov) uses `chatml-llava`.\n",
    "- [LLaVA-NeXT](https://huggingface.co/collections/lmms-lab/llava-next-6623288e2d61edba3ddbf5ff) uses `chatml-llava`.\n",
    "- [Llama3-LLaVA-NeXT](https://huggingface.co/lmms-lab/llama3-llava-next-8b) uses `llava_llama_3`.\n",
    "- [LLaVA-v1.5 / 1.6](https://huggingface.co/liuhaotian/llava-v1.6-34b) uses `vicuna_v1.1`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
