{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speculative Decoding\n",
    "\n",
    "SGLang now provides an EAGLE-based (EAGLE-2/EAGLE-3) speculative decoding option. Our implementation aims to maximize speed and efficiency and is considered to be among the fastest in open-source LLM engines.\n",
    "\n",
    "### Performance Highlights\n",
    "\n",
    "Please see below for the huge improvements on throughput for LLaMA-Instruct 3.1 8B tested on MT bench that can be achieved via EAGLE3 decoding.\n",
    "For further details please see the [EAGLE3 paper](https://arxiv.org/pdf/2503.01840).\n",
    "\n",
    "| Method | Throughput (tokens/s) |\n",
    "|--------|----------------|\n",
    "| SGLang (w/o speculative, 1x H100) | 158.34 tokens/s |\n",
    "| SGLang + EAGLE-2 (1x H100) | 244.10 tokens/s |\n",
    "| SGLang + EAGLE-3 (1x H100) | 373.25 tokens/s |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EAGLE Decoding\n",
    "\n",
    "To enable EAGLE speculative decoding the following parameters are relevant:\n",
    "* `speculative_draft_model_path`: Specifies draft model. This parameter is required.\n",
    "* `speculative_num_steps`: Depth of autoregressive drafting. Increases speculation range but risks rejection cascades. Default is 5.\n",
    "* `speculative_eagle_topk`: Branching factor per step. Improves candidate diversity, will lead to higher acceptance rate, but more lead to higher memory/compute consumption. Default is 4.\n",
    "* `speculative_num_draft_tokens`: Maximum parallel verification capacity. Allows deeper tree evaluation but will lead to higher GPU memory usage. Default is 8.\n",
    "\n",
    "These parameters are the same for EAGLE-2 and EAGLE-3.\n",
    "\n",
    "You can find the best combinations of these parameters with [bench_speculative.py](https://github.com/sgl-project/sglang/blob/main/scripts/playground/bench_speculative.py).\n",
    "\n",
    "In the documentation below, we set `--cuda-graph-max-bs` to be a small value for faster engine startup. For your own workloads, please tune the above parameters together with `--cuda-graph-max-bs`, `--max-running-requests`, `--mem-fraction-static` for the best performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EAGLE-2 decoding\n",
    "\n",
    "You can enable EAGLE-2 decoding by setting `--speculative-algorithm EAGLE` and choosing an appropriate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:42:36.788889Z",
     "iopub.status.busy": "2025-12-12T01:42:36.788685Z",
     "iopub.status.idle": "2025-12-12T01:42:43.617937Z",
     "shell.execute_reply": "2025-12-12T01:42:43.616964Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:42:43] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:42:43] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:42:43] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    }
   ],
   "source": [
    "from sglang.test.doc_patch import launch_server_cmd\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:42:43.620122Z",
     "iopub.status.busy": "2025-12-12T01:42:43.619720Z",
     "iopub.status.idle": "2025-12-12T01:43:25.721476Z",
     "shell.execute_reply": "2025-12-12T01:43:25.720652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:42:49] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:42:49] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:42:49] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:42:52] WARNING server_args.py:1413: Attention backend not explicitly specified. Use flashinfer backend by default.\n",
      "[2025-12-12 01:42:52] WARNING server_args.py:1763: Overlap scheduler is disabled because of using eagle3 or standalone speculative decoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:42:58] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:42:58] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:42:58] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 01:42:58] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:42:58] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:42:58] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 01:43:00.745153 2587261 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 01:43:00.745170 2587261 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.187.8.134 port: 12001\n",
      "I1212 01:43:00.745196 2587261 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.187.8.134:16390\n",
      "I1212 01:43:00.745281 2587261 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 01:43:00.747958 2587261 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 01:43:00.775279 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 01:43:00.776121 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:48:45\n",
      "I1212 01:43:00.803161 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 01:43:00.803963 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:c8:45\n",
      "I1212 01:43:00.831123 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 01:43:00.831902 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:48:45\n",
      "I1212 01:43:00.920310 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 01:43:00.921129 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 01:43:00.951295 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 01:43:00.952170 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:48:45\n",
      "I1212 01:43:00.979611 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 01:43:00.980696 2587261 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:bb:08:86\n",
      "I1212 01:43:01.007189 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 01:43:01.008011 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:c8:45\n",
      "I1212 01:43:01.035131 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 01:43:01.035935 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:48:45\n",
      "I1212 01:43:01.063112 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 01:43:01.063913 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 01:43:02.129072 2587261 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f9b13fff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:43:04] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.29s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.37s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=55.16 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=55.16 GB):  25%|██▌       | 1/4 [00:00<00:00,  5.51it/s]\r",
      "Capturing batches (bs=3 avail_mem=55.01 GB):  25%|██▌       | 1/4 [00:00<00:00,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=2 avail_mem=54.95 GB):  25%|██▌       | 1/4 [00:00<00:00,  5.51it/s]\r",
      "Capturing batches (bs=2 avail_mem=54.95 GB):  75%|███████▌  | 3/4 [00:00<00:00, 11.70it/s]\r",
      "Capturing batches (bs=1 avail_mem=54.86 GB):  75%|███████▌  | 3/4 [00:00<00:00, 11.70it/s]\r",
      "Capturing batches (bs=1 avail_mem=54.86 GB): 100%|██████████| 4/4 [00:00<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:43:10] SPECULATIVE_MOE_RUNNER_BACKEND is not initialized, using auto backend\n",
      "[2025-12-12 01:43:10] SPECULATIVE_MOE_A2A_BACKEND is not initialized, using none backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 01:43:10.992125 2587261 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 01:43:10.992143 2587261 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.187.8.134 port: 12001\n",
      "I1212 01:43:10.992156 2587261 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.187.8.134:16416\n",
      "I1212 01:43:10.992236 2587261 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 01:43:10.992877 2587261 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 01:43:11.020256 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 01:43:11.020936 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:48:45\n",
      "I1212 01:43:11.047183 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 01:43:11.047829 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:c8:45\n",
      "I1212 01:43:11.075163 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 01:43:11.075803 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:48:45\n",
      "I1212 01:43:11.103196 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 01:43:11.103837 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:c8:45\n",
      "I1212 01:43:11.132282 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 01:43:11.132941 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:48:45\n",
      "I1212 01:43:11.159632 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 01:43:11.160565 2587261 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:bb:08:86\n",
      "I1212 01:43:11.187187 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 01:43:11.187804 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 01:43:11.215188 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 01:43:11.215797 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:48:45\n",
      "I1212 01:43:11.243199 2587261 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 01:43:11.243815 2587261 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 01:43:11.960671 2587261 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f9297fff010, len: 2147483648: Operation not permitted [1]\n",
      "\r",
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.36s/it]\n",
      "\r",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.36s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=53.79 GB):   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=53.79 GB):  25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]\r",
      "Capturing batches (bs=3 avail_mem=53.72 GB):  25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=3 avail_mem=53.72 GB):  50%|█████     | 2/4 [00:03<00:02,  1.37s/it]\r",
      "Capturing batches (bs=2 avail_mem=53.70 GB):  50%|█████     | 2/4 [00:03<00:02,  1.37s/it]\r",
      "Capturing batches (bs=1 avail_mem=53.67 GB):  50%|█████     | 2/4 [00:03<00:02,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=1 avail_mem=53.67 GB): 100%|██████████| 4/4 [00:05<00:00,  1.14s/it]\r",
      "Capturing batches (bs=1 avail_mem=53.67 GB): 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=53.65 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=3 avail_mem=53.60 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=2 avail_mem=53.59 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=1 avail_mem=53.57 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=1 avail_mem=53.57 GB): 100%|██████████| 4/4 [00:00<00:00, 114.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:43:21] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "server_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model meta-llama/Llama-2-7b-chat-hf  --speculative-algorithm EAGLE \\\n",
    "    --speculative-draft-model-path lmsys/sglang-EAGLE-llama2-chat-7B --speculative-num-steps 3 \\\n",
    "    --speculative-eagle-topk 4 --speculative-num-draft-tokens 16 --cuda-graph-max-bs 8 --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:43:25.731166Z",
     "iopub.status.busy": "2025-12-12T01:43:25.730998Z",
     "iopub.status.idle": "2025-12-12T01:43:26.105762Z",
     "shell.execute_reply": "2025-12-12T01:43:26.104994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Response: ChatCompletion(id='713c92aa49b245189bf839564d815df6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='  Sure! Here are three countries and their capitals:\\n\\n1. Country: France\\nCapital: Paris\\n2. Country: Japan\\nCapital: Tokyo\\n3. Country: Brazil\\nCapital: Brasília', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content=None), matched_stop=2)], created=1765503806, model='meta-llama/Llama-2-7b-chat-hf', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=48, prompt_tokens=17, total_tokens=65, completion_tokens_details=None, prompt_tokens_details=None, reasoning_tokens=0), metadata={'weight_version': 'default'})</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=64,\n",
    ")\n",
    "\n",
    "print_highlight(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:43:26.107769Z",
     "iopub.status.busy": "2025-12-12T01:43:26.107604Z",
     "iopub.status.idle": "2025-12-12T01:43:26.122792Z",
     "shell.execute_reply": "2025-12-12T01:43:26.122007Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EAGLE-2 Decoding with `torch.compile`\n",
    "\n",
    "You can also enable `torch.compile` for further optimizations and optionally set `--torch-compile-max-bs`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:43:26.124502Z",
     "iopub.status.busy": "2025-12-12T01:43:26.124338Z",
     "iopub.status.idle": "2025-12-12T01:44:27.259728Z",
     "shell.execute_reply": "2025-12-12T01:44:27.258915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:43:31] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:43:31] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:43:31] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:43:34] WARNING server_args.py:1413: Attention backend not explicitly specified. Use flashinfer backend by default.\n",
      "[2025-12-12 01:43:34] WARNING server_args.py:1763: Overlap scheduler is disabled because of using eagle3 or standalone speculative decoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:43:40] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:43:40] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:43:40] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:43:40] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:43:40] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:43:40] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 01:43:44.787106 2588059 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 01:43:44.787127 2588059 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.187.8.134 port: 12001\n",
      "I1212 01:43:44.787148 2588059 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.187.8.134:15422\n",
      "I1212 01:43:44.787233 2588059 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 01:43:44.790194 2588059 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 01:43:44.796389 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 01:43:44.797247 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:48:45\n",
      "I1212 01:43:44.827113 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 01:43:44.827896 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:c8:45\n",
      "I1212 01:43:44.855122 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 01:43:44.855911 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:48:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 01:43:44.972301 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 01:43:44.973070 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:c8:45\n",
      "I1212 01:43:45.003132 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 01:43:45.003930 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:48:45\n",
      "I1212 01:43:45.031414 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 01:43:45.032409 2588059 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:bb:08:86\n",
      "I1212 01:43:45.059135 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 01:43:45.059923 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:c8:45\n",
      "I1212 01:43:45.087173 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 01:43:45.087961 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:48:45\n",
      "I1212 01:43:45.115127 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 01:43:45.115913 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 01:43:45.804032 2588059 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7fe98bfff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:43:48] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.32s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=55.13 GB):   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=55.13 GB):  25%|██▌       | 1/4 [00:00<00:02,  1.04it/s]\r",
      "Capturing batches (bs=3 avail_mem=54.97 GB):  25%|██▌       | 1/4 [00:00<00:02,  1.04it/s]\r",
      "Capturing batches (bs=3 avail_mem=54.97 GB):  50%|█████     | 2/4 [00:01<00:00,  2.19it/s]\r",
      "Capturing batches (bs=2 avail_mem=54.89 GB):  50%|█████     | 2/4 [00:01<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py:1692: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS=\"+dynamo\" for a DEBUG stack trace.\n",
      "  torch._dynamo.utils.warn_once(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=2 avail_mem=54.89 GB):  75%|███████▌  | 3/4 [00:08<00:03,  3.80s/it]\r",
      "Capturing batches (bs=1 avail_mem=54.80 GB):  75%|███████▌  | 3/4 [00:08<00:03,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=1 avail_mem=54.80 GB): 100%|██████████| 4/4 [00:16<00:00,  5.33s/it]\r",
      "Capturing batches (bs=1 avail_mem=54.80 GB): 100%|██████████| 4/4 [00:16<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:44:10] SPECULATIVE_MOE_RUNNER_BACKEND is not initialized, using auto backend\n",
      "[2025-12-12 01:44:10] SPECULATIVE_MOE_A2A_BACKEND is not initialized, using none backend\n",
      "I1212 01:44:10.241904 2588059 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 01:44:10.241923 2588059 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.187.8.134 port: 12001\n",
      "I1212 01:44:10.241940 2588059 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.187.8.134:16700\n",
      "I1212 01:44:10.242026 2588059 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 01:44:10.242698 2588059 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 01:44:10.271169 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 01:44:10.271833 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:48:45\n",
      "I1212 01:44:10.300525 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 01:44:10.301156 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:c8:45\n",
      "I1212 01:44:10.327135 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 01:44:10.327735 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:48:45\n",
      "I1212 01:44:10.355146 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 01:44:10.355773 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:c8:45\n",
      "I1212 01:44:10.383190 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 01:44:10.383817 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:48:45\n",
      "I1212 01:44:10.411612 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 01:44:10.412533 2588059 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:bb:08:86\n",
      "I1212 01:44:10.416942 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 01:44:10.417553 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:c8:45\n",
      "I1212 01:44:10.443179 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 01:44:10.443794 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:48:45\n",
      "I1212 01:44:10.471225 2588059 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 01:44:10.471843 2588059 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 01:44:11.059454 2588059 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7fe0fbfff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.37s/it]\n",
      "\r",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.37s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=53.70 GB):   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=53.70 GB):  25%|██▌       | 1/4 [00:03<00:09,  3.20s/it]\r",
      "Capturing batches (bs=3 avail_mem=53.58 GB):  25%|██▌       | 1/4 [00:03<00:09,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=3 avail_mem=53.58 GB):  50%|█████     | 2/4 [00:03<00:03,  1.58s/it]\r",
      "Capturing batches (bs=2 avail_mem=53.56 GB):  50%|█████     | 2/4 [00:03<00:03,  1.58s/it]\r",
      "Capturing batches (bs=2 avail_mem=53.56 GB):  75%|███████▌  | 3/4 [00:03<00:00,  1.08it/s]\r",
      "Capturing batches (bs=1 avail_mem=53.51 GB):  75%|███████▌  | 3/4 [00:03<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=1 avail_mem=53.51 GB): 100%|██████████| 4/4 [00:06<00:00,  1.70s/it]\r",
      "Capturing batches (bs=1 avail_mem=53.51 GB): 100%|██████████| 4/4 [00:06<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=53.46 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=3 avail_mem=53.39 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=2 avail_mem=53.39 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=1 avail_mem=53.37 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=1 avail_mem=53.37 GB): 100%|██████████| 4/4 [00:00<00:00, 42.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:44:23] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "server_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model meta-llama/Llama-2-7b-chat-hf  --speculative-algorithm EAGLE \\\n",
    "    --speculative-draft-model-path lmsys/sglang-EAGLE-llama2-chat-7B --speculative-num-steps 5 \\\n",
    "        --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --mem-fraction 0.6 \\\n",
    "            --enable-torch-compile --torch-compile-max-bs 2 --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:44:27.261803Z",
     "iopub.status.busy": "2025-12-12T01:44:27.261631Z",
     "iopub.status.idle": "2025-12-12T01:44:27.462734Z",
     "shell.execute_reply": "2025-12-12T01:44:27.461902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Response: ChatCompletion(id='a5849f7c362749389476c29d1cf5ea7c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='  Sure! Here are three countries and their capitals:\\n\\n1. Country: France\\nCapital: Paris\\n2. Country: Japan\\nCapital: Tokyo\\n3. Country: Brazil\\nCapital: Brasília', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content=None), matched_stop=2)], created=1765503867, model='meta-llama/Llama-2-7b-chat-hf', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=48, prompt_tokens=17, total_tokens=65, completion_tokens_details=None, prompt_tokens_details=None, reasoning_tokens=0), metadata={'weight_version': 'default'})</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=64,\n",
    ")\n",
    "\n",
    "print_highlight(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:44:27.464945Z",
     "iopub.status.busy": "2025-12-12T01:44:27.464776Z",
     "iopub.status.idle": "2025-12-12T01:44:27.482003Z",
     "shell.execute_reply": "2025-12-12T01:44:27.481275Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EAGLE-2 Decoding via Frequency-Ranked Speculative Sampling\n",
    "\n",
    "By employing a truncated high-frequency token vocabulary in the draft model, Eagle speculative decoding reduces `lm_head` computational overhead while accelerating the pipeline without quality degradation. For more details, checkout [the paper](https://arxiv.org/pdf/arXiv:2502.14856).\n",
    "\n",
    "In our implementation, set `--speculative-token-map` to enable the optimization. You can get the high-frequency token in FR-Spec from [this model](https://huggingface.co/thunlp/LLaMA3-Instruct-8B-FR-Spec). Or you can obtain high-frequency token by directly downloading these token from [this repo](https://github.com/thunlp/FR-Spec/tree/main?tab=readme-ov-file#prepare-fr-spec-vocabulary-subset).\n",
    "\n",
    "Thanks for the contribution from [Weilin Zhao](https://github.com/Achazwl) and [Zhousx](https://github.com/Zhou-sx). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:44:27.483999Z",
     "iopub.status.busy": "2025-12-12T01:44:27.483848Z",
     "iopub.status.idle": "2025-12-12T01:45:24.607146Z",
     "shell.execute_reply": "2025-12-12T01:45:24.606448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:44:33] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:44:33] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:44:33] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:44:35] WARNING model_config.py:917: Casting torch.bfloat16 to torch.float16.\n",
      "[2025-12-12 01:44:35] WARNING server_args.py:1413: Attention backend not explicitly specified. Use flashinfer backend by default.\n",
      "[2025-12-12 01:44:35] WARNING server_args.py:1763: Overlap scheduler is disabled because of using eagle3 or standalone speculative decoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:44:35] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:44:41] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:44:41] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:44:41] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 01:44:41] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:44:41] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:44:41] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:44:43] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:44:43] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 01:44:44.327113 2589066 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 01:44:44.327131 2589066 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.187.8.134 port: 12001\n",
      "I1212 01:44:44.327150 2589066 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.187.8.134:16414\n",
      "I1212 01:44:44.327219 2589066 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 01:44:44.329898 2589066 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 01:44:44.359144 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 01:44:44.359792 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:48:45\n",
      "I1212 01:44:44.387177 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 01:44:44.387765 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:c8:45\n",
      "I1212 01:44:44.415120 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 01:44:44.415709 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:48:45\n",
      "I1212 01:44:44.504290 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 01:44:44.504933 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 01:44:44.527235 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 01:44:44.527890 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:48:45\n",
      "I1212 01:44:44.555404 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 01:44:44.556217 2589066 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:bb:08:86\n",
      "I1212 01:44:44.583105 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 01:44:44.583719 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:c8:45\n",
      "I1212 01:44:44.611119 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 01:44:44.611732 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:48:45\n",
      "I1212 01:44:44.639138 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 01:44:44.639760 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 01:44:45.242744 2589066 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f8307fff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:44:47] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.21s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.64s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=60.13 GB):   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=60.13 GB):  25%|██▌       | 1/4 [00:00<00:01,  2.72it/s]\r",
      "Capturing batches (bs=3 avail_mem=59.89 GB):  25%|██▌       | 1/4 [00:00<00:01,  2.72it/s]\r",
      "Capturing batches (bs=3 avail_mem=59.89 GB):  50%|█████     | 2/4 [00:00<00:00,  4.00it/s]\r",
      "Capturing batches (bs=2 avail_mem=59.83 GB):  50%|█████     | 2/4 [00:00<00:00,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=2 avail_mem=59.83 GB):  75%|███████▌  | 3/4 [00:00<00:00,  4.20it/s]\r",
      "Capturing batches (bs=1 avail_mem=59.74 GB):  75%|███████▌  | 3/4 [00:00<00:00,  4.20it/s]\r",
      "Capturing batches (bs=1 avail_mem=59.74 GB): 100%|██████████| 4/4 [00:00<00:00,  4.89it/s]\r",
      "Capturing batches (bs=1 avail_mem=59.74 GB): 100%|██████████| 4/4 [00:00<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:45:06] SPECULATIVE_MOE_RUNNER_BACKEND is not initialized, using auto backend\n",
      "[2025-12-12 01:45:06] SPECULATIVE_MOE_A2A_BACKEND is not initialized, using none backend\n",
      "[2025-12-12 01:45:06] Warning: Target model's context_length (8192) is greater than the derived context_length (2048). This may lead to incorrect model outputs or CUDA errors. Note that the derived context_length may differ from max_position_embeddings in the model's config.\n",
      "[2025-12-12 01:45:06] Overriding the draft model's max_position_embeddings to 8192.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 01:45:06.456552 2589066 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 01:45:06.456569 2589066 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.187.8.134 port: 12001\n",
      "I1212 01:45:06.456585 2589066 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.187.8.134:15586\n",
      "I1212 01:45:06.456684 2589066 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 01:45:06.457707 2589066 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 01:45:06.483124 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 01:45:06.483834 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:48:45\n",
      "I1212 01:45:06.511091 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 01:45:06.511736 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:c8:45\n",
      "I1212 01:45:06.536099 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 01:45:06.536778 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:48:45\n",
      "I1212 01:45:06.559979 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 01:45:06.560633 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:c8:45\n",
      "I1212 01:45:06.587006 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 01:45:06.587615 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:48:45\n",
      "I1212 01:45:06.611011 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 01:45:06.611943 2589066 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:bb:08:86\n",
      "I1212 01:45:06.639019 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 01:45:06.639652 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 01:45:06.667083 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 01:45:06.667732 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:48:45\n",
      "I1212 01:45:06.695000 2589066 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 01:45:06.695633 2589066 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 01:45:07.514832 2589066 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f7bdbfff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.26s/it]\n",
      "\r",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.26s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=58.58 GB):   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=58.58 GB):  25%|██▌       | 1/4 [00:03<00:10,  3.63s/it]\r",
      "Capturing batches (bs=3 avail_mem=58.47 GB):  25%|██▌       | 1/4 [00:03<00:10,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=3 avail_mem=58.47 GB):  50%|█████     | 2/4 [00:04<00:03,  1.82s/it]\r",
      "Capturing batches (bs=2 avail_mem=58.44 GB):  50%|█████     | 2/4 [00:04<00:03,  1.82s/it]\r",
      "Capturing batches (bs=2 avail_mem=58.44 GB):  75%|███████▌  | 3/4 [00:04<00:01,  1.08s/it]\r",
      "Capturing batches (bs=1 avail_mem=58.40 GB):  75%|███████▌  | 3/4 [00:04<00:01,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=1 avail_mem=58.40 GB): 100%|██████████| 4/4 [00:06<00:00,  1.54s/it]\r",
      "Capturing batches (bs=1 avail_mem=58.40 GB): 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=58.36 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=3 avail_mem=58.28 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=2 avail_mem=58.28 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=1 avail_mem=58.26 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=1 avail_mem=58.26 GB): 100%|██████████| 4/4 [00:00<00:00, 94.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:45:19] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "server_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model meta-llama/Meta-Llama-3-8B-Instruct --speculative-algorithm EAGLE \\\n",
    "    --speculative-draft-model-path lmsys/sglang-EAGLE-LLaMA3-Instruct-8B --speculative-num-steps 5 \\\n",
    "    --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --speculative-token-map thunlp/LLaMA3-Instruct-8B-FR-Spec/freq_32768.pt \\\n",
    "    --mem-fraction 0.7 --cuda-graph-max-bs 2 --dtype float16  --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:45:24.608894Z",
     "iopub.status.busy": "2025-12-12T01:45:24.608653Z",
     "iopub.status.idle": "2025-12-12T01:45:24.837812Z",
     "shell.execute_reply": "2025-12-12T01:45:24.837205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Response: ChatCompletion(id='cc21b5d1a3164277926bf318a73b962d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here are 3 countries and their capitals:\\n\\n1. **France** - **Paris**\\n2. **Japan** - **Tokyo**\\n3. **Australia** - **Canberra**', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content=None), matched_stop=128009)], created=1765503924, model='meta-llama/Meta-Llama-3-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=39, prompt_tokens=18, total_tokens=57, completion_tokens_details=None, prompt_tokens_details=None, reasoning_tokens=0), metadata={'weight_version': 'default'})</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=64,\n",
    ")\n",
    "\n",
    "print_highlight(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:45:24.839331Z",
     "iopub.status.busy": "2025-12-12T01:45:24.839186Z",
     "iopub.status.idle": "2025-12-12T01:45:24.856412Z",
     "shell.execute_reply": "2025-12-12T01:45:24.855571Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EAGLE-3 Decoding\n",
    "\n",
    "You can enable EAGLE-3 decoding by setting `--speculative-algorithm EAGLE3` and choosing an appropriate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:45:24.858391Z",
     "iopub.status.busy": "2025-12-12T01:45:24.858123Z",
     "iopub.status.idle": "2025-12-12T01:46:20.007527Z",
     "shell.execute_reply": "2025-12-12T01:46:20.006728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:45:30] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:45:30] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:45:30] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:45:32] WARNING model_config.py:917: Casting torch.bfloat16 to torch.float16.\n",
      "[2025-12-12 01:45:32] WARNING server_args.py:1413: Attention backend not explicitly specified. Use flashinfer backend by default.\n",
      "[2025-12-12 01:45:32] WARNING server_args.py:1763: Overlap scheduler is disabled because of using eagle3 or standalone speculative decoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:45:32] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:45:38] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:45:38] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:45:38] INFO utils.py:164: NumExpr defaulting to 16 threads.\n",
      "[2025-12-12 01:45:38] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:45:38] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:45:38] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:45:40] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:45:41] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 01:45:41.507543 2589953 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 01:45:41.507561 2589953 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.187.8.134 port: 12001\n",
      "I1212 01:45:41.507583 2589953 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.187.8.134:15606\n",
      "I1212 01:45:41.507669 2589953 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 01:45:41.510650 2589953 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 01:45:41.539106 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 01:45:41.539773 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:48:45\n",
      "I1212 01:45:41.545711 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 01:45:41.546347 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:c8:45\n",
      "I1212 01:45:41.575176 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 01:45:41.575831 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:48:45\n",
      "I1212 01:45:41.688311 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 01:45:41.689023 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:c8:45\n",
      "I1212 01:45:41.711138 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 01:45:41.711810 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:48:45\n",
      "I1212 01:45:41.739398 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 01:45:41.740223 2589953 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:bb:08:86\n",
      "I1212 01:45:41.767114 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 01:45:41.767753 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:c8:45\n",
      "I1212 01:45:41.795132 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 01:45:41.795776 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:48:45\n",
      "I1212 01:45:41.823151 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 01:45:41.823824 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 01:45:42.608451 2589953 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f2cb3fff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:45:45] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.20s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.69s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=60.00 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=60.00 GB):  25%|██▌       | 1/4 [00:00<00:00,  6.01it/s]\r",
      "Capturing batches (bs=3 avail_mem=59.82 GB):  25%|██▌       | 1/4 [00:00<00:00,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=2 avail_mem=59.75 GB):  25%|██▌       | 1/4 [00:00<00:00,  6.01it/s]\r",
      "Capturing batches (bs=2 avail_mem=59.75 GB):  75%|███████▌  | 3/4 [00:00<00:00, 12.02it/s]\r",
      "Capturing batches (bs=1 avail_mem=59.67 GB):  75%|███████▌  | 3/4 [00:00<00:00, 12.02it/s]\r",
      "Capturing batches (bs=1 avail_mem=59.67 GB): 100%|██████████| 4/4 [00:00<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:46:04] SPECULATIVE_MOE_RUNNER_BACKEND is not initialized, using auto backend\n",
      "[2025-12-12 01:46:04] SPECULATIVE_MOE_A2A_BACKEND is not initialized, using none backend\n",
      "[2025-12-12 01:46:04] Warning: Target model's context_length (131072) is greater than the derived context_length (2048). This may lead to incorrect model outputs or CUDA errors. Note that the derived context_length may differ from max_position_embeddings in the model's config.\n",
      "[2025-12-12 01:46:04] Overriding the draft model's max_position_embeddings to 131072.\n",
      "I1212 01:46:04.507577 2589953 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 01:46:04.507598 2589953 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.187.8.134 port: 12001\n",
      "I1212 01:46:04.507613 2589953 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.187.8.134:16626\n",
      "I1212 01:46:04.507719 2589953 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 01:46:04.508410 2589953 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 01:46:04.535192 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 01:46:04.535861 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:48:45\n",
      "I1212 01:46:04.563148 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 01:46:04.563761 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:c8:45\n",
      "I1212 01:46:04.591132 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 01:46:04.591729 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:48:45\n",
      "I1212 01:46:04.619076 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 01:46:04.619676 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 01:46:04.647143 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 01:46:04.647773 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:48:45\n",
      "I1212 01:46:04.675567 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 01:46:04.676472 2589953 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:bb:08:86\n",
      "I1212 01:46:04.703140 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 01:46:04.703747 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:c8:45\n",
      "I1212 01:46:04.731151 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 01:46:04.731772 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:48:45\n",
      "I1212 01:46:04.759145 2589953 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 01:46:04.759764 2589953 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 01:46:05.346603 2589953 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f257ffff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.66it/s]\n",
      "\r",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.66it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=58.32 GB):   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=58.32 GB):  25%|██▌       | 1/4 [00:02<00:08,  2.84s/it]\r",
      "Capturing batches (bs=3 avail_mem=58.27 GB):  25%|██▌       | 1/4 [00:02<00:08,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=3 avail_mem=58.27 GB):  50%|█████     | 2/4 [00:03<00:02,  1.39s/it]\r",
      "Capturing batches (bs=2 avail_mem=58.23 GB):  50%|█████     | 2/4 [00:03<00:02,  1.39s/it]\r",
      "Capturing batches (bs=1 avail_mem=58.19 GB):  50%|█████     | 2/4 [00:03<00:02,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=1 avail_mem=58.19 GB): 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]\r",
      "Capturing batches (bs=1 avail_mem=58.19 GB): 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=58.13 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=3 avail_mem=58.06 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=2 avail_mem=58.06 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=1 avail_mem=58.04 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=1 avail_mem=58.04 GB): 100%|██████████| 4/4 [00:00<00:00, 99.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:46:15] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "server_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model meta-llama/Llama-3.1-8B-Instruct  --speculative-algorithm EAGLE3 \\\n",
    "    --speculative-draft-model-path jamesliu1/sglang-EAGLE3-Llama-3.1-Instruct-8B --speculative-num-steps 5 \\\n",
    "        --speculative-eagle-topk 8 --speculative-num-draft-tokens 32 --mem-fraction 0.6 \\\n",
    "        --cuda-graph-max-bs 2 --dtype float16 --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:46:20.009368Z",
     "iopub.status.busy": "2025-12-12T01:46:20.009213Z",
     "iopub.status.idle": "2025-12-12T01:46:20.220940Z",
     "shell.execute_reply": "2025-12-12T01:46:20.220149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Response: ChatCompletion(id='6bbb379ee23e4057815570203ccbc80f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here are 3 countries and their capitals:\\n\\n1. Country: Japan\\n   Capital: Tokyo\\n\\n2. Country: Australia\\n   Capital: Canberra\\n\\n3. Country: Brazil\\n   Capital: Brasília', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content=None), matched_stop=128009)], created=1765503980, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=43, prompt_tokens=43, total_tokens=86, completion_tokens_details=None, prompt_tokens_details=None, reasoning_tokens=0), metadata={'weight_version': 'default'})</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=64,\n",
    ")\n",
    "\n",
    "print_highlight(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:46:20.222592Z",
     "iopub.status.busy": "2025-12-12T01:46:20.222441Z",
     "iopub.status.idle": "2025-12-12T01:46:20.244233Z",
     "shell.execute_reply": "2025-12-12T01:46:20.243536Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Token Prediction\n",
    "\n",
    "We support [MTP(Multi-Token Prediction)](https://arxiv.org/pdf/2404.19737) in SGLang by using speculative decoding. We use Xiaomi/MiMo-7B-RL model as example here (deepseek mtp usage refer to [deepseek doc](../basic_usage/deepseek.md#multi-token-prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:46:20.245920Z",
     "iopub.status.busy": "2025-12-12T01:46:20.245776Z",
     "iopub.status.idle": "2025-12-12T01:46:58.339575Z",
     "shell.execute_reply": "2025-12-12T01:46:58.338863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:46:25] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:46:25] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:46:25] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:46:28] WARNING server_args.py:1413: Attention backend not explicitly specified. Use flashinfer backend by default.\n",
      "[2025-12-12 01:46:28] WARNING server_args.py:1763: Overlap scheduler is disabled because of using eagle3 or standalone speculative decoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:46:34] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:46:34] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:46:34] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:46:35] INFO utils.py:148: Note: detected 112 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2025-12-12 01:46:35] INFO utils.py:151: Note: NumExpr detected 112 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-12-12 01:46:35] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I1212 01:46:38.336585 2590750 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 01:46:38.336601 2590750 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.187.8.134 port: 12001\n",
      "I1212 01:46:38.336624 2590750 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.187.8.134:16855\n",
      "I1212 01:46:38.336719 2590750 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 01:46:38.339804 2590750 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 01:46:38.367527 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 01:46:38.368338 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:48:45\n",
      "I1212 01:46:38.374404 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 01:46:38.375159 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:c8:45\n",
      "I1212 01:46:38.399386 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 01:46:38.400152 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:48:45\n",
      "I1212 01:46:38.488328 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 01:46:38.489061 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:c8:45\n",
      "I1212 01:46:38.495061 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 01:46:38.495702 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:48:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 01:46:38.519394 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 01:46:38.520249 2590750 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:bb:08:86\n",
      "I1212 01:46:38.526223 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 01:46:38.526860 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:c8:45\n",
      "I1212 01:46:38.532799 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 01:46:38.533429 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:48:45\n",
      "I1212 01:46:38.539347 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 01:46:38.539975 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 01:46:39.490888 2590750 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f9d4bfff010, len: 2147483648: Operation not permitted [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:46:42] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.29it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.29it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=60.52 GB):   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=60.52 GB):  25%|██▌       | 1/4 [00:00<00:01,  2.77it/s]\r",
      "Capturing batches (bs=3 avail_mem=60.38 GB):  25%|██▌       | 1/4 [00:00<00:01,  2.77it/s]\r",
      "Capturing batches (bs=2 avail_mem=60.31 GB):  25%|██▌       | 1/4 [00:00<00:01,  2.77it/s]\r",
      "Capturing batches (bs=1 avail_mem=60.22 GB):  25%|██▌       | 1/4 [00:00<00:01,  2.77it/s]\r",
      "Capturing batches (bs=1 avail_mem=60.22 GB): 100%|██████████| 4/4 [00:00<00:00,  9.03it/s]\r",
      "Capturing batches (bs=1 avail_mem=60.22 GB): 100%|██████████| 4/4 [00:00<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:46:48] SPECULATIVE_MOE_RUNNER_BACKEND is not initialized, using auto backend\n",
      "[2025-12-12 01:46:48] SPECULATIVE_MOE_A2A_BACKEND is not initialized, using none backend\n",
      "I1212 01:46:48.881862 2590750 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\n",
      "I1212 01:46:48.881880 2590750 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.187.8.134 port: 12001\n",
      "I1212 01:46:48.881896 2590750 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.187.8.134:16905\n",
      "I1212 01:46:48.881989 2590750 transfer_engine.cpp:185] Auto-discovering topology...\n",
      "I1212 01:46:48.882701 2590750 transfer_engine.cpp:200] Topology discovery complete. Found 9 HCAs.\n",
      "I1212 01:46:48.903301 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce0/\u0001\n",
      "I1212 01:46:48.903981 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:48:45\n",
      "I1212 01:46:48.931187 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce1/\u0001\n",
      "I1212 01:46:48.931830 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce1, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4c:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1212 01:46:48.959179 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce2/\u0001\n",
      "I1212 01:46:48.959836 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce2, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:48:45\n",
      "I1212 01:46:48.987309 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce3/\u0001\n",
      "I1212 01:46:48.987954 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce3, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4d:c8:45\n",
      "I1212 01:46:49.015101 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce4/\u0001\n",
      "I1212 01:46:49.015717 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce4, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:48:45\n",
      "I1212 01:46:49.043598 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_eth0/\u0001\n",
      "I1212 01:46:49.044548 2590750 rdma_context.cpp:126] RDMA device: mlx5_eth0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:0a:bb:08:86\n",
      "I1212 01:46:49.071105 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce5/\u0001\n",
      "I1212 01:46:49.071750 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce5, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4e:c8:45\n",
      "I1212 01:46:49.099085 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce6/\u0001\n",
      "I1212 01:46:49.099723 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce6, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:48:45\n",
      "I1212 01:46:49.127099 2590750 rdma_context.cpp:533] Find best gid index: 3 on mlx5_roce7/\u0001\n",
      "I1212 01:46:49.127907 2590750 rdma_context.cpp:126] RDMA device: mlx5_roce7, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:16:4f:c8:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1212 01:46:49.851728 2590750 memory_location.cpp:72] Failed to get NUMA node, addr: 0x7f960ffff010, len: 2147483648: Operation not permitted [1]\n",
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  8.19it/s]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  7.87it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=59.47 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=3 avail_mem=59.39 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=2 avail_mem=59.39 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=1 avail_mem=59.37 GB):   0%|          | 0/4 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=1 avail_mem=59.37 GB): 100%|██████████| 4/4 [00:00<00:00, 52.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 01:46:53] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "server_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "    python3 -m sglang.launch_server --model-path XiaomiMiMo/MiMo-7B-RL --host 0.0.0.0 --trust-remote-code \\\n",
    "    --speculative-algorithm EAGLE --speculative-num-steps 1 --speculative-eagle-topk 1 --speculative-num-draft-tokens 2 \\\n",
    "    --mem-fraction 0.5 --log-level warning\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:46:58.341382Z",
     "iopub.status.busy": "2025-12-12T01:46:58.341217Z",
     "iopub.status.idle": "2025-12-12T01:47:01.891364Z",
     "shell.execute_reply": "2025-12-12T01:47:01.890679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'id': 'c138f2802c6e43479328cfab0be81598', 'object': 'chat.completion', 'created': 1765504021, 'model': 'XiaomiMiMo/MiMo-7B-RL', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '<think>\\nOkay, so the user is asking, \"What is the capital of France?\" Let me start by recalling what I know about France. France is a country in Europe, right? I remember that Paris is a major city there. But wait, I should make sure that\\'s correct. Maybe I can think of some famous landmarks in France. The Eiffel Tower comes to mind, and that\\'s in Paris. Also, the Louvre Museum, which I think is located there as well. There\\'s also the France Tower, but wait, isn\\'t that a different structure? Maybe I\\'m confusing it with something else.\\n\\nWait, no, the Eiffel Tower is definitely the main one in Paris. So if Paris has all these iconic places, it\\'s probably the capital. But just to be thorough, maybe I should check if there\\'s another city that\\'s the capital. Sometimes countries have surprising capitals. For example, I know Berlin is the capital of Germany, not just a big city. But France is a very old country, so maybe Paris has been the capital for a long time.\\n\\nI think historically, Paris was the capital of France during the French Revolution. That was in the late 18th century. Before that, maybe some other regions were considered capitals, but I\\'m pretty sure Paris has been the main capital city since then. Let me also think about government structures. The President of France resides in Paris, right? The Élysée Palace. That\\'s a strong indication that Paris is the capital. Also, many government buildings and important institutions are located there, like the National Assembly.\\n\\nAnother point: the currency of France is the Euro, and the symbol for Euro is EUR. The headquarters of the European Central Bank is in Frankfurt, Germany, but that\\'s not directly related. However, Paris being the capital makes sense because it\\'s the largest city in France both in population and area. So, putting it all together, I\\'m pretty confident that the answer is Paris. But to be absolutely sure, maybe I can recall any recent changes. Has there been any official announcement that the capital is changing? I don\\'t recall any such news. So, yes, the capital of France is Paris.\\n</think>\\nThe capital of France is **Paris**. This vibrant city is home to iconic landmarks such as the Eiffel Tower, the Louvre Museum, and the historic Parisian district of the Marais. It serves as the nation\\'s political, cultural, and economic center, housing major institutions like the Élysée Palace (where the French President resides) and the National Assembly. With its rich history, global influence, and status as France\\'s largest city, Paris remains a symbol of European heritage and modernity.', 'reasoning_content': None, 'tool_calls': None}, 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 151645}], 'usage': {'prompt_tokens': 26, 'total_tokens': 584, 'completion_tokens': 558, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'metadata': {'weight_version': 'default'}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = f\"http://localhost:{port}/v1/chat/completions\"\n",
    "\n",
    "data = {\n",
    "    \"model\": \"XiaomiMiMo/MiMo-7B-RL\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:47:01.892992Z",
     "iopub.status.busy": "2025-12-12T01:47:01.892845Z",
     "iopub.status.idle": "2025-12-12T01:47:01.908194Z",
     "shell.execute_reply": "2025-12-12T01:47:01.907626Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "EAGLE process is as follows:\n",
    "\n",
    "- Within EAGLE the draft model predicts the next feature vector, i.e. the last hidden state of the original LLM, using the feature sequence $(f_1, ..., f_k)$ and the token sequence $(t_2, ..., t_{k+1})$. \n",
    "- The next token is then sampled from $p_{k+2}=\\text{LMHead}(f_{k+1})$. Afterwards, the two sequences are extended in a tree style—branching out multiple potential continuations, with the branching factor per step controlled by the `speculative_eagle_topk` parameter—to ensure a more coherent connection of context, and are given as input again.\n",
    "- EAGLE-2 additionally uses the draft model to evaluate how probable certain branches in the draft tree are, dynamically stopping the expansion of unlikely branches. After the expansion phase, reranking is employed to select only the top `speculative_num_draft_tokens` final nodes as draft tokens.\n",
    "- EAGLE-3 removes the feature prediction objective, incorporates low and mid-layer features, and is trained in an on-policy manner.\n",
    "\n",
    "This enhances drafting accuracy by operating on the features instead of tokens for more regular inputs and passing the tokens from the next timestep additionally to minimize randomness effects from sampling. Furthermore the dynamic adjustment of the draft tree and selection of reranked final nodes increases acceptance rate of draft tokens further. For more details see [EAGLE-2](https://arxiv.org/abs/2406.16858) and [EAGLE-3](https://arxiv.org/abs/2503.01840) paper.\n",
    "\n",
    "\n",
    "For guidance how to train your own EAGLE model please see the [EAGLE repo](https://github.com/SafeAILab/EAGLE/tree/main?tab=readme-ov-file#train)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
