{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs For Reasoning Models\n",
    "\n",
    "When working with reasoning models that use special tokens like `<think>...</think>` to denote reasoning sections, you might want to allow free-form text within these sections while still enforcing grammar constraints on the rest of the output.\n",
    "\n",
    "SGLang provides a feature to disable grammar restrictions within reasoning sections. This is particularly useful for models that need to perform complex reasoning steps before providing a structured output.\n",
    "\n",
    "To enable this feature, use the `--reasoning-parser` flag which decide the think_end_token, such as `</think>`, when launching the server. You can also specify the reasoning parser using the `--reasoning-parser` flag.\n",
    "\n",
    "## Supported Models\n",
    "\n",
    "Currently, SGLang supports the following reasoning models:\n",
    "- [DeepSeek R1 series](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d): The reasoning content is wrapped with `<think>` and `</think>` tags.\n",
    "- [QwQ](https://huggingface.co/Qwen/QwQ-32B): The reasoning content is wrapped with `<think>` and `</think>` tags.\n",
    "\n",
    "\n",
    "## Usage\n",
    "\n",
    "## OpenAI Compatible API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the `--grammar-backend`, `--reasoning-parser` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:12:05.550789Z",
     "iopub.status.busy": "2026-02-27T02:12:05.550699Z",
     "iopub.status.idle": "2026-02-27T02:12:40.485670Z",
     "shell.execute_reply": "2026-02-27T02:12:40.484974Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 02:12:11] INFO utils.py:148: Note: detected 192 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 02:12:11] INFO utils.py:151: Note: NumExpr detected 192 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 02:12:11] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-27 02:12:16] INFO utils.py:148: Note: detected 192 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-02-27 02:12:16] INFO utils.py:151: Note: NumExpr detected 192 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-02-27 02:12:16] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-27 02:12:17] INFO server_args.py:1859: Attention backend not specified. Use fa3 backend by default.\n",
      "[2026-02-27 02:12:17] INFO server_args.py:2929: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-27 02:12:23] INFO utils.py:148: Note: detected 192 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-02-27 02:12:23] INFO utils.py:151: Note: NumExpr detected 192 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-02-27 02:12:23] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-27 02:12:23] INFO utils.py:148: Note: detected 192 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2026-02-27 02:12:23] INFO utils.py:151: Note: NumExpr detected 192 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2026-02-27 02:12:23] INFO utils.py:164: NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-27 02:12:29] Ignore import error when loading sglang.srt.models.glm_ocr: No module named 'transformers.models.glm_ocr'\n",
      "[2026-02-27 02:12:29] Ignore import error when loading sglang.srt.models.glm_ocr_nextn: No module named 'transformers.models.glm_ocr'\n",
      "[2026-02-27 02:12:29] Ignore import error when loading sglang.srt.models.glmasr: cannot import name 'GlmAsrConfig' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.51s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.52s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=4 avail_mem=104.32 GB):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=4 avail_mem=104.32 GB):  33%|███▎      | 1/3 [00:00<00:00,  4.47it/s]\r",
      "Capturing batches (bs=2 avail_mem=104.26 GB):  33%|███▎      | 1/3 [00:00<00:00,  4.47it/s]\r",
      "Capturing batches (bs=1 avail_mem=104.25 GB):  33%|███▎      | 1/3 [00:00<00:00,  4.47it/s]\r",
      "Capturing batches (bs=1 avail_mem=104.25 GB): 100%|██████████| 3/3 [00:00<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/fastapi/routing.py:105: FastAPIDeprecationWarning: ORJSONResponse is deprecated, FastAPI now serializes data directly to JSON bytes via Pydantic when a return type or response model is set, which is faster and doesn't need a custom response class. Read more in the FastAPI docs: https://fastapi.tiangolo.com/advanced/custom-response/#orjson-or-response-model and https://fastapi.tiangolo.com/tutorial/response-model/\n",
      "  response = await f(request)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>        NOTE: Typically, the server runs in a separate terminal.<br>        In this notebook, we run the server and notebook code together, so their outputs are combined.<br>        To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>        To reduce the log length, we set the log level to warning for the server, the default log level is info.<br>        We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.<br>        </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from sglang.test.doc_patch import launch_server_cmd\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --host 0.0.0.0 --reasoning-parser deepseek-r1 --log-level warning\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\", process=server_process)\n",
    "client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "you can directly define a JSON schema or use [Pydantic](https://docs.pydantic.dev/latest/) to define and validate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pydantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:12:40.487471Z",
     "iopub.status.busy": "2026-02-27T02:12:40.487167Z",
     "iopub.status.idle": "2026-02-27T02:12:43.021371Z",
     "shell.execute_reply": "2026-02-27T02:12:43.020088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so I need to figure out the capital of France and its population. I know that the capital of France is Paris, but I'm not exactly sure about the current population numbers. I remember that Paris is a very big city, but I think it's not the largest in the world. Maybe around 20 million? I'm not certain, though. I should check if that's correct.<br><br>Wait, I think the population might have changed a bit over the years. I recall reading somewhere that Paris has grown a lot, especially with the influx of people moving there for work. But I'm not sure if it's exactly 21 million or maybe a bit more. I should look up the latest data to confirm.<br><br>I also wonder if the population figure includes just the city proper or the entire metropolitan area. Sometimes, people talk about the metro area, which can be much larger. But I think the question is specifically about the capital, so probably just the city limits. <br><br>Another thing to consider is that population figures can vary depending on the source. Some might cite estimates from government agencies, while others might use more recent surveys. I should make sure to use a reliable source to get the most accurate number.<br><br>I'm pretty confident that Paris is the capital, so I don't need to worry about that part. But for the population, I should double-check. Maybe I can recall that in recent years, Paris has been growing steadily. I think it's somewhere between 20 and 22 million. Let me try to remember any specific numbers or events that might have affected the population, like the COVID-19 pandemic. I think that had a temporary impact, but the city has been recovering since then.<br><br>So, putting it all together, I'm going to say that the capital of France is Paris, and its population is approximately 21 million people. I'll present this information in a JSON format as requested.<br><br><br>content: {<br><br>\"name\": \"Paris\",<br>\"population\": 21000000<br>}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define the schema using Pydantic\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n",
    "    population: int = Field(..., description=\"Population of the capital city\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Give me the information and population of the capital of France in the JSON format.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"foo\",\n",
    "            # convert the pydantic model to json schema\n",
    "            \"schema\": CapitalInfo.model_json_schema(),\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Schema Directly**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:12:43.023333Z",
     "iopub.status.busy": "2026-02-27T02:12:43.023164Z",
     "iopub.status.idle": "2026-02-27T02:12:45.104032Z",
     "shell.execute_reply": "2026-02-27T02:12:45.103224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so I need to figure out the capital of France and its population. I know that the capital of France is Paris, but I'm not exactly sure about the current population numbers. I remember that Paris is a very big city, but I think it's not the largest in the world. Maybe around 20 million? I'm not certain, though. I should check if that's correct.<br><br>Wait, I think the population might have changed a bit over the years. I recall reading somewhere that Paris has grown a lot, especially with the influx of people moving there for work. But I'm not sure if it's exactly 21 million or maybe a bit more. I should look up the latest data to confirm.<br><br>I also wonder if the population figure includes just the city proper or the entire metropolitan area. Sometimes, people talk about the metro area, which can be much larger. But I think the question is specifically about the capital, so probably just the city limits. <br><br>Another thing to consider is that population figures can vary depending on the source. Some might cite estimates from government agencies, while others might use more recent surveys. I should make sure to use a reliable source to get the most accurate number.<br><br>I'm pretty confident that Paris is the capital, so I don't need to worry about that part. But for the population, I should double-check. Maybe I can recall that in recent years, Paris has been growing steadily. I think it's somewhere between 20 and 22 million. Let me try to remember any specific numbers or events that might have affected the population, like the COVID-19 pandemic. I think that had a temporary impact, but the city has been recovering since then.<br><br>So, putting it all together, I'm going to say that the capital of France is Paris, and its population is approximately 21 million people. I'll present this information in a JSON format as requested.<br><br><br>content: {<br><br>\"name\": \"Paris\",<br>\"population\": 21000000<br>}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_schema = json.dumps(\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n",
    "            \"population\": {\"type\": \"integer\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"population\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Give me the information and population of the capital of France in the JSON format.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\"name\": \"foo\", \"schema\": json.loads(json_schema)},\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:12:45.105723Z",
     "iopub.status.busy": "2026-02-27T02:12:45.105579Z",
     "iopub.status.idle": "2026-02-27T02:12:45.846588Z",
     "shell.execute_reply": "2026-02-27T02:12:45.845889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so I need to figure out the capital of France and its population. I know that the capital of France is Paris, but I'm not entirely sure about the population number. I think it's a big city, so maybe around 3 million? But I'm not certain. I should probably double-check that. Maybe I can recall that Paris is one of the largest cities in Europe, so 3.5 million sounds about right. I don't think it's more than that because I've heard it's a major tourist attraction but not the largest in the world. So, I'll go with Paris having a population of approximately 3.5 million.<br><br><br>content: London is the capital of France</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ebnf_grammar = \"\"\"\n",
    "root ::= city | description\n",
    "city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\n",
    "description ::= city \" is \" status\n",
    "status ::= \"the capital of \" country\n",
    "country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful geography bot.\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Give me the information and population of the capital of France in the JSON format.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    extra_body={\"ebnf\": ebnf_grammar},\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:12:45.849038Z",
     "iopub.status.busy": "2026-02-27T02:12:45.848895Z",
     "iopub.status.idle": "2026-02-27T02:12:47.312564Z",
     "shell.execute_reply": "2026-02-27T02:12:47.311870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so I need to figure out the capital of France. Hmm, I remember learning a bit about France in school, but I'm not 100% sure. Let me think. I know that Paris is a major city in France, and it's often referred to as the \"City of Light\" because of the famous Eiffel Tower. But is it the capital? I think so, but I'm not entirely certain. <br><br>Wait, I also recall that there's another city called Lyon. Isn't that the capital? No, I think I'm mixing things up. Lyon is definitely a significant city in France, known for its gastronomy and being the second-largest city. But I'm pretty sure Paris is the capital. <br><br>Let me try to remember any other capitals I know. London is the capital of the UK, Rome is Italy, Beijing is China, and Tokyo is Japan. So, for France, it's probably Paris. I think I've heard it a lot in news and media. Also, the Eiffel Tower is in Paris, which is a symbol of the country, so that reinforces the idea that Paris is the capital. <br><br>I don't remember any major political figures from Lyon; they're more from France's historical past. Maybe some people confuse Lyon with the capital, but I'm pretty confident it's Paris. I'll go with Paris as the capital of France.<br><br><br>content: Paris</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=[\n",
    "        {\"role\": \"assistant\", \"content\": \"What is the capital of France?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    extra_body={\"regex\": \"(Paris|London)\"},\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:12:47.314260Z",
     "iopub.status.busy": "2026-02-27T02:12:47.314127Z",
     "iopub.status.idle": "2026-02-27T02:12:49.284066Z",
     "shell.execute_reply": "2026-02-27T02:12:49.283346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, I need to figure out how to respond to the user's request. They mentioned being in New York and want the current date and time along with the weather. Let me break this down.<br><br>First, the user wants the current date and time for their location, which is New York. I remember there's a function called get_current_date that can do this. It requires a timezone parameter. Since the user is in New York, I should use the timezone 'America/New_York'. So, I'll call get_current_date with that timezone.<br><br>Next, they also want the weather. For that, I should use the get_current_weather function. The city is New York, the state is NY, and I'll set the unit to Fahrenheit since they didn't specify otherwise. So, I'll call get_current_weather with city='New York', state='NY', and unit='fahrenheit'.<br><br>I need to make sure each function call is separate and follows the required format. I'll structure each function call on its own line with the correct parameters. Also, I should include the sources where I got the information from, which are the function definitions provided.<br><br>Wait, the user mentioned using the function names and parameters in the response. So, I'll format each function call with the function name and the parameters as a JSON object. That way, it's clear and follows the instructions.<br><br>Let me double-check the parameters. For get_current_date, the parameter is timezone, and for get_current_weather, it's city, state, and unit. All required parameters are included. I think that covers everything the user asked for.<br><br><br>content: <function=get_current_date>{\"timezone\": \"America/New_York\"}</function>  <br><function=get_current_weather>{\"city\": \"New York\", \"state\": \"NY\", \"unit\": \"fahrenheit\"}</function></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_get_current_weather = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n",
    "                },\n",
    "                \"state\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"the two-letter abbreviation for the state that the city is\"\n",
    "                    \" in, e.g. 'CA' which would mean 'California'\",\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unit to fetch the temperature in\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"city\", \"state\", \"unit\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "tool_get_current_date = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"Get the current date and time for a given timezone\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"timezone\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The timezone to fetch the current date and time for, e.g. 'America/New_York'\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"timezone\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "schema_get_current_weather = tool_get_current_weather[\"function\"][\"parameters\"]\n",
    "schema_get_current_date = tool_get_current_date[\"function\"][\"parameters\"]\n",
    "\n",
    "\n",
    "def get_messages():\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "# Tool Instructions\n",
    "- Always execute python code in messages that you share.\n",
    "- When looking for real time information use relevant functions if available else fallback to brave_search\n",
    "You have access to the following functions:\n",
    "Use the function 'get_current_weather' to: Get the current weather in a given location\n",
    "{tool_get_current_weather[\"function\"]}\n",
    "Use the function 'get_current_date' to: Get the current date and time for a given timezone\n",
    "{tool_get_current_date[\"function\"]}\n",
    "If a you choose to call a function ONLY reply in the following format:\n",
    "<{{start_tag}}={{function_name}}>{{parameters}}{{end_tag}}\n",
    "where\n",
    "start_tag => `<function`\n",
    "parameters => a JSON dict with the function argument name as key and function argument value as value.\n",
    "end_tag => `</function>`\n",
    "Here is an example,\n",
    "<function=example_function_name>{{\"example_name\": \"example_value\"}}</function>\n",
    "Reminder:\n",
    "- Function calls MUST follow the specified format\n",
    "- Required parameters MUST be specified\n",
    "- Only call one function at a time\n",
    "- Put the entire function call reply on one line\n",
    "- Always add your sources when using search results to answer the user query\n",
    "You are a helpful assistant.\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"You are in New York. Please get the current date and time, and the weather.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "messages = get_messages()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    messages=messages,\n",
    "    response_format={\n",
    "        \"type\": \"structural_tag\",\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"structures\": [\n",
    "            {\n",
    "                \"begin\": \"<function=get_current_weather>\",\n",
    "                \"schema\": schema_get_current_weather,\n",
    "                \"end\": \"</function>\",\n",
    "            },\n",
    "            {\n",
    "                \"begin\": \"<function=get_current_date>\",\n",
    "                \"schema\": schema_get_current_date,\n",
    "                \"end\": \"</function>\",\n",
    "            },\n",
    "        ],\n",
    "        \"triggers\": [\"<function=\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(\n",
    "    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native API and SGLang Runtime (SRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: For native API, as a work-around, you need to set `require_reasoning` argument to `True` to ensure the model will think before generating the structured output. It's not required for chat-completion API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pydantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:12:49.285693Z",
     "iopub.status.busy": "2026-02-27T02:12:49.285563Z",
     "iopub.status.idle": "2026-02-27T02:12:51.997052Z",
     "shell.execute_reply": "2026-02-27T02:12:51.996233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Okay, so the user is asking for the information and population of the capital of France in JSON format. Let me break this down. First, I need to identify what the capital of France is. I know that Paris is the capital, so that\\'s straightforward. \\n\\nNext, I need to find the population. I remember that Paris is a major city, so its population is quite large. I think it\\'s over 3 million, but I\\'m not exactly sure of the exact number. Maybe around 3,700,000? I should double-check that to make sure I have the correct figure.\\n\\nNow, the user wants this information in JSON format. JSON stands for JavaScript Object Notation, and it\\'s a way to structure data. So I\\'ll need to create an object with relevant fields. The user mentioned \"information and population,\" so I should include both the city name and its population. \\n\\nI should also consider if there are any other details that might be useful. Maybe the country it\\'s in, which is France, and perhaps the region or something else. But the user didn\\'t specify, so I\\'ll stick to the essentials: name and population.\\n\\nPutting it all together, I\\'ll create a JSON object with a \"name\" field for the city and a \"population\" field with the number. I\\'ll make sure the numbers are accurate and formatted correctly. \\n\\nI should also think about the user\\'s possible intent. They might be looking for a quick data point, maybe for a project or a presentation. Providing the information in a structured format like JSON would make it easy for them to use in their work. \\n\\nI wonder if they need more details, like the population figure\\'s source or the year it\\'s from. But since they didn\\'t ask for that, I\\'ll keep it simple. \\n\\nFinally, I\\'ll present the JSON in a clear and concise manner, ensuring that it\\'s correctly formatted so there are no syntax errors. That way, the user can easily copy and use it without issues.\\n</think>{\\n  \"name\": \"Paris\",\\n  \"population\": 3700000\\n}', 'output_ids': [32313, 11, 773, 279, 1196, 374, 10161, 369, 279, 1995, 323, 7042, 315, 279, 6722, 315, 9625, 304, 4718, 3561, 13, 6771, 752, 1438, 419, 1495, 13, 5512, 11, 358, 1184, 311, 10542, 1128, 279, 6722, 315, 9625, 374, 13, 358, 1414, 429, 12095, 374, 279, 6722, 11, 773, 429, 594, 30339, 13, 4710, 5847, 11, 358, 1184, 311, 1477, 279, 7042, 13, 358, 6099, 429, 12095, 374, 264, 3598, 3283, 11, 773, 1181, 7042, 374, 5008, 3460, 13, 358, 1744, 432, 594, 916, 220, 18, 3526, 11, 714, 358, 2776, 537, 6896, 2704, 315, 279, 4734, 1372, 13, 10696, 2163, 220, 18, 11, 22, 15, 15, 11, 15, 15, 15, 30, 358, 1265, 1990, 15934, 429, 311, 1281, 2704, 358, 614, 279, 4396, 7071, 382, 7039, 11, 279, 1196, 6801, 419, 1995, 304, 4718, 3561, 13, 4718, 13352, 369, 12914, 3002, 2806, 367, 11, 323, 432, 594, 264, 1616, 311, 5944, 821, 13, 2055, 358, 3278, 1184, 311, 1855, 458, 1633, 448, 9760, 5043, 13, 576, 1196, 9733, 330, 25069, 323, 7042, 1335, 773, 358, 1265, 2924, 2176, 279, 3283, 829, 323, 1181, 7042, 13, 4710, 40, 1265, 1083, 2908, 421, 1052, 525, 894, 1008, 3565, 429, 2578, 387, 5390, 13, 10696, 279, 3146, 432, 594, 304, 11, 892, 374, 9625, 11, 323, 8365, 279, 5537, 476, 2494, 770, 13, 1988, 279, 1196, 3207, 944, 13837, 11, 773, 358, 3278, 9214, 311, 279, 58786, 25, 829, 323, 7042, 382, 97904, 432, 678, 3786, 11, 358, 3278, 1855, 264, 4718, 1633, 448, 264, 330, 606, 1, 2070, 369, 279, 3283, 323, 264, 330, 44441, 1, 2070, 448, 279, 1372, 13, 358, 3278, 1281, 2704, 279, 5109, 525, 13382, 323, 23126, 12440, 13, 4710, 40, 1265, 1083, 1744, 911, 279, 1196, 594, 3204, 7385, 13, 2379, 2578, 387, 3330, 369, 264, 3974, 821, 1459, 11, 7196, 369, 264, 2390, 476, 264, 15496, 13, 80100, 279, 1995, 304, 264, 32930, 3561, 1075, 4718, 1035, 1281, 432, 4135, 369, 1105, 311, 990, 304, 862, 975, 13, 4710, 40, 5775, 421, 807, 1184, 803, 3565, 11, 1075, 279, 7042, 7071, 594, 2530, 476, 279, 1042, 432, 594, 504, 13, 1988, 2474, 807, 3207, 944, 2548, 369, 429, 11, 358, 3278, 2506, 432, 4285, 13, 4710, 23949, 11, 358, 3278, 3042, 279, 4718, 304, 264, 2797, 323, 63594, 11566, 11, 22573, 429, 432, 594, 12440, 23126, 773, 1052, 525, 902, 19482, 5975, 13, 2938, 1616, 11, 279, 1196, 646, 6707, 2975, 323, 990, 432, 2041, 4714, 624, 151649, 515, 220, 330, 606, 788, 330, 59604, 756, 220, 330, 44441, 788, 220, 18, 22, 15, 15, 15, 15, 15, 198, 92, 151643], 'meta_info': {'id': '4c5534d0da4a4161a7a37b795e5f7b8e', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 23, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 436, 'cached_tokens': 1, 'cached_tokens_details': None, 'dp_rank': None, 'e2e_latency': 2.1972439929377288, 'response_sent_to_client_ts': 1772158371.9922574}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>reasoing_content: Okay, so the user is asking for the information and population of the capital of France in JSON format. Let me break this down. First, I need to identify what the capital of France is. I know that Paris is the capital, so that's straightforward. <br><br>Next, I need to find the population. I remember that Paris is a major city, so its population is quite large. I think it's over 3 million, but I'm not exactly sure of the exact number. Maybe around 3,700,000? I should double-check that to make sure I have the correct figure.<br><br>Now, the user wants this information in JSON format. JSON stands for JavaScript Object Notation, and it's a way to structure data. So I'll need to create an object with relevant fields. The user mentioned \"information and population,\" so I should include both the city name and its population. <br><br>I should also consider if there are any other details that might be useful. Maybe the country it's in, which is France, and perhaps the region or something else. But the user didn't specify, so I'll stick to the essentials: name and population.<br><br>Putting it all together, I'll create a JSON object with a \"name\" field for the city and a \"population\" field with the number. I'll make sure the numbers are accurate and formatted correctly. <br><br>I should also think about the user's possible intent. They might be looking for a quick data point, maybe for a project or a presentation. Providing the information in a structured format like JSON would make it easy for them to use in their work. <br><br>I wonder if they need more details, like the population figure's source or the year it's from. But since they didn't ask for that, I'll keep it simple. <br><br>Finally, I'll present the JSON in a clear and concise manner, ensuring that it's correctly formatted so there are no syntax errors. That way, the user can easily copy and use it without issues.<br><br><br>content: {<br>  \"name\": \"Paris\",<br>  \"population\": 3700000<br>}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "\n",
    "\n",
    "# Define the schema using Pydantic\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n",
    "    population: int = Field(..., description=\"Population of the capital city\")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Give me the information and population of the capital of France in the JSON format.\",\n",
    "    },\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True, return_dict=False\n",
    ")\n",
    "# Make API request\n",
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": text,\n",
    "        \"require_reasoning\": True,\n",
    "        \"sampling_params\": {\n",
    "            \"temperature\": 0,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"json_schema\": json.dumps(CapitalInfo.model_json_schema()),\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(response.json())\n",
    "\n",
    "\n",
    "reasoing_content = response.json()[\"text\"].split(\"</think>\")[0]\n",
    "content = response.json()[\"text\"].split(\"</think>\")[1]\n",
    "print_highlight(f\"reasoing_content: {reasoing_content}\\n\\ncontent: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Schema Directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:12:51.998471Z",
     "iopub.status.busy": "2026-02-27T02:12:51.998344Z",
     "iopub.status.idle": "2026-02-27T02:12:54.220369Z",
     "shell.execute_reply": "2026-02-27T02:12:54.219558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': 'Okay, so the user is asking for the information and population of the capital of France in JSON format. Let me break this down. First, I need to identify what the capital of France is. I know that Paris is the capital, so that\\'s straightforward. \\n\\nNext, I need to find the population. I remember that Paris is a major city, so its population is quite large. I think it\\'s over 3 million, but I\\'m not exactly sure of the exact number. Maybe around 3,700,000? I should double-check that to make sure I have the correct figure.\\n\\nNow, the user wants this information in JSON format. JSON stands for JavaScript Object Notation, and it\\'s a way to structure data. So I\\'ll need to create an object with relevant fields. The user mentioned \"information and population,\" so I should include both the city name and its population. \\n\\nI should also consider if there are any other details that might be useful. Maybe the country it\\'s in, which is France, and perhaps the region or something else. But the user didn\\'t specify, so I\\'ll stick to the essentials: name and population.\\n\\nPutting it all together, I\\'ll create a JSON object with a \"name\" field for the city and a \"population\" field with the number. I\\'ll make sure the numbers are accurate and formatted correctly. \\n\\nI should also think about the user\\'s possible intent. They might be looking for a quick data point, maybe for a project or a presentation. Providing the information in a structured format like JSON would make it easy for them to use in their work. \\n\\nI wonder if they need more details, like the population figure\\'s source or the year it\\'s from. But since they didn\\'t ask for that, I\\'ll keep it simple. \\n\\nFinally, I\\'ll present the JSON in a clear and concise manner, ensuring that it\\'s correctly formatted so there are no syntax errors. That way, the user can easily copy and use it without issues.\\n</think>{\\n  \"name\": \"Paris\",\\n  \"population\": 3700000\\n}', 'output_ids': [32313, 11, 773, 279, 1196, 374, 10161, 369, 279, 1995, 323, 7042, 315, 279, 6722, 315, 9625, 304, 4718, 3561, 13, 6771, 752, 1438, 419, 1495, 13, 5512, 11, 358, 1184, 311, 10542, 1128, 279, 6722, 315, 9625, 374, 13, 358, 1414, 429, 12095, 374, 279, 6722, 11, 773, 429, 594, 30339, 13, 4710, 5847, 11, 358, 1184, 311, 1477, 279, 7042, 13, 358, 6099, 429, 12095, 374, 264, 3598, 3283, 11, 773, 1181, 7042, 374, 5008, 3460, 13, 358, 1744, 432, 594, 916, 220, 18, 3526, 11, 714, 358, 2776, 537, 6896, 2704, 315, 279, 4734, 1372, 13, 10696, 2163, 220, 18, 11, 22, 15, 15, 11, 15, 15, 15, 30, 358, 1265, 1990, 15934, 429, 311, 1281, 2704, 358, 614, 279, 4396, 7071, 382, 7039, 11, 279, 1196, 6801, 419, 1995, 304, 4718, 3561, 13, 4718, 13352, 369, 12914, 3002, 2806, 367, 11, 323, 432, 594, 264, 1616, 311, 5944, 821, 13, 2055, 358, 3278, 1184, 311, 1855, 458, 1633, 448, 9760, 5043, 13, 576, 1196, 9733, 330, 25069, 323, 7042, 1335, 773, 358, 1265, 2924, 2176, 279, 3283, 829, 323, 1181, 7042, 13, 4710, 40, 1265, 1083, 2908, 421, 1052, 525, 894, 1008, 3565, 429, 2578, 387, 5390, 13, 10696, 279, 3146, 432, 594, 304, 11, 892, 374, 9625, 11, 323, 8365, 279, 5537, 476, 2494, 770, 13, 1988, 279, 1196, 3207, 944, 13837, 11, 773, 358, 3278, 9214, 311, 279, 58786, 25, 829, 323, 7042, 382, 97904, 432, 678, 3786, 11, 358, 3278, 1855, 264, 4718, 1633, 448, 264, 330, 606, 1, 2070, 369, 279, 3283, 323, 264, 330, 44441, 1, 2070, 448, 279, 1372, 13, 358, 3278, 1281, 2704, 279, 5109, 525, 13382, 323, 23126, 12440, 13, 4710, 40, 1265, 1083, 1744, 911, 279, 1196, 594, 3204, 7385, 13, 2379, 2578, 387, 3330, 369, 264, 3974, 821, 1459, 11, 7196, 369, 264, 2390, 476, 264, 15496, 13, 80100, 279, 1995, 304, 264, 32930, 3561, 1075, 4718, 1035, 1281, 432, 4135, 369, 1105, 311, 990, 304, 862, 975, 13, 4710, 40, 5775, 421, 807, 1184, 803, 3565, 11, 1075, 279, 7042, 7071, 594, 2530, 476, 279, 1042, 432, 594, 504, 13, 1988, 2474, 807, 3207, 944, 2548, 369, 429, 11, 358, 3278, 2506, 432, 4285, 13, 4710, 23949, 11, 358, 3278, 3042, 279, 4718, 304, 264, 2797, 323, 63594, 11566, 11, 22573, 429, 432, 594, 12440, 23126, 773, 1052, 525, 902, 19482, 5975, 13, 2938, 1616, 11, 279, 1196, 646, 6707, 2975, 323, 990, 432, 2041, 4714, 624, 151649, 515, 220, 330, 606, 788, 330, 59604, 756, 220, 330, 44441, 788, 220, 18, 22, 15, 15, 15, 15, 15, 198, 92, 151643], 'meta_info': {'id': 'c383067437a7488e8f962da3f30c6926', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 23, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 436, 'cached_tokens': 22, 'cached_tokens_details': None, 'dp_rank': None, 'e2e_latency': 2.214041488012299, 'response_sent_to_client_ts': 1772158374.2164078}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_schema = json.dumps(\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n",
    "            \"population\": {\"type\": \"integer\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"population\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# JSON\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True, return_dict=False\n",
    ")\n",
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": text,\n",
    "        \"require_reasoning\": True,\n",
    "        \"sampling_params\": {\n",
    "            \"temperature\": 0,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"json_schema\": json_schema,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:12:54.221975Z",
     "iopub.status.busy": "2026-02-27T02:12:54.221850Z",
     "iopub.status.idle": "2026-02-27T02:12:54.381243Z",
     "shell.execute_reply": "2026-02-27T02:12:54.380398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Berlin is the capital of France', 'output_ids': [3430, 81, 742, 77, 374, 279, 6722, 315, 9625, 151643], 'meta_info': {'id': '78dec6a1e90e4756b7488cb590d33498', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 11, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 10, 'cached_tokens': 10, 'cached_tokens_details': None, 'dp_rank': None, 'e2e_latency': 0.1341529011260718, 'response_sent_to_client_ts': 1772158374.378014}}, {'text': 'Berlin is the capital of France', 'output_ids': [3430, 81, 742, 77, 374, 279, 6722, 315, 9625, 151643], 'meta_info': {'id': 'b0d1076fbd924858b7ed2ea394b8ef33', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 11, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 10, 'cached_tokens': 10, 'cached_tokens_details': None, 'dp_rank': None, 'e2e_latency': 0.13409967301413417, 'response_sent_to_client_ts': 1772158374.3780253}}, {'text': 'Berlin is the capital of France', 'output_ids': [3430, 81, 742, 77, 374, 279, 6722, 315, 9625, 151643], 'meta_info': {'id': '059969e408f3453ea3d1c67166daee77', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 11, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 10, 'cached_tokens': 10, 'cached_tokens_details': None, 'dp_rank': None, 'e2e_latency': 0.13405127078294754, 'response_sent_to_client_ts': 1772158374.3780305}}]\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": \"Give me the information of the capital of France.\",\n",
    "        \"require_reasoning\": True,\n",
    "        \"sampling_params\": {\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"temperature\": 0,\n",
    "            \"n\": 3,\n",
    "            \"ebnf\": (\n",
    "                \"root ::= city | description\\n\"\n",
    "                'city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\\n'\n",
    "                'description ::= city \" is \" status\\n'\n",
    "                'status ::= \"the capital of \" country\\n'\n",
    "                'country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"'\n",
    "            ),\n",
    "        },\n",
    "        \"stream\": False,\n",
    "        \"return_logprob\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:12:54.382880Z",
     "iopub.status.busy": "2026-02-27T02:12:54.382724Z",
     "iopub.status.idle": "2026-02-27T02:13:08.537867Z",
     "shell.execute_reply": "2026-02-27T02:13:08.536946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' France, and the \\n\\\\( n \\\\)  \\\\( m \\\\) \\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\) \\n\\\\( n \\\\) \\\\( m \\\\)', 'output_ids': [9625, 11, 323, 279, 220, 198, 44292, 308, 1124, 8, 220, 17767, 296, 1124, 8, 17767, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8, 715, 44292, 308, 1124, 8, 17767, 296, 1124, 8], 'meta_info': {'id': '6eb7ecac40fb435eb341a6697e17f947', 'finish_reason': {'type': 'length', 'length': 2048}, 'prompt_tokens': 6, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 2048, 'cached_tokens': 1, 'cached_tokens_details': None, 'dp_rank': None, 'e2e_latency': 14.147486110916361, 'response_sent_to_client_ts': 1772158388.5340128}}\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/generate\",\n",
    "    json={\n",
    "        \"text\": \"Paris is the capital of\",\n",
    "        \"require_reasoning\": True,\n",
    "        \"sampling_params\": {\n",
    "            \"temperature\": 0,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"regex\": \"(France|England)\",\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:13:08.539414Z",
     "iopub.status.busy": "2026-02-27T02:13:08.539284Z",
     "iopub.status.idle": "2026-02-27T02:13:12.542181Z",
     "shell.execute_reply": "2026-02-27T02:13:12.541442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{'text': 'Okay, so I need to figure out the information and population of the capital of France in JSON format. Let me break this down. First, I know that the capital of France is Paris. That\\'s pretty straightforward. I remember hearing that Paris is a major city not just in France but in the whole of France, acting as both the political and cultural heart.\\n\\nNow, regarding the population, I think Paris is quite populous. I\\'ve heard numbers ranging from maybe around 2 million to a bit more. Let me try to pin this down. Wait, I think the official population is somewhere around 2.1 million. But I\\'m not 100% sure. I should probably cross-check that in my mind. I recall that major urban areas in Europe, including Paris, have populations over 2 million. Maybe it\\'s closer to 2.2 million now, considering recent growth.\\n\\nI guess I should express this in JSON format. JSON requires key-value pairs, so I\\'ll structure it accordingly. I\\'m considering two approaches: either a basic object with population as an integer or a more detailed object that includes other information like area,人均GDP, and maybe GDP itself. But since the user specifically asked for population, maybe the basic structure is sufficient. However, adding more fields could provide a more comprehensive response.\\n\\nWait, the user didn\\'t specify whether they want just the population or a combination of related information. Since they only asked for population, I\\'ll make sure to include that. But including a summary field with population and area might be helpful, so that gives a quick overview. \\n\\nSo, structuring the JSON, I\\'ll have a \"capital\" object with \"name\" as \"Paris\" and \"population\" as the number I thought of. Then, a \"summary\" that includes the population and area. I should make sure the numbers are accurate. Paris\\'s area is 10.55 square kilometers if I recall correctly. \\n\\nPutting it all together, I\\'ll write the JSON with the capital key containing both the name and population, and the summary key with population and area. Numbers are important here, so I need to be precise. I think I can recall that Paris\\'s population is roughly 2,207,000, but I should verify if that\\'s the most recent figure. In any case, that\\'s a solid starting point.\\n\\nI don\\'t see any other information requested, so I\\'ll stick to just population for now, but organizing the data in a JSON structure with related fields could be beneficial. That way, it\\'s more informative and user-friendly if more data is added later.\\n\\nAnother thought: should I format the population as a whole number or include decimals? For simplicity, I\\'ll use an integer, so 2207000. That\\'s clear and easy to read. \\n\\nTo sum up, the plan is to create a JSON object with the capital of France, including its name and population, and a summary section that compiles population and area for a quick glance. This should fulfill the user\\'s request effectively.\\n</think>\\n\\n```json\\n{\\n  \"capital\": {\\n    \"name\": \"Paris\",\\n    \"population\": 2207000\\n  },\\n  \"summary\": {\\n    \"population\": 2207000,\\n    \"area\": 10.55\\n  }\\n}\\n```', 'output_ids': [32313, 11, 773, 358, 1184, 311, 7071, 700, 279, 1995, 323, 7042, 315, 279, 6722, 315, 9625, 304, 4718, 3561, 13, 6771, 752, 1438, 419, 1495, 13, 5512, 11, 358, 1414, 429, 279, 6722, 315, 9625, 374, 12095, 13, 2938, 594, 5020, 30339, 13, 358, 6099, 10778, 429, 12095, 374, 264, 3598, 3283, 537, 1101, 304, 9625, 714, 304, 279, 4361, 315, 9625, 11, 15358, 438, 2176, 279, 4948, 323, 12752, 4746, 382, 7039, 11, 8826, 279, 7042, 11, 358, 1744, 12095, 374, 5008, 94451, 13, 358, 3003, 6617, 5109, 23994, 504, 7196, 2163, 220, 17, 3526, 311, 264, 2699, 803, 13, 6771, 752, 1430, 311, 8983, 419, 1495, 13, 13824, 11, 358, 1744, 279, 3946, 7042, 374, 14696, 2163, 220, 17, 13, 16, 3526, 13, 1988, 358, 2776, 537, 220, 16, 15, 15, 4, 2704, 13, 358, 1265, 4658, 5312, 15934, 429, 304, 847, 3971, 13, 358, 19091, 429, 3598, 15662, 5671, 304, 4505, 11, 2670, 12095, 11, 614, 21910, 916, 220, 17, 3526, 13, 10696, 432, 594, 12128, 311, 220, 17, 13, 17, 3526, 1431, 11, 12831, 3213, 6513, 382, 40, 7942, 358, 1265, 3158, 419, 304, 4718, 3561, 13, 4718, 7460, 1376, 19083, 13530, 11, 773, 358, 3278, 5944, 432, 27079, 13, 358, 2776, 12831, 1378, 19827, 25, 2987, 264, 6770, 1633, 448, 7042, 438, 458, 7546, 476, 264, 803, 11682, 1633, 429, 5646, 1008, 1995, 1075, 3082, 11, 106372, 38, 10298, 11, 323, 7196, 29730, 5086, 13, 1988, 2474, 279, 1196, 11689, 4588, 369, 7042, 11, 7196, 279, 6770, 5944, 374, 14016, 13, 4354, 11, 7842, 803, 5043, 1410, 3410, 264, 803, 15817, 2033, 382, 14190, 11, 279, 1196, 3207, 944, 13837, 3425, 807, 1366, 1101, 279, 7042, 476, 264, 10601, 315, 5435, 1995, 13, 8704, 807, 1172, 4588, 369, 7042, 11, 358, 3278, 1281, 2704, 311, 2924, 429, 13, 1988, 2670, 264, 12126, 2070, 448, 7042, 323, 3082, 2578, 387, 10950, 11, 773, 429, 6696, 264, 3974, 23251, 13, 4710, 4416, 11, 2036, 1677, 279, 4718, 11, 358, 3278, 614, 264, 330, 65063, 1, 1633, 448, 330, 606, 1, 438, 330, 59604, 1, 323, 330, 44441, 1, 438, 279, 1372, 358, 3381, 315, 13, 5005, 11, 264, 330, 1708, 1, 429, 5646, 279, 7042, 323, 3082, 13, 358, 1265, 1281, 2704, 279, 5109, 525, 13382, 13, 12095, 594, 3082, 374, 220, 16, 15, 13, 20, 20, 9334, 40568, 421, 358, 19091, 12440, 13, 4710, 97904, 432, 678, 3786, 11, 358, 3278, 3270, 279, 4718, 448, 279, 6722, 1376, 8482, 2176, 279, 829, 323, 7042, 11, 323, 279, 12126, 1376, 448, 7042, 323, 3082, 13, 34713, 525, 2989, 1588, 11, 773, 358, 1184, 311, 387, 23560, 13, 358, 1744, 358, 646, 19091, 429, 12095, 594, 7042, 374, 17267, 220, 17, 11, 17, 15, 22, 11, 15, 15, 15, 11, 714, 358, 1265, 10146, 421, 429, 594, 279, 1429, 3213, 7071, 13, 758, 894, 1142, 11, 429, 594, 264, 6437, 5916, 1459, 382, 40, 1513, 944, 1490, 894, 1008, 1995, 11223, 11, 773, 358, 3278, 9214, 311, 1101, 7042, 369, 1431, 11, 714, 34721, 279, 821, 304, 264, 4718, 5944, 448, 5435, 5043, 1410, 387, 23699, 13, 2938, 1616, 11, 432, 594, 803, 38219, 323, 1196, 21896, 421, 803, 821, 374, 3694, 2937, 382, 14037, 3381, 25, 1265, 358, 3561, 279, 7042, 438, 264, 4361, 1372, 476, 2924, 58328, 30, 1752, 38975, 11, 358, 3278, 990, 458, 7546, 11, 773, 220, 17, 17, 15, 22, 15, 15, 15, 13, 2938, 594, 2797, 323, 4135, 311, 1349, 13, 4710, 1249, 2629, 705, 11, 279, 3119, 374, 311, 1855, 264, 4718, 1633, 448, 279, 6722, 315, 9625, 11, 2670, 1181, 829, 323, 7042, 11, 323, 264, 12126, 3772, 429, 1367, 3658, 7042, 323, 3082, 369, 264, 3974, 33422, 13, 1096, 1265, 20423, 279, 1196, 594, 1681, 13444, 624, 151649, 271, 73594, 2236, 198, 515, 220, 330, 65063, 788, 341, 262, 330, 606, 788, 330, 59604, 756, 262, 330, 44441, 788, 220, 17, 17, 15, 22, 15, 15, 15, 198, 220, 1153, 220, 330, 1708, 788, 341, 262, 330, 44441, 788, 220, 17, 17, 15, 22, 15, 15, 15, 345, 262, 330, 4798, 788, 220, 16, 15, 13, 20, 20, 198, 220, 456, 532, 73594, 151643], 'meta_info': {'id': 'f147fb75d39440888bc047ef664b5fe3', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 23, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 696, 'cached_tokens': 22, 'cached_tokens_details': None, 'dp_rank': None, 'e2e_latency': 3.9937551252078265, 'response_sent_to_client_ts': 1772158392.5378695}}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True, return_dict=False\n",
    ")\n",
    "payload = {\n",
    "    \"text\": text,\n",
    "    \"require_reasoning\": True,\n",
    "    \"sampling_params\": {\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"structural_tag\": json.dumps(\n",
    "            {\n",
    "                \"type\": \"structural_tag\",\n",
    "                \"structures\": [\n",
    "                    {\n",
    "                        \"begin\": \"<function=get_current_weather>\",\n",
    "                        \"schema\": schema_get_current_weather,\n",
    "                        \"end\": \"</function>\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"begin\": \"<function=get_current_date>\",\n",
    "                        \"schema\": schema_get_current_date,\n",
    "                        \"end\": \"</function>\",\n",
    "                    },\n",
    "                ],\n",
    "                \"triggers\": [\"<function=\"],\n",
    "            }\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Send POST request to the API endpoint\n",
    "response = requests.post(f\"http://localhost:{port}/generate\", json=payload)\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:13:12.543766Z",
     "iopub.status.busy": "2026-02-27T02:13:12.543626Z",
     "iopub.status.idle": "2026-02-27T02:13:12.560320Z",
     "shell.execute_reply": "2026-02-27T02:13:12.559537Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Engine API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:13:12.562181Z",
     "iopub.status.busy": "2026-02-27T02:13:12.562059Z",
     "iopub.status.idle": "2026-02-27T02:13:31.358309Z",
     "shell.execute_reply": "2026-02-27T02:13:31.357696Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 02:13:15] INFO server_args.py:1859: Attention backend not specified. Use fa3 backend by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 02:13:15] INFO server_args.py:2929: Set soft_watchdog_timeout since in CI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-27 02:13:15] INFO engine.py:156: server_args=ServerArgs(model_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', tokenizer_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.907, max_running_requests=128, max_queued_requests=None, max_total_tokens=20480, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', enable_prefill_delayer=False, prefill_delayer_max_delay_passes=30, prefill_delayer_token_usage_low_watermark=None, prefill_delayer_forward_passes_buckets=None, prefill_delayer_wait_seconds_buckets=None, device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=851486735, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=300, dist_timeout=None, download_dir=None, model_checksum=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, log_requests_format='text', log_requests_target=None, uvicorn_access_log_exclude_prefixes=[], crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, extra_metric_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, admin_api_key=None, served_model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', weight_version='default', chat_template=None, hf_chat_template_name=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser='deepseek-r1', tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', attn_cp_size=1, moe_dp_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, enable_lora_overlap_loading=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='fa3', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', fp4_gemm_runner_backend='flashinfer_cutlass', nsa_prefill_backend=None, nsa_decode_backend=None, disable_flashinfer_autotune=False, mamba_backend='triton', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, enable_multi_layer_eagle=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, enable_aiter_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype=None, mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, linear_attn_backend='triton', linear_attn_decode_backend=None, linear_attn_prefill_backend=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', disable_hicache_numa_detect=False, hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, hierarchical_sparse_attention_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=8192, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 576, 640, 704, 768, 832, 896, 960, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816, 3072, 3328, 3584, 3840, 4096, 4608, 5120, 5632, 6144, 6656, 7168, 7680, 8192], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, nsa_prefill_cp_mode='round-robin-split', enable_fused_qk_norm_rope=False, enable_precise_embedding_interpolation=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, limit_mm_data_per_request=None, enable_mm_global_cache=False, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.41s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.43s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.43s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=128 avail_mem=107.59 GB):   0%|          | 0/20 [00:00<?, ?it/s]\r",
      "Capturing batches (bs=128 avail_mem=107.59 GB):   5%|▌         | 1/20 [00:00<00:03,  5.59it/s]\r",
      "Capturing batches (bs=120 avail_mem=107.48 GB):   5%|▌         | 1/20 [00:00<00:03,  5.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=112 avail_mem=107.47 GB):   5%|▌         | 1/20 [00:00<00:03,  5.59it/s]\r",
      "Capturing batches (bs=104 avail_mem=107.47 GB):   5%|▌         | 1/20 [00:00<00:03,  5.59it/s]\r",
      "Capturing batches (bs=104 avail_mem=107.47 GB):  20%|██        | 4/20 [00:00<00:00, 16.08it/s]\r",
      "Capturing batches (bs=96 avail_mem=107.46 GB):  20%|██        | 4/20 [00:00<00:00, 16.08it/s] \r",
      "Capturing batches (bs=88 avail_mem=107.46 GB):  20%|██        | 4/20 [00:00<00:00, 16.08it/s]\r",
      "Capturing batches (bs=80 avail_mem=107.45 GB):  20%|██        | 4/20 [00:00<00:00, 16.08it/s]\r",
      "Capturing batches (bs=80 avail_mem=107.45 GB):  35%|███▌      | 7/20 [00:00<00:00, 20.23it/s]\r",
      "Capturing batches (bs=72 avail_mem=107.45 GB):  35%|███▌      | 7/20 [00:00<00:00, 20.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=64 avail_mem=107.44 GB):  35%|███▌      | 7/20 [00:00<00:00, 20.23it/s]\r",
      "Capturing batches (bs=56 avail_mem=107.44 GB):  35%|███▌      | 7/20 [00:00<00:00, 20.23it/s]\r",
      "Capturing batches (bs=56 avail_mem=107.44 GB):  50%|█████     | 10/20 [00:00<00:00, 22.79it/s]\r",
      "Capturing batches (bs=48 avail_mem=107.43 GB):  50%|█████     | 10/20 [00:00<00:00, 22.79it/s]\r",
      "Capturing batches (bs=40 avail_mem=107.43 GB):  50%|█████     | 10/20 [00:00<00:00, 22.79it/s]\r",
      "Capturing batches (bs=32 avail_mem=107.42 GB):  50%|█████     | 10/20 [00:00<00:00, 22.79it/s]\r",
      "Capturing batches (bs=32 avail_mem=107.42 GB):  65%|██████▌   | 13/20 [00:00<00:00, 24.04it/s]\r",
      "Capturing batches (bs=24 avail_mem=107.41 GB):  65%|██████▌   | 13/20 [00:00<00:00, 24.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=16 avail_mem=107.41 GB):  65%|██████▌   | 13/20 [00:00<00:00, 24.04it/s]\r",
      "Capturing batches (bs=12 avail_mem=107.41 GB):  65%|██████▌   | 13/20 [00:00<00:00, 24.04it/s]\r",
      "Capturing batches (bs=12 avail_mem=107.41 GB):  80%|████████  | 16/20 [00:00<00:00, 22.74it/s]\r",
      "Capturing batches (bs=8 avail_mem=107.40 GB):  80%|████████  | 16/20 [00:00<00:00, 22.74it/s] \r",
      "Capturing batches (bs=4 avail_mem=107.40 GB):  80%|████████  | 16/20 [00:00<00:00, 22.74it/s]\r",
      "Capturing batches (bs=2 avail_mem=107.39 GB):  80%|████████  | 16/20 [00:00<00:00, 22.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing batches (bs=1 avail_mem=107.39 GB):  80%|████████  | 16/20 [00:00<00:00, 22.74it/s]\r",
      "Capturing batches (bs=1 avail_mem=107.39 GB): 100%|██████████| 20/20 [00:00<00:00, 26.02it/s]\r",
      "Capturing batches (bs=1 avail_mem=107.39 GB): 100%|██████████| 20/20 [00:00<00:00, 22.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import sglang as sgl\n",
    "\n",
    "llm = sgl.Engine(\n",
    "    model_path=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    reasoning_parser=\"deepseek-r1\",\n",
    "    grammar_backend=\"xgrammar\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Pydantic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:13:31.360622Z",
     "iopub.status.busy": "2026-02-27T02:13:31.360165Z",
     "iopub.status.idle": "2026-02-27T02:13:43.492037Z",
     "shell.execute_reply": "2026-02-27T02:13:43.491446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Give me the information of the capital of China in the JSON format.\n",
      "Generated text: {\n",
      "  \"name\": \"Beijing\",\n",
      "  \"population\": 316000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of France in the JSON format.\n",
      "Generated text: {\n",
      "  \"name\": \"Paris\",\n",
      "  \"population\": 2154000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Ireland in the JSON format.\n",
      "Generated text: {\n",
      "  \"name\": \"Ireland\",\n",
      "  \"population\": 500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "prompts = [\n",
    "    \"Give me the information of the capital of China in the JSON format.\",\n",
    "    \"Give me the information of the capital of France in the JSON format.\",\n",
    "    \"Give me the information of the capital of Ireland in the JSON format.\",\n",
    "]\n",
    "\n",
    "\n",
    "# Define the schema using Pydantic\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n",
    "    population: int = Field(..., description=\"Population of the capital city\")\n",
    "\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"json_schema\": json.dumps(CapitalInfo.model_json_schema()),\n",
    "}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON Schema Directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:13:43.493809Z",
     "iopub.status.busy": "2026-02-27T02:13:43.493688Z",
     "iopub.status.idle": "2026-02-27T02:13:54.273209Z",
     "shell.execute_reply": "2026-02-27T02:13:54.272623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Give me the information of the capital of China in the JSON format.\n",
      "Generated text: {\n",
      "  \"name\": \"Beijing\",\n",
      "  \"population\": 316000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of France in the JSON format.\n",
      "Generated text: {\n",
      "  \"name\": \"Paris\",\n",
      "  \"population\": 2154000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Ireland in the JSON format.\n",
      "Generated text: {\n",
      "  \"name\": \"Ireland\",\n",
      "  \"population\": 500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Give me the information of the capital of China in the JSON format.\",\n",
    "    \"Give me the information of the capital of France in the JSON format.\",\n",
    "    \"Give me the information of the capital of Ireland in the JSON format.\",\n",
    "]\n",
    "\n",
    "json_schema = json.dumps(\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n",
    "            \"population\": {\"type\": \"integer\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"population\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "sampling_params = {\"temperature\": 0, \"max_new_tokens\": 2048, \"json_schema\": json_schema}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBNF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:13:54.274593Z",
     "iopub.status.busy": "2026-02-27T02:13:54.274480Z",
     "iopub.status.idle": "2026-02-27T02:13:54.379717Z",
     "shell.execute_reply": "2026-02-27T02:13:54.379225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Give me the information of the capital of France.\n",
      "Generated text: Berlin is the capital of France\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Germany.\n",
      "Generated text: Berlin is the capital of Germany\n",
      "===============================\n",
      "Prompt: Give me the information of the capital of Italy.\n",
      "Generated text: Paris is the capital of France\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Give me the information of the capital of France.\",\n",
    "    \"Give me the information of the capital of Germany.\",\n",
    "    \"Give me the information of the capital of Italy.\",\n",
    "]\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"ebnf\": (\n",
    "        \"root ::= city | description\\n\"\n",
    "        'city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\\n'\n",
    "        'description ::= city \" is \" status\\n'\n",
    "        'status ::= \"the capital of \" country\\n'\n",
    "        'country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"'\n",
    "    ),\n",
    "}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:13:54.381197Z",
     "iopub.status.busy": "2026-02-27T02:13:54.381081Z",
     "iopub.status.idle": "2026-02-27T02:13:54.486167Z",
     "shell.execute_reply": "2026-02-27T02:13:54.485666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Please provide information about London as a major global city:\n",
      "Generated text: France\n",
      "===============================\n",
      "Prompt: Please provide information about Paris as a major global city:\n",
      "Generated text: France\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Please provide information about London as a major global city:\",\n",
    "    \"Please provide information about Paris as a major global city:\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95, \"regex\": \"(France|England)\"}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:13:54.487328Z",
     "iopub.status.busy": "2026-02-27T02:13:54.487218Z",
     "iopub.status.idle": "2026-02-27T02:13:57.483071Z",
     "shell.execute_reply": "2026-02-27T02:13:57.482446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: <｜begin▁of▁sentence｜><｜Assistant｜>Give me the information and population of the capital of France in the JSON format.<｜end▁of▁sentence｜><｜Assistant｜><think>\n",
      "\n",
      "Generated text: Alright, so the user is asking for the information and population of the capital of France. Okay, first things first, I need to figure out which city is the capital of France. I'm pretty sure it's Paris, but I should double-check that to make sure I'm accurate. \n",
      "\n",
      "Now, they want this information in JSON format. JSON is a data format that's commonly used for transmitting data in web applications, so it's structured with key-value pairs. That means I'll need to create a JSON object with relevant fields. \n",
      "\n",
      "The key points they're asking for are the city name and its population. So, the keys in the JSON should probably be \"city\" and \"population\". For the population, I need the latest figure. I think as of the last update, Paris had a population around 2.1 million. But I should confirm that number to ensure it's current. \n",
      "\n",
      "Wait, sometimes population figures change every few years. I know the French census is conducted every ten years, so the most recent one was in 2022. Maybe I should look up the exact number from that census to provide the most accurate data. \n",
      "\n",
      "Looking it up, the population of Paris is approximately 2,155,383 as of the 2022 census. That's a bit over 2 million. So I should use that number to make the JSON accurate. \n",
      "\n",
      "Also, I should include the country name in the JSON since they mentioned the capital of France. So adding a \"country\" key with the value \"France\" would make it complete. \n",
      "\n",
      "Putting it all together, the JSON structure would have a main key, maybe \"capital\", which contains an object with \"city\", \"population\", and \"country\". That way, it's organized and easy to parse. \n",
      "\n",
      "I should make sure the JSON syntax is correct, with proper commas and quotation marks. No trailing commas after the keys, that's important for JSON validity. \n",
      "\n",
      "So the final JSON would look like this:\n",
      "\n",
      "{\n",
      "  \"capital\": {\n",
      "    \"city\": \"Paris\",\n",
      "    \"population\": 2155383,\n",
      "    \"country\": \"France\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Double-checking everything, the city name is correct, the population is up to date, and the structure is proper. I think that covers what the user asked for. \n",
      "\n",
      "I wonder if the user needs more details, like the area or the year of the population data. But since they only specified population, I'll stick to that. \n",
      "\n",
      "Maybe they're building a simple application or just curious about the data. Either way, providing the accurate and well-formatted JSON should meet their needs.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"capital\": {\n",
      "    \"city\": \"Paris\",\n",
      "    \"population\": 2155383,\n",
      "    \"country\": \"France\"\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True, return_dict=False\n",
    ")\n",
    "prompts = [text]\n",
    "\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"structural_tag\": json.dumps(\n",
    "        {\n",
    "            \"type\": \"structural_tag\",\n",
    "            \"structures\": [\n",
    "                {\n",
    "                    \"begin\": \"<function=get_current_weather>\",\n",
    "                    \"schema\": schema_get_current_weather,\n",
    "                    \"end\": \"</function>\",\n",
    "                },\n",
    "                {\n",
    "                    \"begin\": \"<function=get_current_date>\",\n",
    "                    \"schema\": schema_get_current_date,\n",
    "                    \"end\": \"</function>\",\n",
    "                },\n",
    "            ],\n",
    "            \"triggers\": [\"<function=\"],\n",
    "        }\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# Send POST request to the API endpoint\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T02:13:57.484379Z",
     "iopub.status.busy": "2026-02-27T02:13:57.484264Z",
     "iopub.status.idle": "2026-02-27T02:13:57.500177Z",
     "shell.execute_reply": "2026-02-27T02:13:57.499670Z"
    }
   },
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
