
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Speculative Decoding &#8212; SGLang</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom_log.css?v=731335ad" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=0927e61e"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=ccdb6887"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'advanced_features/speculative_decoding';</script>
    <link rel="icon" href="../_static/logo.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Structured Outputs" href="structured_outputs.html" />
    <link rel="prev" title="Attention Backend" href="attention_backend.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Feb 26, 2026"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="SGLang - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="SGLang - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../get_started/install.html">Install SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Basic Usage</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/send_request.html">Sending Requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/openai_api.html">OpenAI-Compatible APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/ollama_api.html">Ollama-Compatible API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/offline_engine_api.html">Offline Engine API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/native_api.html">SGLang Native APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/sampling_params.html">Sampling Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage/popular_model_usage.html">Popular Model Usage (DeepSeek, GPT-OSS, GLM, Llama, MiniMax, Qwen, and more)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="server_arguments.html">Server Arguments</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyperparameter_tuning.html">Hyperparameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="attention_backend.html">Attention Backend</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Speculative Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured_outputs.html">Structured Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured_outputs_for_reasoning_models.html">Structured Outputs For Reasoning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="tool_parser.html">Tool Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="separate_reasoning.html">Reasoning Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantized_kv_cache.html">Quantized KV Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="expert_parallelism.html">Expert Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="dp_dpa_smg_guide.html">DP, DPA and SGLang DP Router</a></li>
<li class="toctree-l1"><a class="reference internal" href="lora.html">LoRA Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="pd_disaggregation.html">PD Disaggregation</a></li>
<li class="toctree-l1"><a class="reference internal" href="epd_disaggregation.html">EPD Disaggregation</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_parallelism.html">Pipeline Parallelism for Long Context</a></li>
<li class="toctree-l1"><a class="reference internal" href="hicache.html">Hierarchical KV Caching (HiCache)</a></li>
<li class="toctree-l1"><a class="reference internal" href="vlm_query.html">Query VLM with Offline Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="dp_for_multi_modal_encoder.html">DP for Multi-Modal Encoder in SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda_graph_for_multi_modal_encoder.html">Cuda Graph for Multi-Modal Encoder in SGLang</a></li>
<li class="toctree-l1"><a class="reference internal" href="sgl_model_gateway.html">SGLang Model Gateway</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic_inference.html">Deterministic Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="observability.html">Observability</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint_engine.html">Checkpoint Engine Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="sglang_for_rl.html">SGLang for RL Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supported Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../supported_models/text_generation/index.html">Text Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/retrieval_ranking/index.html">Retrieval &amp; Ranking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/specialized/index.html">Specialized Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supported_models/extending/index.html">Extending SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">SGLang Diffusion</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../diffusion/index.html">SGLang Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/installation.html">Install SGLang-Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/compatibility_matrix.html">Compatibility Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/api/cli.html">SGLang diffusion CLI Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/api/openai_api.html">SGLang Diffusion OpenAI API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/performance/index.html">Performance Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/performance/attention_backends.html">Attention Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/performance/profiling.html">Profiling Multimodal Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/performance/cache/index.html">Caching Acceleration for Diffusion Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/performance/cache/cache_dit.html">Cache-DiT Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/performance/cache/teacache.html">TeaCache Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/support_new_models.html">How to Support New Diffusion Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/contributing.html">Contributing to SGLang Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/ci_perf.html">Perf Baseline Generation Script</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion/environment_variables.html">Caching Acceleration</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Hardware Platforms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../platforms/amd_gpu.html">AMD GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/cpu_server.html">CPU Servers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/tpu.html">TPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/nvidia_jetson.html">NVIDIA Jetson Orin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/ascend_npu_support.html">Ascend NPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platforms/xpu.html">XPU</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/contribution_guide.html">Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/development_guide_using_docker.html">Development Guide Using Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/development_jit_kernel_guide.html">Development Guide for JIT Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/benchmark_and_profiling.html">Benchmark and Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/bench_serving.html">Bench Serving Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/evaluating_new_models.html">Evaluating New Models with SGLang</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../references/faq.html">Troubleshooting and Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/environment_variables.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/production_metrics.html">Production Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/production_request_trace.html">Production Request Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/multi_node_deployment/multi_node_index.html">Multi-Node Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/custom_chat_template.html">Custom Chat Template</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/frontend/frontend_index.html">Frontend Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/post_training_integration.html">Post-Training Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/release_lookup.html">Release Lookup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/learn_more.html">Learn More and Join the Community</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/blob/main/advanced_features/speculative_decoding.md?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/edit/main/advanced_features/speculative_decoding.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sgl-project/sgl-project.github.io/issues/new?title=Issue%20on%20page%20%2Fadvanced_features/speculative_decoding.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/advanced_features/speculative_decoding.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Speculative Decoding</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jump-to-sections">Jump to sections</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-guidance">Quick guidance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#method-comparison-mini-table">Method comparison (mini table)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-highlights">Performance Highlights</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eagle-decoding">EAGLE Decoding</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eagle-2-decoding">EAGLE-2 decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eagle-2-decoding-with-torch-compile">EAGLE-2 Decoding with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eagle-2-decoding-via-frequency-ranked-speculative-sampling">EAGLE-2 Decoding via Frequency-Ranked Speculative Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eagle-3-decoding">EAGLE-3 Decoding</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-token-prediction">Multi Token Prediction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standalone-speculative-decoding-small-draft-model">Standalone Speculative Decoding (Small Draft Model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#speculative-decoding-v2-overlap-scheduler">Speculative Decoding V2 (Overlap Scheduler)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ngram-speculative-decoding">Ngram Speculative Decoding</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ngram-specific-parameters">Ngram-specific parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#full-parameter-reference">Full Parameter Reference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-parameters">Core parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Ngram-specific parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-variables">Environment variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-related-flags">Other related flags</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#oom-troubleshooting">OOM Troubleshooting</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-reduce-draft-tree-size-most-effective">Step 1: Reduce draft tree size (most effective)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-lower-static-memory-fraction">Step 2: Lower static memory fraction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-reduce-cuda-graph-batch-size">Step 3: Reduce CUDA graph batch size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-limit-concurrent-requests">Step 4: Limit concurrent requests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-use-quantization">Step 5: Use quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-use-a-smaller-dtype">Step 6: Use a smaller dtype</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-7-use-fr-spec-to-reduce-lm-head-memory-eagle-2-standalone">Step 7: Use FR-Spec to reduce lm_head memory (EAGLE-2 / STANDALONE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-oom-recovery-recipe">Quick OOM recovery recipe</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="speculative-decoding">
<h1>Speculative Decoding<a class="headerlink" href="#speculative-decoding" title="Link to this heading">#</a></h1>
<p>SGLang provides several speculative decoding options, including EAGLE-2/EAGLE-3, MTP, classic draft-model decoding, and an NGRAM-based variant. Our implementation aims to maximize speed and efficiency and is considered to be among the fastest in open-source LLM engines.</p>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<section id="jump-to-sections">
<h3>Jump to sections<a class="headerlink" href="#jump-to-sections" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference internal" href="#eagle-decoding"><span class="std std-ref">EAGLE Decoding</span></a></p>
<ul>
<li><p><a class="reference internal" href="#eagle-2-decoding"><span class="std std-ref">EAGLE-2 Decoding</span></a></p></li>
<li><p>EAGLE-2 Decoding with torch.compile</p></li>
<li><p><a class="reference internal" href="#eagle-2-decoding-via-frequency-ranked-speculative-sampling"><span class="std std-ref">EAGLE-2 Decoding via Frequency-Ranked Speculative Sampling</span></a></p></li>
<li><p><a class="reference internal" href="#eagle-3-decoding"><span class="std std-ref">EAGLE-3 Decoding</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#multi-token-prediction"><span class="std std-ref">Multi Token Prediction</span></a></p></li>
<li><p><a class="reference internal" href="#standalone-speculative-decoding-small-draft-model"><span class="std std-ref">Standalone Speculative Decoding (Small Draft Model)</span></a></p></li>
<li><p><a class="reference internal" href="#speculative-decoding-v2-overlap-scheduler"><span class="std std-ref">Speculative Decoding V2 (Overlap Scheduler)</span></a></p></li>
<li><p><a class="reference internal" href="#ngram-speculative-decoding"><span class="std std-ref">Ngram Speculative Decoding</span></a></p></li>
<li><p><a class="reference internal" href="#full-parameter-reference"><span class="std std-ref">Full Parameter Reference</span></a></p></li>
<li><p><a class="reference internal" href="#oom-troubleshooting"><span class="std std-ref">OOM Troubleshooting</span></a></p></li>
<li><p><a class="reference internal" href="#references"><span class="std std-ref">References</span></a></p></li>
</ul>
</section>
<section id="quick-guidance">
<h3>Quick guidance<a class="headerlink" href="#quick-guidance" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Best speed/quality (recommended)</strong>: Use <strong>EAGLE-3</strong> with <code class="docutils literal notranslate"><span class="pre">--speculative-algorithm</span> <span class="pre">EAGLE3</span></code>.</p></li>
<li><p><strong>Strong default / broad compatibility</strong>: Use <strong>EAGLE-2</strong> with <code class="docutils literal notranslate"><span class="pre">--speculative-algorithm</span> <span class="pre">EAGLE</span></code>.</p></li>
<li><p><strong>Lower <code class="docutils literal notranslate"><span class="pre">lm_head</span></code> overhead for EAGLE-2</strong>: Enable <strong>FR-Spec</strong> with <code class="docutils literal notranslate"><span class="pre">--speculative-token-map</span></code>.</p></li>
<li><p><strong>Model is MTP-enabled</strong>: Use <strong>MTP via speculative decoding</strong> (often with small <code class="docutils literal notranslate"><span class="pre">speculative_num_steps/topk/num_draft_tokens</span></code>, see the example section).</p></li>
<li><p><strong>You have a smaller draft LLM</strong>: Use <strong>STANDALONE</strong> (<code class="docutils literal notranslate"><span class="pre">--speculative-algorithm</span> <span class="pre">STANDALONE</span></code>).</p></li>
<li><p><strong>No extra model available</strong>: Use <strong>NGRAM</strong> (<code class="docutils literal notranslate"><span class="pre">--speculative-algorithm</span> <span class="pre">NGRAM</span></code>, CUDA-only).</p></li>
<li><p><strong>Want overlap scheduler (experimental)</strong>: Enable <strong>SpecV2</strong> with <code class="docutils literal notranslate"><span class="pre">SGLANG_ENABLE_SPEC_V2=True</span></code> (requires <code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span> <span class="pre">1</span></code>).</p></li>
</ul>
</section>
<section id="method-comparison-mini-table">
<h3>Method comparison (mini table)<a class="headerlink" href="#method-comparison-mini-table" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Draft source</p></th>
<th class="head text-right"><p>Separate draft model?</p></th>
<th class="head"><p>How to enable</p></th>
<th class="head"><p>Notes / constraints</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>EAGLE-2</p></td>
<td><p>EAGLE draft model (feature drafting + tree)</p></td>
<td class="text-right"><p>Typically yes</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--speculative-algorithm</span> <span class="pre">EAGLE</span></code> + <code class="docutils literal notranslate"><span class="pre">--speculative-draft-model-path</span> <span class="pre">...</span></code></p></td>
<td><p>Tune <code class="docutils literal notranslate"><span class="pre">--speculative-num-steps</span></code>, <code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span></code>, <code class="docutils literal notranslate"><span class="pre">--speculative-num-draft-tokens</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>EAGLE-2 + <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></p></td>
<td><p>Same as EAGLE-2</p></td>
<td class="text-right"><p>Typically yes</p></td>
<td><p>Add <code class="docutils literal notranslate"><span class="pre">--enable-torch-compile</span></code> (optionally <code class="docutils literal notranslate"><span class="pre">--torch-compile-max-bs</span></code>)</p></td>
<td><p>Further kernel-level optimizations</p></td>
</tr>
<tr class="row-even"><td><p>EAGLE-2 + FR-Spec</p></td>
<td><p>Same as EAGLE-2 + token subset</p></td>
<td class="text-right"><p>Typically yes</p></td>
<td><p>Add <code class="docutils literal notranslate"><span class="pre">--speculative-token-map</span> <span class="pre">...</span></code></p></td>
<td><p>Reduces <code class="docutils literal notranslate"><span class="pre">lm_head</span></code> overhead with high-frequency token vocab</p></td>
</tr>
<tr class="row-odd"><td><p>EAGLE-3</p></td>
<td><p>EAGLE3 draft model</p></td>
<td class="text-right"><p>Yes</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--speculative-algorithm</span> <span class="pre">EAGLE3</span></code> + <code class="docutils literal notranslate"><span class="pre">--speculative-draft-model-path</span> <span class="pre">...</span></code></p></td>
<td><p>Best throughput in the benchmark above</p></td>
</tr>
<tr class="row-even"><td><p>MTP</p></td>
<td><p>Built-in multi-token heads (model-specific)</p></td>
<td class="text-right"><p>Often no</p></td>
<td><p>See <strong>Multi Token Prediction</strong> section</p></td>
<td><p>Uses speculative workflow; draft path may be auto-handled for some models</p></td>
</tr>
<tr class="row-odd"><td><p>STANDALONE</p></td>
<td><p>Smaller draft LLM (token-level)</p></td>
<td class="text-right"><p>Yes</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--speculative-algorithm</span> <span class="pre">STANDALONE</span></code> + <code class="docutils literal notranslate"><span class="pre">--speculative-draft-model-path</span> <span class="pre">...</span></code></p></td>
<td><p>Does <strong>not</strong> support <code class="docutils literal notranslate"><span class="pre">--enable-dp-attention</span></code></p></td>
</tr>
<tr class="row-even"><td><p>SpecV2 (experimental)</p></td>
<td><p>V2 workers + overlap scheduler</p></td>
<td class="text-right"><p>N/A</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SGLANG_ENABLE_SPEC_V2=True</span></code></p></td>
<td><p>Only supports <code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span> <span class="pre">1</span></code>; applies to <code class="docutils literal notranslate"><span class="pre">EAGLE</span></code>, <code class="docutils literal notranslate"><span class="pre">EAGLE3</span></code>, <code class="docutils literal notranslate"><span class="pre">STANDALONE</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>NGRAM</p></td>
<td><p>Ngram cache from previous tokens</p></td>
<td class="text-right"><p>No</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--speculative-algorithm</span> <span class="pre">NGRAM</span></code></p></td>
<td><p>CUDA-only; no <code class="docutils literal notranslate"><span class="pre">--enable-dp-attention</span></code>; disables overlap scheduler &amp; mixed chunked prefill</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="performance-highlights">
<h3>Performance Highlights<a class="headerlink" href="#performance-highlights" title="Link to this heading">#</a></h3>
<p>Please see below for the huge improvements on throughput for LLaMA-Instruct 3.1 8B tested on MT bench that can be achieved via EAGLE3 decoding.
For further details please see the <a class="reference external" href="https://arxiv.org/pdf/2503.01840">EAGLE3 paper</a>.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Throughput (tokens/s)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>SGLang (w/o speculative, 1x H100)</p></td>
<td><p>158.34 tokens/s</p></td>
</tr>
<tr class="row-odd"><td><p>SGLang + EAGLE-2 (1x H100)</p></td>
<td><p>244.10 tokens/s</p></td>
</tr>
<tr class="row-even"><td><p>SGLang + EAGLE-3 (1x H100)</p></td>
<td><p>373.25 tokens/s</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="eagle-decoding">
<h2>EAGLE Decoding<a class="headerlink" href="#eagle-decoding" title="Link to this heading">#</a></h2>
<p>To enable EAGLE speculative decoding the following parameters are relevant:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-draft-model-path</span></code></p></td>
<td><p>Draft model path/weights. <strong>Typically required</strong> for EAGLE/EAGLE3 and STANDALONE. For some MTP-enabled models, this can be omitted.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-num-steps</span></code></p></td>
<td><p>Depth of autoregressive drafting. Increases speculation range but risks rejection cascades.</p></td>
<td><p>Auto (<code class="docutils literal notranslate"><span class="pre">5</span></code> for Llama/Grok; <code class="docutils literal notranslate"><span class="pre">3</span></code> for many other models)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span></code></p></td>
<td><p>Branching factor per step. Improves candidate diversity and acceptance rate, but increases memory/compute consumption.</p></td>
<td><p>Auto (<code class="docutils literal notranslate"><span class="pre">4</span></code> for Llama/Grok; <code class="docutils literal notranslate"><span class="pre">1</span></code> for many other models)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-num-draft-tokens</span></code></p></td>
<td><p>Maximum parallel verification capacity. Allows deeper tree evaluation but increases GPU memory usage.</p></td>
<td><p>Auto (<code class="docutils literal notranslate"><span class="pre">8</span></code> for Llama/Grok; <code class="docutils literal notranslate"><span class="pre">4</span></code> for many other models). If <code class="docutils literal notranslate"><span class="pre">topk=1</span></code>, it is adjusted to <code class="docutils literal notranslate"><span class="pre">num_steps</span> <span class="pre">+</span> <span class="pre">1</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-accept-threshold-single</span></code></p></td>
<td><p>Acceptance threshold for single-token verification. Lower values accept more aggressively.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1.0</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-accept-threshold-acc</span></code></p></td>
<td><p>Accumulated acceptance threshold across steps.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1.0</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-attention-mode</span></code></p></td>
<td><p>Attention mode for speculative operations (<code class="docutils literal notranslate"><span class="pre">prefill</span></code> or <code class="docutils literal notranslate"><span class="pre">decode</span></code>), affecting both target verification and draft extension.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;prefill&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-draft-attention-backend</span></code></p></td>
<td><p>Override attention backend for the draft model.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code> (same as target)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-draft-model-quantization</span></code></p></td>
<td><p>Quantization method for the draft model. Use <code class="docutils literal notranslate"><span class="pre">&quot;unquant&quot;</span></code> to force no quantization even when the target model is quantized.</p></td>
<td><p>Same as target model</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-draft-model-revision</span></code></p></td>
<td><p>Specific revision/commit of the draft model to load.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code> (auto-set to <code class="docutils literal notranslate"><span class="pre">&quot;main&quot;</span></code> when <code class="docutils literal notranslate"><span class="pre">--speculative-draft-model-path</span></code> is set and revision is omitted)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-draft-load-format</span></code></p></td>
<td><p>Load format for the draft model weights.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p>These parameters are mostly the same for EAGLE-2 and EAGLE-3. <code class="docutils literal notranslate"><span class="pre">--speculative-token-map</span></code> is ignored for EAGLE-3 models.
For <code class="docutils literal notranslate"><span class="pre">--speculative-num-steps</span></code>, <code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span></code>, and <code class="docutils literal notranslate"><span class="pre">--speculative-num-draft-tokens</span></code>: leave all three unset to use auto-tuning, or set all three explicitly when tuning.</p>
<p>You can find the best combinations of these parameters with <a class="reference external" href="https://github.com/sgl-project/sglang/blob/main/scripts/playground/bench_speculative.py">bench_speculative.py</a>.</p>
<section id="eagle-2-decoding">
<h3>EAGLE-2 decoding<a class="headerlink" href="#eagle-2-decoding" title="Link to this heading">#</a></h3>
<p>You can enable EAGLE-2 decoding by setting <code class="docutils literal notranslate"><span class="pre">--speculative-algorithm</span> <span class="pre">EAGLE</span></code> and choosing an appropriate model.</p>
<p><strong>Launch the server:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>meta-llama/Llama-2-7b-chat-hf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-algorithm<span class="w"> </span>EAGLE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-draft-model-path<span class="w"> </span>lmsys/sglang-EAGLE-llama2-chat-7B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-steps<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-eagle-topk<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-draft-tokens<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cuda-graph-max-bs<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--log-level<span class="w"> </span>warning
</pre></div>
</div>
<p><strong>Send a request:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://127.0.0.1:30000/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;List 3 countries and their capitals.&quot;</span><span class="p">},</span>
    <span class="p">],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="eagle-2-decoding-with-torch-compile">
<h3>EAGLE-2 Decoding with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code><a class="headerlink" href="#eagle-2-decoding-with-torch-compile" title="Link to this heading">#</a></h3>
<p>You can also enable <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> for further optimizations and optionally set <code class="docutils literal notranslate"><span class="pre">--torch-compile-max-bs</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>meta-llama/Llama-2-7b-chat-hf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-algorithm<span class="w"> </span>EAGLE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-draft-model-path<span class="w"> </span>lmsys/sglang-EAGLE-llama2-chat-7B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-steps<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-eagle-topk<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-draft-tokens<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--enable-torch-compile<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--torch-compile-max-bs<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--log-level<span class="w"> </span>warning
</pre></div>
</div>
<p><strong>Send a request:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://127.0.0.1:30000/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;List 3 countries and their capitals.&quot;</span><span class="p">},</span>
    <span class="p">],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="eagle-2-decoding-via-frequency-ranked-speculative-sampling">
<h3>EAGLE-2 Decoding via Frequency-Ranked Speculative Sampling<a class="headerlink" href="#eagle-2-decoding-via-frequency-ranked-speculative-sampling" title="Link to this heading">#</a></h3>
<p>By employing a truncated high-frequency token vocabulary in the draft model, Eagle speculative decoding reduces <code class="docutils literal notranslate"><span class="pre">lm_head</span></code> computational overhead while accelerating the pipeline without quality degradation. For more details, checkout <a class="reference external" href="https://arxiv.org/pdf/2502.14856">the paper</a>.</p>
<p>In our implementation, set <code class="docutils literal notranslate"><span class="pre">--speculative-token-map</span></code> to enable the optimization. You can get the high-frequency token in FR-Spec from <a class="reference external" href="https://huggingface.co/thunlp/LLaMA3-Instruct-8B-FR-Spec">this model</a>. Or you can obtain high-frequency token by directly downloading these token from <a class="reference external" href="https://github.com/thunlp/FR-Spec/tree/main?tab=readme-ov-file#prepare-fr-spec-vocabulary-subset">this repo</a>.</p>
<p>Thanks for the contribution from <a class="reference external" href="https://github.com/Achazwl">Weilin Zhao</a> and <a class="reference external" href="https://github.com/Zhou-sx">Zhousx</a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>meta-llama/Meta-Llama-3-8B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-algorithm<span class="w"> </span>EAGLE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-draft-model-path<span class="w"> </span>lmsys/sglang-EAGLE-LLaMA3-Instruct-8B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-steps<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-eagle-topk<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-draft-tokens<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-token-map<span class="w"> </span>thunlp/LLaMA3-Instruct-8B-FR-Spec/freq_32768.pt<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cuda-graph-max-bs<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dtype<span class="w"> </span>float16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--log-level<span class="w"> </span>warning
</pre></div>
</div>
<p><strong>Send a request:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://127.0.0.1:30000/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;List 3 countries and their capitals.&quot;</span><span class="p">},</span>
    <span class="p">],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="eagle-3-decoding">
<h3>EAGLE-3 Decoding<a class="headerlink" href="#eagle-3-decoding" title="Link to this heading">#</a></h3>
<p>You can enable EAGLE-3 decoding by setting <code class="docutils literal notranslate"><span class="pre">--speculative-algorithm</span> <span class="pre">EAGLE3</span></code> and choosing an appropriate model.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>meta-llama/Meta-Llama-3.1-8B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-algorithm<span class="w"> </span>EAGLE3<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-draft-model-path<span class="w"> </span>jamesliu1/sglang-EAGLE3-Llama-3.1-Instruct-8B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-steps<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-eagle-topk<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-draft-tokens<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cuda-graph-max-bs<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dtype<span class="w"> </span>float16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--log-level<span class="w"> </span>warning
</pre></div>
</div>
<p><strong>Send a request:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://127.0.0.1:30000/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;List 3 countries and their capitals.&quot;</span><span class="p">},</span>
    <span class="p">],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="multi-token-prediction">
<h2>Multi Token Prediction<a class="headerlink" href="#multi-token-prediction" title="Link to this heading">#</a></h2>
<p>We support <a class="reference external" href="https://arxiv.org/pdf/2404.19737">MTP(Multi-Token Prediction)</a> in SGLang by using speculative decoding. We use <code class="docutils literal notranslate"><span class="pre">XiaomiMiMo/MiMo-7B-RL</span></code> as an example here (for DeepSeek MTP usage, refer to <a class="reference internal" href="../basic_usage/deepseek_v32.html#multi-token-prediction"><span class="std std-ref">deepseek_v32 doc</span></a>).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>XiaomiMiMo/MiMo-7B-RL<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-algorithm<span class="w"> </span>EAGLE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-eagle-topk<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-draft-tokens<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cuda-graph-max-bs<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--log-level<span class="w"> </span>warning
</pre></div>
</div>
<p><strong>Send a request:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:30000/v1/chat/completions&quot;</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;XiaomiMiMo/MiMo-7B-RL&quot;</span><span class="p">,</span>
    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">}],</span>
<span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">())</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="standalone-speculative-decoding-small-draft-model">
<h2>Standalone Speculative Decoding (Small Draft Model)<a class="headerlink" href="#standalone-speculative-decoding-small-draft-model" title="Link to this heading">#</a></h2>
<p>Besides EAGLE/MTP, SGLang also supports <strong>token-level speculative decoding</strong> using a smaller <strong>draft model</strong>. Enable it with <code class="docutils literal notranslate"><span class="pre">--speculative-algorithm</span> <span class="pre">STANDALONE</span></code> and provide a draft model via <code class="docutils literal notranslate"><span class="pre">--speculative-draft-model-path</span></code>.</p>
<p>Relevant parameters:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-draft-model-path</span></code></p></td>
<td><p>Draft model weights (smaller than the target model).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-num-steps</span></code></p></td>
<td><p>Draft depth (how many steps the draft model runs autoregressively).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">3</span></code> (auto default for STANDALONE)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span></code></p></td>
<td><p>Branching factor (token candidates per step).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code> (auto default for STANDALONE)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-num-draft-tokens</span></code></p></td>
<td><p>Verification capacity.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">4</span></code> (auto default for STANDALONE)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-draft-model-quantization</span></code></p></td>
<td><p>Quantization for the draft model. Use <code class="docutils literal notranslate"><span class="pre">&quot;unquant&quot;</span></code> to disable quantization on the draft even when the target is quantized.</p></td>
<td><p>Same as target</p></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<div><p><strong>Note:</strong> Standalone speculative decoding currently <strong>does not support</strong> <code class="docutils literal notranslate"><span class="pre">--enable-dp-attention</span></code>.</p>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-algorithm<span class="w"> </span>STANDALONE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-draft-model-path<span class="w"> </span>Qwen/Qwen2.5-1.5B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-steps<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-eagle-topk<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-draft-tokens<span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cuda-graph-max-bs<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--log-level<span class="w"> </span>warning
</pre></div>
</div>
<p><strong>Send a request:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://127.0.0.1:30000/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;List 3 countries and their capitals.&quot;</span><span class="p">},</span>
    <span class="p">],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="speculative-decoding-v2-overlap-scheduler">
<h2>Speculative Decoding V2 (Overlap Scheduler)<a class="headerlink" href="#speculative-decoding-v2-overlap-scheduler" title="Link to this heading">#</a></h2>
<p>SGLang provides an <strong>experimental Speculative Decoding V2</strong> implementation that enables an overlap scheduler and uses V2 speculative workers (e.g. <code class="docutils literal notranslate"><span class="pre">StandaloneWorkerV2</span></code>, <code class="docutils literal notranslate"><span class="pre">EAGLEWorkerV2</span></code>).</p>
<p>To enable it, set the environment variable:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">SGLANG_ENABLE_SPEC_V2=True</span></code></p></li>
</ul>
<p>Notes:</p>
<ul class="simple">
<li><p>SpecV2 currently only supports <code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span> <span class="pre">1</span></code>. When SpecV2 is enabled, <strong>set <code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span> <span class="pre">1</span></code> explicitly</strong>.</p></li>
<li><p>If you explicitly set <code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, the server will error.</p></li>
<li><p>If you omit <code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span></code>, auto-tuning may pick <code class="docutils literal notranslate"><span class="pre">topk</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> for some models (e.g. Llama). This is incompatible with SpecV2 and may not always trigger an immediate config error, so set <code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span> <span class="pre">1</span></code> explicitly.</p></li>
<li><p>This applies to <code class="docutils literal notranslate"><span class="pre">EAGLE</span></code>, <code class="docutils literal notranslate"><span class="pre">EAGLE3</span></code>, and <code class="docutils literal notranslate"><span class="pre">STANDALONE</span></code>.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">SGLANG_ENABLE_SPEC_V2</span><span class="o">=</span>True<span class="w"> </span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-algorithm<span class="w"> </span>STANDALONE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-draft-model-path<span class="w"> </span>Qwen/Qwen2.5-1.5B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-steps<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-eagle-topk<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-draft-tokens<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cuda-graph-max-bs<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--log-level<span class="w"> </span>warning
</pre></div>
</div>
<p><strong>Send a request:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://127.0.0.1:30000/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;List 3 countries and their capitals.&quot;</span><span class="p">},</span>
    <span class="p">],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="ngram-speculative-decoding">
<h2>Ngram Speculative Decoding<a class="headerlink" href="#ngram-speculative-decoding" title="Link to this heading">#</a></h2>
<p>SGLang also supports <strong>ngram-based speculative decoding</strong> (no separate draft model). It retrieves draft tokens from an ngram cache built from previously generated tokens, and then verifies them with the target model.</p>
<p>Enable it with:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--speculative-algorithm</span> <span class="pre">NGRAM</span></code></p></li>
</ul>
<section id="ngram-specific-parameters">
<h3>Ngram-specific parameters<a class="headerlink" href="#ngram-specific-parameters" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-num-draft-tokens</span></code></p></td>
<td><p>Number of draft tokens verified per step. If omitted, defaults to <code class="docutils literal notranslate"><span class="pre">--speculative-ngram-max-match-window-size</span></code>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">12</span></code> (with default ngram settings)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-min-match-window-size</span></code></p></td>
<td><p>Minimum matching window size.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-max-match-window-size</span></code></p></td>
<td><p>Maximum matching window size.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">12</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-min-bfs-breadth</span></code></p></td>
<td><p>Minimum BFS breadth.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-max-bfs-breadth</span></code></p></td>
<td><p>Maximum BFS breadth.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">10</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-match-type</span></code></p></td>
<td><p>Match type: <code class="docutils literal notranslate"><span class="pre">&quot;BFS&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;PROB&quot;</span></code>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;BFS&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-branch-length</span></code></p></td>
<td><p>How many recent tokens to insert into the cache.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">18</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-capacity</span></code></p></td>
<td><p>Cache capacity (number of entries).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">10,000,000</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p>Notes:</p>
<ul class="simple">
<li><p>Ngram speculative decoding <strong>only supports CUDA</strong>.</p></li>
<li><p>It currently <strong>does not support</strong> <code class="docutils literal notranslate"><span class="pre">--enable-dp-attention</span></code>.</p></li>
<li><p>It disables the overlap scheduler and mixed chunked prefill.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">--speculative-ngram-max-bfs-breadth</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> (thus <code class="docutils literal notranslate"><span class="pre">speculative_eagle_topk</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>) and <code class="docutils literal notranslate"><span class="pre">page_size</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, use <code class="docutils literal notranslate"><span class="pre">--attention-backend</span> <span class="pre">flashinfer</span></code>; otherwise the server will error.</p></li>
<li><p>Optional: set <code class="docutils literal notranslate"><span class="pre">SGLANG_NGRAM_FORCE_GREEDY_VERIFY=True</span></code> to force greedy verification.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-algorithm<span class="w"> </span>NGRAM<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-draft-tokens<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-ngram-max-match-window-size<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-ngram-max-bfs-breadth<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cuda-graph-max-bs<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--log-level<span class="w"> </span>warning
</pre></div>
</div>
<p><strong>Send a request:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://127.0.0.1:30000/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;List 3 countries and their capitals.&quot;</span><span class="p">},</span>
    <span class="p">],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="full-parameter-reference">
<h2>Full Parameter Reference<a class="headerlink" href="#full-parameter-reference" title="Link to this heading">#</a></h2>
<p>Below is a comprehensive list of all speculative decoding parameters available in SGLang:</p>
<section id="core-parameters">
<h3>Core parameters<a class="headerlink" href="#core-parameters" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-algorithm</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Algorithm to use: <code class="docutils literal notranslate"><span class="pre">EAGLE</span></code>, <code class="docutils literal notranslate"><span class="pre">EAGLE3</span></code>, <code class="docutils literal notranslate"><span class="pre">STANDALONE</span></code>, <code class="docutils literal notranslate"><span class="pre">NGRAM</span></code>, <code class="docutils literal notranslate"><span class="pre">NEXTN</span></code> (alias of <code class="docutils literal notranslate"><span class="pre">EAGLE</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-draft-model-path</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Path to the draft model weights</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-draft-model-revision</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Specific revision/commit of the draft model (<code class="docutils literal notranslate"><span class="pre">&quot;main&quot;</span></code> is auto-used when draft path is set and revision is omitted)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-draft-load-format</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Load format for draft model weights</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-num-steps</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code> (auto-chosen when omitted)</p></td>
<td><p>Autoregressive drafting depth</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code> (auto-chosen when omitted)</p></td>
<td><p>Branching factor per drafting step</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-num-draft-tokens</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code> (auto-chosen when omitted)</p></td>
<td><p>Maximum number of draft tokens for verification</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-accept-threshold-single</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1.0</span></code></p></td>
<td><p>Single-token acceptance threshold</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-accept-threshold-acc</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1.0</span></code></p></td>
<td><p>Accumulated acceptance threshold</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-token-map</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Path to FR-Spec high-frequency token map</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-attention-mode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;prefill&quot;</span></code></p></td>
<td><p>Attention mode for speculative operations (<code class="docutils literal notranslate"><span class="pre">&quot;prefill&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;decode&quot;</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-draft-attention-backend</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Override attention backend for the draft model</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-moe-runner-backend</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>MoE runner backend for the draft model</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-moe-a2a-backend</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>MoE all-to-all backend for the draft model</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-draft-model-quantization</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p>Same as target</p></td>
<td><p>Quantization for the draft model (<code class="docutils literal notranslate"><span class="pre">&quot;unquant&quot;</span></code> to disable)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="id1">
<h3>Ngram-specific parameters<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-min-match-window-size</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p>Minimum ngram matching window</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-max-match-window-size</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">12</span></code></p></td>
<td><p>Maximum ngram matching window</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-min-bfs-breadth</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p>Minimum BFS breadth</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-max-bfs-breadth</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">10</span></code></p></td>
<td><p>Maximum BFS breadth</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-match-type</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;BFS&quot;</span></code></p></td>
<td><p>Match type: <code class="docutils literal notranslate"><span class="pre">&quot;BFS&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;PROB&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-branch-length</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">18</span></code></p></td>
<td><p>Recent tokens to insert into cache</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--speculative-ngram-capacity</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">10,000,000</span></code></p></td>
<td><p>Cache capacity</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="environment-variables">
<h3>Environment variables<a class="headerlink" href="#environment-variables" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Variable</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">SGLANG_ENABLE_SPEC_V2</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Enable Speculative Decoding V2 (overlap scheduler)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SGLANG_NGRAM_FORCE_GREEDY_VERIFY</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Force greedy verification for ngram decoding</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="other-related-flags">
<h3>Other related flags<a class="headerlink" href="#other-related-flags" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--enable-multi-layer-eagle</span></code></p></td>
<td><p>Enable multi-layer EAGLE (auto-enabled for MiMoV2 and Step3p5 models)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--enable-torch-compile</span></code></p></td>
<td><p>Enable <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> for kernel-level optimizations</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--torch-compile-max-bs</span></code></p></td>
<td><p>Maximum batch size for <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="oom-troubleshooting">
<h2>OOM Troubleshooting<a class="headerlink" href="#oom-troubleshooting" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>[!WARNING]
<strong>Out of Memory (OOM)?</strong> Speculative decoding may increase GPU memory usage because the draft tree, CUDA graphs, and verification-related buffers consume additional VRAM. If you encounter OOM errors, try the following adjustments.</p>
</div></blockquote>
<section id="step-1-reduce-draft-tree-size-most-effective">
<h3>Step 1: Reduce draft tree size (most effective)<a class="headerlink" href="#step-1-reduce-draft-tree-size-most-effective" title="Link to this heading">#</a></h3>
<p>These three parameters directly control how much memory the draft tree consumes:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Before (aggressive, high memory)</span>
--speculative-num-steps<span class="w"> </span><span class="m">5</span><span class="w"> </span>--speculative-eagle-topk<span class="w"> </span><span class="m">8</span><span class="w"> </span>--speculative-num-draft-tokens<span class="w"> </span><span class="m">64</span>

<span class="c1"># After (conservative, lower memory)</span>
--speculative-num-steps<span class="w"> </span><span class="m">3</span><span class="w"> </span>--speculative-eagle-topk<span class="w"> </span><span class="m">4</span><span class="w"> </span>--speculative-num-draft-tokens<span class="w"> </span><span class="m">16</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">--speculative-num-draft-tokens</span></code></strong>: This is the single most impactful parameter. Reducing from 64  16 can cut draft-related memory by ~75%. Start here.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span></code></strong>: Reducing from 8  4 or even 2 halves the branching factor.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">--speculative-num-steps</span></code></strong>: Reducing from 5  3 shortens the draft depth.</p></li>
</ul>
</section>
<section id="step-2-lower-static-memory-fraction">
<h3>Step 2: Lower static memory fraction<a class="headerlink" href="#step-2-lower-static-memory-fraction" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Give more room for dynamic allocations (CUDA graphs, draft model, etc.)</span>
--mem-fraction-static<span class="w"> </span><span class="m">0</span>.5<span class="w">   </span><span class="c1"># when omitted, this value is auto-computed</span>
</pre></div>
</div>
</section>
<section id="step-3-reduce-cuda-graph-batch-size">
<h3>Step 3: Reduce CUDA graph batch size<a class="headerlink" href="#step-3-reduce-cuda-graph-batch-size" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fewer CUDA graph captures = less memory reserved</span>
--cuda-graph-max-bs<span class="w"> </span><span class="m">4</span><span class="w">   </span><span class="c1"># or even 2 for tight memory situations</span>
</pre></div>
</div>
</section>
<section id="step-4-limit-concurrent-requests">
<h3>Step 4: Limit concurrent requests<a class="headerlink" href="#step-4-limit-concurrent-requests" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fewer concurrent requests lowers in-flight load and can reduce OOM risk</span>
--max-running-requests<span class="w"> </span><span class="m">4</span>
</pre></div>
</div>
</section>
<section id="step-5-use-quantization">
<h3>Step 5: Use quantization<a class="headerlink" href="#step-5-use-quantization" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Quantize the target model (if supported by your checkpoint/hardware)</span>
--quantization<span class="w"> </span>fp8

<span class="c1"># Or quantize only the draft model (keep target at full precision)</span>
--speculative-draft-model-quantization<span class="w"> </span>fp8
</pre></div>
</div>
</section>
<section id="step-6-use-a-smaller-dtype">
<h3>Step 6: Use a smaller dtype<a class="headerlink" href="#step-6-use-a-smaller-dtype" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--dtype<span class="w"> </span>float16<span class="w">   </span><span class="c1"># instead of bfloat16/float32 (when supported)</span>
</pre></div>
</div>
</section>
<section id="step-7-use-fr-spec-to-reduce-lm-head-memory-eagle-2-standalone">
<h3>Step 7: Use FR-Spec to reduce lm_head memory (EAGLE-2 / STANDALONE)<a class="headerlink" href="#step-7-use-fr-spec-to-reduce-lm-head-memory-eagle-2-standalone" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--speculative-token-map<span class="w"> </span>thunlp/LLaMA3-Instruct-8B-FR-Spec/freq_32768.pt
</pre></div>
</div>
<blockquote>
<div><p>Note: For EAGLE-3, <code class="docutils literal notranslate"><span class="pre">--speculative-token-map</span></code> is ignored because EAGLE-3 models already provide built-in hot-token handling.</p>
</div></blockquote>
</section>
<section id="quick-oom-recovery-recipe">
<h3>Quick OOM recovery recipe<a class="headerlink" href="#quick-oom-recovery-recipe" title="Link to this heading">#</a></h3>
<p>If youre hitting OOM and just want something that works, start with this minimal configuration and scale up:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>sglang.launch_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>&lt;your-model&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-algorithm<span class="w"> </span>EAGLE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-draft-model-path<span class="w"> </span>&lt;your-draft-model&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-steps<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-eagle-topk<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speculative-num-draft-tokens<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cuda-graph-max-bs<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mem-fraction-static<span class="w"> </span><span class="m">0</span>.5<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max-running-requests<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dtype<span class="w"> </span>float16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--log-level<span class="w"> </span>warning
</pre></div>
</div>
<p>Then gradually increase <code class="docutils literal notranslate"><span class="pre">--speculative-num-draft-tokens</span></code>, <code class="docutils literal notranslate"><span class="pre">--speculative-eagle-topk</span></code>, and <code class="docutils literal notranslate"><span class="pre">--cuda-graph-max-bs</span></code> until you find the sweet spot for your GPU.</p>
<blockquote>
<div><p>[!TIP]
<strong>Memory budget rule of thumb</strong>: during automatic <code class="docutils literal notranslate"><span class="pre">--mem-fraction-static</span></code> estimation, STANDALONE reserves about 6 GB and EAGLE/EAGLE3 reserves about 2 GB as additional headroom. Plan your <code class="docutils literal notranslate"><span class="pre">--mem-fraction-static</span></code> accordingly.</p>
</div></blockquote>
</section>
</section>
<hr class="docutils" />
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<p>EAGLE process is as follows:</p>
<ul class="simple">
<li><p>Within EAGLE the draft model predicts the next feature vector, i.e. the last hidden state of the original LLM, using the feature sequence <span class="math notranslate nohighlight">\((f_1, ..., f_k)\)</span> and the token sequence <span class="math notranslate nohighlight">\((t_2, ..., t_{k+1})\)</span>.</p></li>
<li><p>The next token is then sampled from <span class="math notranslate nohighlight">\(p_{k+2}=\text{LMHead}(f_{k+1})\)</span>. Afterwards, the two sequences are extended in a tree stylebranching out multiple potential continuations, with the branching factor per step controlled by the <code class="docutils literal notranslate"><span class="pre">speculative_eagle_topk</span></code> parameterto ensure a more coherent connection of context, and are given as input again.</p></li>
<li><p>In SGLangs EAGLE-2 implementation, the draft tree is expanded for the configured steps and then reranked to select the top <code class="docutils literal notranslate"><span class="pre">speculative_num_draft_tokens</span></code> final nodes as draft tokens.</p></li>
<li><p>EAGLE-3 removes the feature prediction objective, incorporates low and mid-layer features, and is trained in an on-policy manner.</p></li>
</ul>
<p>This enhances drafting accuracy by operating on features instead of tokens for more regular inputs and by additionally passing tokens from the next timestep to reduce sampling randomness. For more details, see the <a class="reference external" href="https://arxiv.org/abs/2406.16858">EAGLE-2</a> and <a class="reference external" href="https://arxiv.org/abs/2503.01840">EAGLE-3</a> papers.</p>
<p>For guidance how to train your own EAGLE model please see the <a class="reference external" href="https://github.com/SafeAILab/EAGLE/tree/main?tab=readme-ov-file#train">EAGLE repo</a>.</p>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="attention_backend.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Attention Backend</p>
      </div>
    </a>
    <a class="right-next"
       href="structured_outputs.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Structured Outputs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jump-to-sections">Jump to sections</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-guidance">Quick guidance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#method-comparison-mini-table">Method comparison (mini table)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-highlights">Performance Highlights</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eagle-decoding">EAGLE Decoding</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eagle-2-decoding">EAGLE-2 decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eagle-2-decoding-with-torch-compile">EAGLE-2 Decoding with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eagle-2-decoding-via-frequency-ranked-speculative-sampling">EAGLE-2 Decoding via Frequency-Ranked Speculative Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eagle-3-decoding">EAGLE-3 Decoding</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-token-prediction">Multi Token Prediction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standalone-speculative-decoding-small-draft-model">Standalone Speculative Decoding (Small Draft Model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#speculative-decoding-v2-overlap-scheduler">Speculative Decoding V2 (Overlap Scheduler)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ngram-speculative-decoding">Ngram Speculative Decoding</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ngram-specific-parameters">Ngram-specific parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#full-parameter-reference">Full Parameter Reference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-parameters">Core parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Ngram-specific parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-variables">Environment variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-related-flags">Other related flags</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#oom-troubleshooting">OOM Troubleshooting</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-reduce-draft-tree-size-most-effective">Step 1: Reduce draft tree size (most effective)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-lower-static-memory-fraction">Step 2: Lower static memory fraction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-reduce-cuda-graph-batch-size">Step 3: Reduce CUDA graph batch size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-limit-concurrent-requests">Step 4: Limit concurrent requests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-use-quantization">Step 5: Use quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-use-a-smaller-dtype">Step 6: Use a smaller dtype</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-7-use-fr-spec-to-reduce-lm-head-memory-eagle-2-standalone">Step 7: Use FR-Spec to reduce lm_head memory (EAGLE-2 / STANDALONE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-oom-recovery-recipe">Quick OOM recovery recipe</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By SGLang Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
       Copyright 2023-2026, SGLang.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Feb 26, 2026.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  
    <!-- RunLLM Widget Script -->
    <script type="module" id="runllm-widget-script" src="https://widget.runllm.com" crossorigin="true" version="stable" runllm-keyboard-shortcut="Mod+j" runllm-name="SGLang Chatbot" runllm-position="BOTTOM_RIGHT" runllm-assistant-id="629" async></script>
    
</body>
</html>